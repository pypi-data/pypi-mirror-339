Metadata-Version: 2.4
Name: agent-flow-tdd
Version: 0.1.3
Summary: Framework para desenvolvimento orientado a testes com agentes de IA
Home-page: https://github.com/seu-usuario/agent-flow-tdd
Author: Seu Nome
Author-email: seu.email@exemplo.com
Classifier: Development Status :: 3 - Alpha
Classifier: Intended Audience :: Developers
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Requires-Python: >=3.8
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: typer>=0.9.0
Requires-Dist: rich>=13.7.0
Requires-Dist: openai>=1.12.0
Requires-Dist: openrouter>=1.0.0
Requires-Dist: google-generativeai>=0.8.0
Requires-Dist: python-dotenv>=1.0.0
Requires-Dist: pydantic>=2.6.0
Requires-Dist: requests>=2.31.0
Requires-Dist: tenacity>=8.2.0
Requires-Dist: cachetools>=5.3.0
Requires-Dist: anthropic>=0.18.0
Requires-Dist: PyYAML>=6.0.1
Requires-Dist: llama-cpp-python>=0.2.10
Provides-Extra: dev
Requires-Dist: pytest>=8.0.0; extra == "dev"
Requires-Dist: pytest-cov>=4.1.0; extra == "dev"
Requires-Dist: pytest-mock>=3.12.0; extra == "dev"
Requires-Dist: black>=23.0.0; extra == "dev"
Requires-Dist: isort>=5.12.0; extra == "dev"
Requires-Dist: flake8>=6.1.0; extra == "dev"
Requires-Dist: mypy>=1.5.0; extra == "dev"
Requires-Dist: autoflake>=2.2.0; extra == "dev"
Requires-Dist: build>=1.0.3; extra == "dev"
Requires-Dist: twine>=4.0.2; extra == "dev"
Dynamic: author
Dynamic: author-email
Dynamic: classifier
Dynamic: description
Dynamic: description-content-type
Dynamic: home-page
Dynamic: license-file
Dynamic: provides-extra
Dynamic: requires-dist
Dynamic: requires-python
Dynamic: summary

# Prompt TDD

Um sistema para desenvolvimento orientado a testes usando prompts de IA.

## üöÄ Funcionalidades

- Gera√ß√£o de features com crit√©rios de aceite e cen√°rios de teste
- An√°lise de complexidade e estimativas
- Suporte a m√∫ltiplos modelos de IA (GPT-3.5, GPT-4)
- Interface CLI com modo interativo e MCP (Multi-Command Protocol)
- Sa√≠da em formatos JSON e Markdown
- Configura√ß√£o unificada e centralizada

## üìã Pr√©-requisitos

- Python 3.13+
- Chave de API OpenAI (`OPENAI_API_KEY`)
- Ambiente virtual Python (venv)

## üõ†Ô∏è Instala√ß√£o

1. Clone o reposit√≥rio:
```bash
git clone https://github.com/seu-usuario/prompt-tdd.git
cd prompt-tdd
```

2. Configure as vari√°veis de ambiente necess√°rias:
```bash
# N√£o use arquivos .env, configure diretamente no ambiente
export OPENAI_API_KEY="sua-chave-aqui"
```

3. Instale as depend√™ncias:
```bash
make install
```

## ‚öôÔ∏è Configura√ß√£o

O projeto usa um arquivo de configura√ß√£o unificado em `src/configs/cli.yaml` com tr√™s se√ß√µes principais:

### 1. CLI (`cli`)
- Configura√ß√µes de sa√≠da (formatos, indenta√ß√£o)
- Configura√ß√µes do modelo (nome, temperatura)
- Mensagens do sistema
- Formata√ß√£o JSON

### 2. MCP (`mcp`)
- Configura√ß√µes de logging do MCP
- Configura√ß√µes do LLM
- Configura√ß√µes do handler MCP
- Formatos de metadados

### 3. Aplica√ß√£o (`app`)
- Configura√ß√µes do modelo
- Configura√ß√µes do banco de dados
- Configura√ß√µes de logging
- Configura√ß√µes de exemplo
- Configura√ß√µes de resultado

## üéÆ Comandos Dispon√≠veis

### `make install`
Instala todas as depend√™ncias do projeto.

```bash
make install
```

### `make test`
Executa todos os testes do projeto.

```bash
make test
```

### `make cli`
Inicia o CLI para processamento de features.

```bash
make cli

# Exemplos de uso:
prompt-tdd feature "Criar sistema de login com autentica√ß√£o de dois fatores"
prompt-tdd feature "Criar sistema de cadastro de usu√°rios" --model gpt-4-turbo
prompt-tdd feature "Criar API REST" --format markdown
prompt-tdd status
```

#### Op√ß√µes do comando `feature`:
- `--model, -m`: Modelo a ser usado (default: gpt-3.5-turbo)
- `--elevation-model, -e`: Modelo para fallback (default: gpt-4-turbo)
- `--force, -f`: For√ßa uso do modelo sem fallback
- `--api-key, -k`: Chave da API (opcional)
- `--timeout, -t`: Tempo limite em segundos (default: 30)
- `--max-retries, -r`: M√°ximo de tentativas (default: 3)
- `--temperature, -temp`: Temperatura do modelo (default: 0.7)
- `--max-tokens, -mt`: Limite de tokens (opcional)
- `--format, -fmt`: Formato de sa√≠da (json/markdown)

### Protocolo MCP (Model Context Protocol)

O projeto agora suporta o [Model Context Protocol](https://github.com/modelcontextprotocol/protocol) oficial, permitindo:
- Integra√ß√£o padronizada com diferentes modelos de IA
- Comunica√ß√£o bidirecional via protocolo MCP
- Suporte a streaming e eventos ass√≠ncronos

#### Como Funciona

1. Inicie o modo MCP:
```bash
prompt-tdd mcp
```

2. Envie mensagens no formato MCP:
```json
{
  "content": "Criar sistema de login",
  "metadata": {
    "type": "feature",
    "options": {
      "model": "gpt-4-turbo",
      "temperature": 0.7,
      "format": "json"
    }
  }
}
```

3. Receba respostas no formato MCP:
```json
{
  "content": {
    "feature": "Sistema de Login",
    "acceptance_criteria": [...],
    "test_scenarios": [...],
    "complexity": 3
  },
  "metadata": {
    "status": "success",
    "type": "feature"
  }
}
```

## ü§ñ Integra√ß√£o de Modelos

O projeto usa o Model Context Protocol para integra√ß√£o com diferentes modelos:

### 1. Via SDK MCP

```python
from mcp_sdk import MCPHandler
from src.app import AgentOrchestrator

handler = MCPHandler()
handler.initialize(api_key="sua-chave")
handler.run()
```

### 2. Via CLI

```bash
# OpenAI GPT-4
prompt-tdd feature "Criar API" --model gpt-4-turbo --api-key $OPENAI_API_KEY

# Anthropic Claude
prompt-tdd feature "Criar API" --model claude-3 --api-key $ANTHROPIC_KEY
```

### 3. Via MCP

Especifique o modelo nas options:

```json
{
  "content": "Criar API REST",
  "metadata": {
    "type": "feature",
    "options": {
      "model": "gpt-4-turbo",
      "api_key": "sua-chave",
      "temperature": 0.7
    }
  }
}
```

### Modelos Suportados

Atualmente:
- OpenAI GPT-3.5 Turbo
- OpenAI GPT-4 Turbo
- Anthropic Claude (via MCP)
- TinyLLaMA 1.1B (local, via llama.cpp)
- Outros modelos compat√≠veis com MCP

### Configura√ß√£o do TinyLLaMA

Para usar o TinyLLaMA:

1. O modelo ser√° baixado automaticamente durante a instala√ß√£o (`make install`). 
   Alternativamente, voc√™ pode baix√°-lo manualmente:
```bash
make download-model
```

2. Use o modelo via CLI:
```bash
prompt-tdd feature "Criar API" --model tinyllama-1.1b
```

3. Ou via MCP:
```json
{
  "content": "Criar API REST",
  "metadata": {
    "type": "feature",
    "options": {
      "model": "tinyllama-1.1b",
      "temperature": 0.7
    }
  }
}
```

## üß™ Testes

O projeto usa pytest para testes. Execute:

```bash
make test
```

## üìù Logs

Os logs s√£o gerados automaticamente com:
- N√≠vel INFO para entrada/sa√≠da de fun√ß√µes
- N√≠vel DEBUG para estados intermedi√°rios
- N√≠vel ERROR para exce√ß√µes (com stacktrace)
- Configura√ß√£o centralizada em `cli.yaml`

## üîí Vari√°veis de Ambiente

Vari√°veis obrigat√≥rias:
- `OPENAI_API_KEY`: Chave da API OpenAI

Vari√°veis opcionais:
- `OPENROUTER_KEY`: Chave da API OpenRouter
- `GEMINI_KEY`: Chave da API Gemini
- `ANTHROPIC_KEY`: Chave da API Anthropic
- `LOG_LEVEL`: N√≠vel de log (default: INFO)
- `CACHE_DIR`: Diret√≥rio de cache
- `CACHE_TTL`: Tempo de vida do cache
- `FALLBACK_ENABLED`: Habilita fallback de modelos
- `DEFAULT_MODEL`: Modelo padr√£o
- `ELEVATION_MODEL`: Modelo para fallback
- `MODEL_TIMEOUT`: Timeout para chamadas de modelo
- `MAX_RETRIES`: M√°ximo de tentativas

**IMPORTANTE**: N√£o use arquivos .env. Configure as vari√°veis diretamente no ambiente ou via argumentos.

## ü§ù Contribuindo

1. Fork o projeto
2. Crie sua branch de feature (`git checkout -b feature/MinhaFeature`)
3. Commit suas mudan√ßas (`git commit -m 'Adiciona MinhaFeature'`)
4. Push para a branch (`git push origin feature/MinhaFeature`)
5. Abra um Pull Request

## üìÑ Licen√ßa

Este projeto est√° sob a licen√ßa MIT. Veja o arquivo [LICENSE](LICENSE) para mais detalhes.

## Uso

O Prompt TDD pode ser usado de duas formas:

### 1. Usando o Makefile (recomendado)

```bash
# Criar uma nova feature
make run prompt-tdd="Criar um sistema de login com autentica√ß√£o JWT"

# Verificar status do ambiente
make run prompt-tdd="" mode=status

# Iniciar servidor MCP em background (sem sa√≠da no terminal)
make run prompt-tdd="" mode=mcp
# O servidor MCP ser√° iniciado em background e voc√™ ver√° apenas o PID do processo
```

### 2. Usando o comando diretamente

Primeiro, ative o ambiente virtual:

```bash
source .venv/bin/activate
```

Ent√£o use o comando `prompt-tdd`:

```bash
# Criar uma nova feature
prompt-tdd "Criar um sistema de login com autentica√ß√£o JWT"

# Verificar status do ambiente
prompt-tdd --mode status ""

# Iniciar servidor MCP (ir√° bloquear o terminal e mostrar logs)
prompt-tdd --mode mcp ""

# Ou inicie em background sem logs
nohup prompt-tdd --mode mcp "" > /dev/null 2>&1 &
```

## Op√ß√µes

- `--format`: Formato de sa√≠da (json ou markdown). Padr√£o: json
- `--mode`: Modo de opera√ß√£o (feature, status ou mcp). Padr√£o: feature

## Testes

```bash
make test
```

## Limpeza

```bash
make clean
```

# Agent Flow TDD

Framework para desenvolvimento orientado a testes com agentes de IA.

## Recursos

- Desenvolvimento orientado a testes para agentes de IA
- Integra√ß√£o com OpenAI Agent SDK
- Logging estruturado em SQLite
- Suporte a m√∫ltiplos provedores de LLM
- Sistema de tracing e monitoramento
- Interface MCP (Model Context Protocol)

## Instala√ß√£o

```bash
# Instala√ß√£o b√°sica
pip install agent-flow-tdd

# Instala√ß√£o com depend√™ncias de desenvolvimento
pip install agent-flow-tdd[dev]
```

## Uso B√°sico

```python
from src.app import AgentOrchestrator

# Inicializa o orquestrador
orchestrator = AgentOrchestrator(api_key="sua-chave-api")

# Processa uma entrada
result = orchestrator.handle_input("Criar sistema de login")
print(result)
```

## Logging Estruturado

O framework inclui um sistema de logging estruturado que armazena todas as intera√ß√µes em SQLite:

### Dados Armazenados

- **Execu√ß√µes de Agentes**
  - Session ID
  - Input/Output
  - √öltimo agente executado
  - Tipo de sa√≠da
  - Timestamp

- **Itens Gerados**
  - MessageOutput
  - HandoffCall/HandoffOutput
  - ToolCall/ToolCallOutput
  - ReasoningItem

- **Guardrails**
  - Resultados de input/output
  - Mensagens de valida√ß√£o

- **Respostas Brutas**
  - Respostas do LLM
  - Metadados de execu√ß√£o

### Consulta de Logs

```python
from src.core.db import DatabaseManager

# Inicializa o gerenciador
db = DatabaseManager()

# Busca hist√≥rico de execu√ß√µes
history = db.get_run_history(limit=10)

# Exemplo de processamento
for run in history:
    print(f"Execu√ß√£o {run['id']}:")
    print(f"- Input: {run['input']}")
    print(f"- Agente: {run['last_agent']}")
    print(f"- Items gerados: {len(run['items'])}")
    print(f"- Guardrails: {len(run['guardrails'])}")
    print(f"- Respostas: {len(run['raw_responses'])}")
```

### Schema SQL

```sql
-- Tabela principal de execu√ß√µes
CREATE TABLE agent_runs (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
    session_id TEXT NOT NULL,
    input TEXT NOT NULL,
    last_agent TEXT,
    output_type TEXT,
    final_output TEXT
);

-- Tabela de itens gerados
CREATE TABLE run_items (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    run_id INTEGER NOT NULL,
    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
    item_type TEXT NOT NULL,
    raw_item TEXT NOT NULL,
    source_agent TEXT,
    target_agent TEXT,
    FOREIGN KEY(run_id) REFERENCES agent_runs(id)
);

-- Tabela de resultados de guardrails
CREATE TABLE guardrail_results (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    run_id INTEGER NOT NULL,
    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
    guardrail_type TEXT CHECK(guardrail_type IN ('input', 'output')),
    results TEXT NOT NULL,
    FOREIGN KEY(run_id) REFERENCES agent_runs(id)
);

-- Tabela de respostas brutas do LLM
CREATE TABLE raw_responses (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    run_id INTEGER NOT NULL,
    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
    response TEXT NOT NULL,
    FOREIGN KEY(run_id) REFERENCES agent_runs(id)
);
```

## Desenvolvimento

### Configura√ß√£o do Ambiente

```bash
# Clone o reposit√≥rio
git clone https://github.com/seu-usuario/agent-flow-tdd.git
cd agent-flow-tdd

# Crie um ambiente virtual
python -m venv .venv
source .venv/bin/activate  # Linux/Mac
.venv\Scripts\activate     # Windows

# Instale em modo desenvolvimento
pip install -e ".[dev]"
```

### Executando Testes

```bash
# Executa todos os testes
make test

# Executa testes com cobertura
pytest --cov=src tests/

# Executa testes espec√≠ficos
pytest tests/test_db.py -v
```

### Linting e Formata√ß√£o

```bash
# Formata o c√≥digo
make format

# Executa linters
make lint

# Limpa imports n√£o utilizados
make autoflake
```

## Contribuindo

1. Fork o projeto
2. Crie uma branch para sua feature (`git checkout -b feature/nova-feature`)
3. Commit suas mudan√ßas (`git commit -am 'Adiciona nova feature'`)
4. Push para a branch (`git push origin feature/nova-feature`)
5. Crie um Pull Request

## Licen√ßa

Este projeto est√° licenciado sob a licen√ßa MIT - veja o arquivo [LICENSE](LICENSE) para detalhes.

## Visualiza√ß√£o de Logs

O framework inclui um visualizador de logs que permite consultar o hist√≥rico de execu√ß√µes dos agentes. Para usar:

```bash
# Lista as √∫ltimas 10 execu√ß√µes
make logs

# Lista as √∫ltimas N execu√ß√µes
make logs ARGS="--limit 20"

# Filtra por session ID
make logs ARGS="--session abc123"

# Filtra por agente
make logs ARGS="--agent CodeReviewer"

# Mostra detalhes de uma execu√ß√£o espec√≠fica
make logs ARGS="--id 42"
```

O visualizador mostra:
- Lista resumida de execu√ß√µes com timestamp, session, agente e contadores
- Detalhes completos de uma execu√ß√£o espec√≠fica incluindo:
  - Input/output
  - Itens gerados entre agentes
  - Resultados de guardrails
  - Respostas brutas do LLM

## Comandos Dispon√≠veis

Para ver todos os comandos dispon√≠veis:

```bash
make help
```

### Ambiente

- `make install` - Instala depend√™ncias do projeto
- `make clean` - Remove arquivos tempor√°rios
- `make dev` - Executa em modo desenvolvimento

### Qualidade

- `make test` - Executa testes
- `make coverage` - Gera relat√≥rio de cobertura
- `make lint` - Executa linters
- `make format` - Formata c√≥digo

### Banco de Dados

- `make db-init` - Inicializa banco de dados
- `make db-clean` - Remove banco de dados
- `make db-backup` - Faz backup do banco
- `make logs` - Visualiza logs do banco

### Exemplos

```bash
# Executa o agente com um prompt
make dev prompt-tdd="Cadastro de pessoas" mode=mcp format=markdown

# Visualiza os √∫ltimos 20 logs de uma sess√£o
make logs ARGS="--limit 20 --session abc123"

# Visualiza detalhes de uma execu√ß√£o espec√≠fica
make logs ARGS="--id 42"
```

## üê≥ Usando com Docker

### Pr√©-requisitos
- Docker
- Docker Compose

### Configura√ß√£o
1. Copie o arquivo de exemplo de vari√°veis de ambiente:
```bash
cp .docker/.env.example .docker/.env
```

2. Configure suas chaves de API no arquivo `.docker/.env`

### Executando
Para desenvolvimento:
```bash
docker-compose -f .docker/docker-compose.yml run dev
```

Para produ√ß√£o:
```bash
docker-compose -f .docker/docker-compose.yml run app
```

### Comandos √öteis
- Construir as imagens:
```bash
docker-compose -f .docker/docker-compose.yml build
```

- Visualizar logs:
```bash
docker-compose -f .docker/docker-compose.yml logs -f
```

- Limpar volumes:
```bash
docker-compose -f .docker/docker-compose.yml down -v
```
