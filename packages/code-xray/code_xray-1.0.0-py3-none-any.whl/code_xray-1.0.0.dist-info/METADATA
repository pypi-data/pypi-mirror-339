Metadata-Version: 2.4
Name: code-xray
Version: 1.0.0
Summary: Terminal-based code explainer powered by Ollama
Author-email: Aayush Rajesh Jadhav <aayushrj22@gmail.com>
License: MIT
Requires-Python: >=3.9
Description-Content-Type: text/markdown
Requires-Dist: textual
Requires-Dist: rich
Requires-Dist: httpx
Requires-Dist: typer[all]
Requires-Dist: prompt_toolkit

<p align="center">
  <img src="https://raw.githubusercontent.com/ARJ2211/code-xray/main/assets/code-xray.png" alt="code-xray logo"/>
</p>

# ðŸ§  code-xray

`code-xray` is a terminal-based code exploration and explanation tool powered by local LLMs (like Ollama).  
Select lines of code interactively, send them for explanation, and get human-friendly insights â€“ right in your terminal.

<img src="assets/detail.gif" alt="Demo" width="600" height="400" />

---

## âœ¨ Features

- âœ… Terminal-based file viewer with syntax highlighting
- âœ… Line-by-line navigation and selection
- âœ… Integration with local LLMs via [Ollama](https://ollama.com)
- âœ… On-demand code explanation using selected lines and full-file context
- âœ… Works fully offline
- âœ… Customizable LLM model and port via CLI

---

## ðŸš€ Usage

### 1. Basic Command

```bash
code-xray /path/to/your/file.py
```

This opens an interactive terminal interface to browse and explain code.

### 2. With Custom Model and Port

```bash
code-xray /path/to/your/file.py --model mistral --port 11434
```

- `--model` or `-m`: LLM model name (e.g. `mistral`, `llama3`, `codellama`)
- `--port` or `-p`: Port where Ollama is running (default is `11434`)

---

## ðŸ§­ Keybindings

| Key       | Action                  |
| --------- | ----------------------- |
| `h`       | Move up one line        |
| `l`       | Move down one line      |
| `Shift+h` | Expand selection up     |
| `Shift+l` | Expand selection down   |
| `e`       | Explain selected code   |
| `q`       | Quit viewer or popup    |
| `Esc`     | Close explanation popup |

---

## ðŸ›  Requirements

- Python 3.10+
- [Ollama](https://ollama.com) running locally with desired model pulled

Example to pull a model:

```bash
ollama pull mistral
```

Then ensure it's running:

```bash
ollama serve
```

---

## ðŸ§© Installation

```bash
pip install code-xray
```

> Make sure `code-xray` is available in your PATH or create an alias.

---

## ðŸ™Œ Acknowledgements

- [Textual](https://github.com/Textualize/textual) for the beautiful terminal UI
- [Ollama](https://ollama.com) for local model hosting
- [Rich](https://github.com/Textualize/rich) for the syntax highlighting

---

## ðŸ”— Contributions

Pull requests welcome! Feel free to fork and build on top of this.
