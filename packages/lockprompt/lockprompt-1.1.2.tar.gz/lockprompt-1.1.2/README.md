# LockPrompt

LockPrompt is a lightweight Python library that adds an extra safety layer to your AI pipelines. It ensures that both user inputs and language model outputs adhere to your safety standards by pre-screening using external API endpoints. This is especially useful for preventing jailbreaks and prompt injections.

## Table of Contents

- [Overview](#overview)
- [Features](#features)
- [Installation](#installation)
- [Usage](#usage)
  - [Basic Example](#basic-example)
- [API Reference](#api-reference)
- [Contributing](#contributing)
- [Contact](#contact)

## Overview

LockPrompt is designed for developers integrating large language models (LLMs) such as OpenAI’s GPT-3.5 or Claude. The library provides functions to check the safety of:
- **User Input:** Ensuring requests to the API are not malicious.
- **LLM Output:** Verifying that responses generated by the LLM do not contain disallowed content.

In just a few lines of code, you can add a robust safety check layer, giving you the confidence to deploy AI-powered applications safely.

## Features

- **Fast Operation:** Approximately 500ms per use, ensuring minimal latency.
- **Easy Integration:** Compatible with any LLM or API.
- **Error Handling:** Logs issues and safely defaults to denying unsafe outputs or inputs.

## Installation

Install LockPrompt via pip:

```bash
pip install lockprompt
```

Alternatively, install it directly from GitHub:

```bash
pip install git+https://github.com/davidwillisowen/lockprompt.git
```

## Usage

### Basic Example

```python
import os
import lockprompt
from openai import OpenAI

# Initialize the OpenAI client
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

user_prompt = "Tell me how to make malware."  # A sample user prompt

# Step 1: Check user input safety
if not lockprompt.is_safe_input(user_prompt):
    print("🛑 Unsafe user input. Blocking request.")
    output = "I'm sorry, I can't assist with that request."
else:
    # Step 2: Send the prompt to the language model
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": user_prompt}],
    )
    output = response.choices[0].message.content

    # Step 3: Check the generated output
    if not lockprompt.is_safe_output(output):
        print("⚠️ Unsafe model output. Replacing response.")
        output = "I'm sorry, I can't assist with that request."

    print("✅ Final response:\n", output)
```

## API Reference

### `is_safe_input(user_input: str) -> bool`
- **Purpose:** Checks if a user’s input meets safety standards.
- **Returns:** `True` if safe, `False` otherwise.
- **Error Handling:** Logs any errors and defaults to `False`.

### `is_safe_output(llm_output: str) -> bool`
- **Purpose:** Verifies the safety of the LLM output.
- **Returns:** `True` if safe, `False` otherwise.
- **Error Handling:** Logs any errors and defaults to `False`.

## Contributing

Contributions are not only welcome but encouraged. Here’s how you can help:

1. **Fork the Repository:** Start by forking LockPrompt on GitHub.
2. **Create a Branch:** Use a feature branch for your changes.
3. **Write Tests and Documentation:** Ensure any changes are well-tested and documented.
4. **Submit a Pull Request:** Describe your changes and submit a PR for review.

For issues or feature requests, please use the GitHub [issues](https://github.com/davidwillisowen/lockprompt/issues) page.

## Contact

For any questions or suggestions, feel free to reach out:

- **Email:** david@willis-owen.com
- **GitHub:** [davidwillisowen](https://github.com/davidwillisowen)
