# Copyright 2025 Emcie Co Ltd.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

from abc import ABC, abstractmethod
import asyncio
from dataclasses import dataclass
from datetime import datetime
from itertools import chain
import re
import jinja2
import jinja2.meta
import json
import traceback
from typing import Any, Mapping, Optional, Sequence, cast
from typing_extensions import override

from parlant.core.async_utils import safe_gather
from parlant.core.contextual_correlator import ContextualCorrelator
from parlant.core.agents import Agent, CompositionMode
from parlant.core.context_variables import ContextVariable, ContextVariableValue
from parlant.core.customers import Customer
from parlant.core.engines.alpha.message_event_composer import (
    MessageCompositionError,
    MessageEventComposer,
    MessageEventComposition,
)
from parlant.core.engines.alpha.message_generator import MessageGenerator
from parlant.core.engines.alpha.tool_caller import ToolInsights
from parlant.core.utterances import Utterance, UtteranceId, UtteranceStore
from parlant.core.nlp.generation import SchematicGenerator
from parlant.core.nlp.generation_info import GenerationInfo
from parlant.core.engines.alpha.guideline_match import GuidelineMatch
from parlant.core.engines.alpha.prompt_builder import PromptBuilder, BuiltInSection, SectionStatus
from parlant.core.glossary import Term
from parlant.core.emissions import EmittedEvent, EventEmitter
from parlant.core.sessions import Event, MessageEventData, Participant, ToolEventData
from parlant.core.common import DefaultBaseModel, JSONSerializable
from parlant.core.loggers import Logger
from parlant.core.shots import Shot, ShotCollection
from parlant.core.tools import ToolId

DEFAULT_NO_MATCH_UTTERANCE = (
    "Sorry, I couldn't hear you very well due to a hiccup. Could you please repeat that?"
)


class UtteranceChoice(DefaultBaseModel):
    insights_about_the_user: Optional[str] = None
    utterance_choice_reasoning: str
    chosen_utterance: Optional[str] = None
    chosen_utterance_id: Optional[str] = None


class UtteranceSelectionSchema(DefaultBaseModel):
    last_message_of_user: Optional[str]
    guidelines: list[str]
    insights: Optional[list[str]] = None
    utterance_choice: Optional[UtteranceChoice] = None


class UtteranceCompositionSchema(DefaultBaseModel):
    revised_utterance: str


@dataclass
class UtteranceSelectorShot(Shot):
    composition_modes: list[CompositionMode]
    expected_result: UtteranceSelectionSchema


@dataclass(frozen=True)
class _UtteranceSelectionResult:
    @staticmethod
    def no_match() -> _UtteranceSelectionResult:
        return _UtteranceSelectionResult(
            message=DEFAULT_NO_MATCH_UTTERANCE,
            utterances=[],
        )

    message: str
    utterances: list[tuple[UtteranceId, str]]


@dataclass(frozen=True)
class UtteranceContext:
    agent: Agent
    customer: Customer
    context_variables: Sequence[tuple[ContextVariable, ContextVariableValue]]
    interaction_history: Sequence[Event]
    terms: Sequence[Term]
    tool_insights: ToolInsights
    staged_events: Sequence[EmittedEvent]


class UtteranceFieldExtractionMethod(ABC):
    @abstractmethod
    async def extract(
        self,
        utterance: str,
        field_name: str,
        context: UtteranceContext,
    ) -> tuple[bool, JSONSerializable]: ...


class StandardFieldExtraction(UtteranceFieldExtractionMethod):
    def __init__(self, logger: Logger) -> None:
        self._logger = logger

    @override
    async def extract(
        self,
        utterance: str,
        field_name: str,
        context: UtteranceContext,
    ) -> tuple[bool, JSONSerializable]:
        if field_name != "std":
            return False, None

        return True, {
            "customer": {"name": context.customer.name},
            "agent": {"name": context.agent.name},
            "variables": {
                variable.name: value.data for variable, value in context.context_variables
            },
            "missing_params": self._extract_missing_params(context.tool_insights),
        }

    def _extract_missing_params(
        self,
        tool_insights: ToolInsights,
    ) -> list[str]:
        return [missing_data.parameter for missing_data in tool_insights.missing_data]


class ToolBasedFieldExtraction(UtteranceFieldExtractionMethod):
    @override
    async def extract(
        self,
        utterance: str,
        field_name: str,
        context: UtteranceContext,
    ) -> tuple[bool, JSONSerializable]:
        if not context.staged_events:
            return False, None

        for tool_event in [e for e in context.staged_events if e.kind == "tool"]:
            data = cast(ToolEventData, tool_event.data)

            for tool_call in data["tool_calls"]:
                if value := tool_call["result"]["utterance_fields"].get(field_name, None):
                    return True, value

        return False, None


class UtteranceFieldExtractionSchema(DefaultBaseModel):
    field_name: Optional[str] = None
    field_value: Optional[str] = None


class GenerativeFieldExtraction(UtteranceFieldExtractionMethod):
    def __init__(
        self,
        logger: Logger,
        generator: SchematicGenerator[UtteranceFieldExtractionSchema],
    ) -> None:
        self._logger = logger
        self._generator = generator

    @override
    async def extract(
        self,
        utterance: str,
        field_name: str,
        context: UtteranceContext,
    ) -> tuple[bool, JSONSerializable]:
        if field_name != "generative":
            return False, None

        generative_fields = set(re.findall(r"\{\{(generative\.[a-zA-Z0-9_]+)\}\}", utterance))

        if not generative_fields:
            return False, None

        tasks = {
            field[len("generative.") :]: asyncio.create_task(
                self._generate_field(utterance, field, context)
            )
            for field in generative_fields
        }

        await safe_gather(*tasks.values())

        fields = {field: task.result() for field, task in tasks.items()}

        if None in fields.values():
            return False, None

        return True, fields

    async def _generate_field(
        self,
        utterance: str,
        field_name: str,
        context: UtteranceContext,
    ) -> Optional[str]:
        builder = PromptBuilder()

        builder.add_section(
            "utterance-generative-field-extraction-instructions",
            "Your only job is to extract a particular value in the most suitable way from the following context.",
        )

        builder.add_agent_identity(context.agent)
        builder.add_context_variables(context.context_variables)
        builder.add_interaction_history(context.interaction_history)
        builder.add_glossary(context.terms)
        builder.add_staged_events(context.staged_events)

        builder.add_section(
            "utterance-generative-field-extraction-field-name",
            """\
We're now working on rendering an utterance template as a reply to the user.

The utterance template we're rendering is this: ###
{utterance}
###

We're rendering one field at a time out of this utterance.
Your job now is to take all of the context above and extract out of it the value for the field '{field_name}' within the utterance template.

Output a JSON object containing the extracted field such that it neatly renders (substituting the field variable) into the utterance template.

When applicable, if the field is substituted by a list or dict, consider rendering the value in Markdown format.

A few examples:
---------------
1) Utterance is "Hello {{{{generative.name}}}}, how may I help you today?"
Example return value: ###
{{ "field_name": "name", "field_value": "John" }}
###

2) Utterance is "Hello {{{{generative.names}}}}, how may I help you today?"
Example return value: ###
{{ "field_name": "names", "field_value": "John and Katie" }}
###

3) Utterance is "Next flights are {{{{generative.flight_list}}}}
Example return value: ###
{{ "field_name": "flight_list", "field_value": "- <FLIGHT_1>\\n- <FLIGHT_2>\\n" }}
###
""",
            props={"utterance": utterance, "field_name": field_name},
        )

        result = await self._generator.generate(builder)

        self._logger.debug(
            f"Utterance GenerativeFieldExtraction Completion:\n{result.content.model_dump_json(indent=2)}"
        )

        return result.content.field_value


class UtteranceFieldExtractor(ABC):
    def __init__(
        self,
        standard: StandardFieldExtraction,
        tool_based: ToolBasedFieldExtraction,
        generative: GenerativeFieldExtraction,
    ) -> None:
        self.methods: list[UtteranceFieldExtractionMethod] = [
            standard,
            tool_based,
            generative,
        ]

    async def extract(
        self,
        utterance: str,
        field_name: str,
        context: UtteranceContext,
    ) -> tuple[bool, JSONSerializable]:
        for method in self.methods:
            success, extracted_value = await method.extract(
                utterance,
                field_name,
                context,
            )

            if success:
                return True, extracted_value

        return False, None


class FluidUtteranceFallback(Exception):
    def __init__(self) -> None:
        pass


class UtteranceSelector(MessageEventComposer):
    def __init__(
        self,
        logger: Logger,
        correlator: ContextualCorrelator,
        utterance_selection_generator: SchematicGenerator[UtteranceSelectionSchema],
        utterance_composition_generator: SchematicGenerator[UtteranceCompositionSchema],
        utterance_store: UtteranceStore,
        field_extractor: UtteranceFieldExtractor,
        message_generator: MessageGenerator,
    ) -> None:
        self._logger = logger
        self._correlator = correlator
        self._utterance_selection_generator = utterance_selection_generator
        self._utterance_composition_generator = utterance_composition_generator
        self._utterance_store = utterance_store
        self._field_extractor = field_extractor
        self._message_generator = message_generator

    async def shots(self, composition_mode: CompositionMode) -> Sequence[UtteranceSelectorShot]:
        shots = await shot_collection.list()
        supported_shots = [s for s in shots if composition_mode in s.composition_modes]
        return supported_shots

    @override
    async def generate_events(
        self,
        event_emitter: EventEmitter,
        agent: Agent,
        customer: Customer,
        context_variables: Sequence[tuple[ContextVariable, ContextVariableValue]],
        interaction_history: Sequence[Event],
        terms: Sequence[Term],
        ordinary_guideline_matches: Sequence[GuidelineMatch],
        tool_enabled_guideline_matches: Mapping[GuidelineMatch, Sequence[ToolId]],
        tool_insights: ToolInsights,
        staged_events: Sequence[EmittedEvent],
    ) -> Sequence[MessageEventComposition]:
        with self._logger.scope("MessageEventComposer"):
            try:
                with self._logger.scope("UtteranceSelector"):
                    with self._logger.operation("Utterance selection and rendering"):
                        return await self._do_generate_events(
                            event_emitter,
                            agent,
                            customer,
                            context_variables,
                            interaction_history,
                            terms,
                            ordinary_guideline_matches,
                            tool_enabled_guideline_matches,
                            tool_insights,
                            staged_events,
                        )
            except FluidUtteranceFallback:
                return await self._message_generator.generate_events(
                    event_emitter,
                    agent,
                    customer,
                    context_variables,
                    interaction_history,
                    terms,
                    ordinary_guideline_matches,
                    tool_enabled_guideline_matches,
                    tool_insights,
                    staged_events,
                )

    async def _get_utterances(
        self,
        staged_events: Sequence[EmittedEvent],
    ) -> list[Utterance]:
        utterances = list(await self._utterance_store.list_utterances())

        utterances_by_staged_event: list[Utterance] = []

        for event in staged_events:
            if event.kind == "tool":
                event_data: dict[str, Any] = cast(dict[str, Any], event.data)
                tool_calls: list[Any] = cast(list[Any], event_data.get("tool_calls", []))
                for tool_call in tool_calls:
                    utterances_by_staged_event.extend(
                        Utterance(
                            id=Utterance.TRANSIENT_ID,
                            value=f.value,
                            fields=f.fields,
                            creation_utc=datetime.now(),
                            tags=[],
                        )
                        for f in tool_call["result"].get("utterances", [])
                    )

        return utterances + utterances_by_staged_event

    async def _do_generate_events(
        self,
        event_emitter: EventEmitter,
        agent: Agent,
        customer: Customer,
        context_variables: Sequence[tuple[ContextVariable, ContextVariableValue]],
        interaction_history: Sequence[Event],
        terms: Sequence[Term],
        ordinary_guideline_matches: Sequence[GuidelineMatch],
        tool_enabled_guideline_matches: Mapping[GuidelineMatch, Sequence[ToolId]],
        tool_insights: ToolInsights,
        staged_events: Sequence[EmittedEvent],
    ) -> Sequence[MessageEventComposition]:
        if (
            not interaction_history
            and not ordinary_guideline_matches
            and not tool_enabled_guideline_matches
        ):
            # No interaction and no guidelines that could trigger
            # a proactive start of the interaction
            self._logger.info("Skipping response; interaction is empty and there are no guidelines")
            return []

        utterances = await self._get_utterances(staged_events)

        if not utterances and agent.composition_mode != "fluid_utterance":
            self._logger.warning("No utterances found; skipping response")
            return []

        prompt = self._build_prompt(
            agent=agent,
            context_variables=context_variables,
            customer=customer,
            interaction_history=interaction_history,
            terms=terms,
            ordinary_guideline_matches=ordinary_guideline_matches,
            tool_enabled_guideline_matches=tool_enabled_guideline_matches,
            staged_events=staged_events,
            tool_insights=tool_insights,
            utterances=utterances,
            shots=await self.shots(agent.composition_mode),
        )

        last_known_event_offset = interaction_history[-1].offset if interaction_history else -1

        await event_emitter.emit_status_event(
            correlation_id=self._correlator.correlation_id,
            data={
                "acknowledged_offset": last_known_event_offset,
                "status": "typing",
                "data": {},
            },
        )

        generation_attempt_temperatures = {
            0: 0.1,
            1: 0.05,
            2: 0.2,
        }

        last_generation_exception: Exception | None = None

        context = UtteranceContext(
            agent=agent,
            customer=customer,
            context_variables=context_variables,
            interaction_history=interaction_history,
            terms=terms,
            tool_insights=tool_insights,
            staged_events=staged_events,
        )

        for generation_attempt in range(3):
            try:
                generation_info, assembly_result = await self._generate_utterance(
                    prompt,
                    context,
                    utterances,
                    agent.composition_mode,
                    temperature=generation_attempt_temperatures[generation_attempt],
                )

                if assembly_result is not None:
                    event = await event_emitter.emit_message_event(
                        correlation_id=self._correlator.correlation_id,
                        data=MessageEventData(
                            message=assembly_result.message,
                            participant=Participant(id=agent.id, display_name=agent.name),
                            utterances=assembly_result.utterances,
                        ),
                    )

                    return [MessageEventComposition(generation_info, [event])]
                else:
                    self._logger.debug("Skipping response; no response deemed necessary")
                    return [MessageEventComposition(generation_info, [])]
            except FluidUtteranceFallback:
                raise
            except Exception as exc:
                self._logger.warning(
                    f"Generation attempt {generation_attempt} failed: {traceback.format_exception(exc)}"
                )
                last_generation_exception = exc

        raise MessageCompositionError() from last_generation_exception

    def _get_utterance_bank_text(
        self,
        utterances: Sequence[Utterance],
    ) -> tuple[str, list[str]]:
        template = """
In formulating your reply, you must rely on the following bank of utterances.
Each utterance contains content, which may or may not refer to "utterance fields" using curly braces.
For example, in the utterance 'I can help you with {{something}}', there is one utterance field called 'something'.
For your references, some utterance may include some examples for how to fill out their utterance fields properly—though you should not fill them yourself! This is only for reference to guide your utterance choice.

Note: If you do not have utterances for fulfilling any instruction, you should at least try to
explain to the user that cannot help (even if only because you don't have the necessary utterances).
Only attempt to say something like this if you do at least have utterances in the bank that help
you explain this situation (the very fact you cannot help). Otherwise, produce no reply (utterance_choice = null).

UTTERANCE BANK:
--------------
{rendered_utterances}
"""

        rendered_utterances = []

        for utterance in utterances:
            utterance_dict: dict[str, Any] = {
                "utterance_id": utterance.id,
                "value": utterance.value,
            }

            if utterance.fields:
                utterance_dict["fields"] = {}

                for field in utterance.fields:
                    field_description = field.description

                    if field.examples:
                        examples = []

                        for i, example in enumerate(field.examples, start=1):
                            examples.append(f"{i}) {example}")

                        field_description += f" -- Example Extractions (only use these for reference on how to properly extract values in the right format): {'; '.join(examples)}"

                    utterance_dict["fields"][field.name] = field_description

            rendered_utterances.append(str(utterance_dict))

        template

        return template, rendered_utterances

    def _get_guideline_matches_text(
        self,
        ordinary: Sequence[GuidelineMatch],
        tool_enabled: Mapping[GuidelineMatch, Sequence[ToolId]],
    ) -> str:
        all_matches = list(chain(ordinary, tool_enabled))

        if not all_matches:
            return """
In formulating your reply, you are normally required to follow a number of behavioral guidelines.
However, in this case, no special behavioral guidelines were provided.
"""
        guidelines = []

        for i, p in enumerate(all_matches, start=1):
            guideline = f"Guideline #{i}) When {p.guideline.content.condition}, then {p.guideline.content.action}"

            guideline += f"\n    [Priority (1-10): {p.score}; Rationale: {p.rationale}]"
            guidelines.append(guideline)

        guideline_list = "\n".join(guidelines)

        return f"""
When crafting your reply, you must follow the behavioral guidelines provided below, which have been identified as relevant to the current state of the interaction.
Each guideline includes a priority score to indicate its importance and a rationale for its relevance.

You may choose not to follow a guideline only in the following cases:
    - It conflicts with a previous user request.
    - It contradicts another guideline of equal or higher priority.
    - It is clearly inappropriate given the current context of the conversation.
In all other situations, you are expected to adhere to the guidelines.
These guidelines have already been pre-filtered based on the interaction's context and other considerations outside your scope.
Never disregard a guideline, even if you believe its 'when' condition or rationale does not apply. All of the guidelines necessarily apply right now.

- **Guidelines**:
{guideline_list}
"""

    def _format_shots(
        self,
        shots: Sequence[UtteranceSelectorShot],
    ) -> str:
        return "\n".join(
            f"""
Example {i} - {shot.description}: ###
{self._format_shot(shot)}
###
"""
            for i, shot in enumerate(shots, start=1)
        )

    def _format_shot(
        self,
        shot: UtteranceSelectorShot,
    ) -> str:
        return f"""
- **Expected Result**:
```json
{json.dumps(shot.expected_result.model_dump(mode="json", exclude_unset=True), indent=2)}
```"""

    def _build_prompt(
        self,
        agent: Agent,
        customer: Customer,
        context_variables: Sequence[tuple[ContextVariable, ContextVariableValue]],
        interaction_history: Sequence[Event],
        terms: Sequence[Term],
        ordinary_guideline_matches: Sequence[GuidelineMatch],
        tool_enabled_guideline_matches: Mapping[GuidelineMatch, Sequence[ToolId]],
        staged_events: Sequence[EmittedEvent],
        tool_insights: ToolInsights,
        utterances: Sequence[Utterance],
        shots: Sequence[UtteranceSelectorShot],
    ) -> PromptBuilder:
        can_suggest_utterances = agent.composition_mode == "fluid_utterance"

        builder = PromptBuilder(
            on_build=lambda prompt: self._logger.debug(f"Utterance Choice Prompt:\n{prompt}")
        )

        builder.add_section(
            name="utterance-selector-general-instructions",
            template="""
GENERAL INSTRUCTIONS
-----------------
You are an AI agent who is part of a system that interacts with a user. The current state of this interaction will be provided to you later in this message.
You role is to generate a reply message to the current (latest) state of the interaction, based on provided guidelines, background information, and user-provided information.

Later in this prompt, you'll be provided with behavioral guidelines and other contextual information you must take into account when generating your response.

""",
            props={},
        )

        builder.add_agent_identity(agent)
        builder.add_section(
            name="utterance-selector-task-description",
            template="""
TASK DESCRIPTION:
-----------------
Continue the provided interaction in a natural and human-like manner.
Your task is to produce a response to the latest state of the interaction.
Always abide by the following general principles (note these are not the "guidelines". The guidelines will be provided later):
1. GENERAL BEHAVIOR: Make your response as human-like as possible. Be concise and avoid being overly polite when not necessary.
2. AVOID REPEATING YOURSELF: When replying— avoid repeating yourself. Instead, refer the user to your previous answer, or choose a new approach altogether. If a conversation is looping, point that out to the user instead of maintaining the loop.
3. REITERATE INFORMATION FROM PREVIOUS MESSAGES IF NECESSARY: If you previously suggested a solution or shared information during the interaction, you may repeat it when relevant. Your earlier response may have been based on information that is no longer available to you, so it's important to trust that it was informed by the context at the time.
4. MAINTAIN GENERATION SECRECY: Never reveal details about the process you followed to produce your response. Do not explicitly mention the tools, context variables, guidelines, glossary, or any other internal information. Present your replies as though all relevant knowledge is inherent to you, not derived from external instructions.
""",
            props={},
        )
        if not interaction_history or all(
            [event.kind != "message" for event in interaction_history]
        ):
            builder.add_section(
                name="utterance-selector-initial-message-instructions",
                template="""
The interaction with the user has just began, and no messages were sent by either party.
If told so by a guideline or some other contextual condition, send the first message. Otherwise, do not produce a reply (utterance is null).
If you decide not to emit a message, output the following:
{{
    "last_message_of_user": "<user's last message>",
    "guidelines": [<list of strings- a re-statement of all guidelines>],
    "insights": [<list of strings- up to 3 original insights>],
    "utterance_choice": null
}}
Otherwise, follow the rest of this prompt to choose the content of your response.
        """,
                props={},
            )

        else:
            builder.add_section(
                name="utterance-selector-ongoing-interaction-instructions",
                template="""
Since the interaction with the user is already ongoing, always produce a reply to the user's last message.
The only exception where you may not produce a reply (i.e., setting utterance_choice = null) is if the user explicitly asked you not to respond to their message.
In all other cases, even if the user is indicating that the conversation is over, you must produce a reply.
                """,
                props={},
            )

        if can_suggest_utterances:
            utterance_instruction = """
Prefer to use an utterance from the bank in generating the revision's content.
If no viable utterances exist in the bank, you may suggest a new utterance.
For new suggested utterances, use the special ID "<auto>".
"""
        else:
            utterance_instruction = "You can ONLY USE UTTERANCES FROM THE UTTERANCE BANK when choosing an utterance to respond with."

        builder.add_section(
            name="utterance-selector-revision-mechanism",
            template="""
REVISION MECHANISM
-----------------
To craft an optimal response, ensure alignment with all provided guidelines based on the latest interaction state.
Choose an utterance based on which one complies to the largest degree with the outlined guidelines and the instructions in this prompt.

Before choosing an utterance, identify up to three key insights based on this prompt and the ongoing conversation.
These insights should include relevant user requests, applicable principles from this prompt, or conclusions drawn from the interaction.
Ensure to include any user request as an insight, whether it's explicit or implicit.
Do not add insights unless you believe that they are absolutely necessary. Prefer suggesting fewer insights, if at all.

How to use utterances: {utterance_instruction}

The final output must be a JSON document detailing the message development process, including insights to abide by,


PRIORITIZING INSTRUCTIONS (GUIDELINES VS. INSIGHTS)
-----------------
Deviating from an instruction (either guideline or insight) is acceptable only when the deviation arises from a deliberate prioritization, based on:
    - Conflicts with a higher-priority guideline (according to their priority scores).
    - Contradictions with a user request.
    - Lack of sufficient context or data.
    - Conflicts with an insight (see below).
In all other cases, even if you believe that a guideline's condition does not apply, you must follow it.
If fulfilling a guideline is not possible, explicitly justify why in your response.

Guidelines vs. Insights:
Sometimes, a guideline may conflict with an insight you've derived.
For example, if your insight suggests "the user is vegetarian," but a guideline instructs you to offer non-vegetarian dishes, prioritizing the insight would better align with the business's goals—since offering vegetarian options would clearly benefit the user.

However, remember that the guidelines reflect the explicit wishes of the business you represent. Deviating from them should only occur if doing so does not put the business at risk.
For instance, if a guideline explicitly prohibits a specific action (e.g., "never do X"), you must not perform that action, even if requested by the user or supported by an insight.

In cases of conflict, prioritize the business's values and ensure your decisions align with their overarching goals.

""",
            props={"utterance_instruction": utterance_instruction},
        )
        builder.add_section(
            name="utterance-selector-examples",
            template="""
EXAMPLES
-----------------
{formatted_shots}
""",
            props={
                "formatted_shots": self._format_shots(shots),
                "shots": shots,
            },
        )
        builder.add_context_variables(context_variables)
        builder.add_glossary(terms)
        utterance_bank_template, utterance_bank_rendered_utterances = self._get_utterance_bank_text(
            utterances
        )
        builder.add_section(
            name="utterance-selector-utterance-bank",
            template=utterance_bank_template,
            props={
                "utterances": utterances,
                "rendered_utterances": utterance_bank_rendered_utterances,
            },
        )
        builder.add_section(
            name=BuiltInSection.GUIDELINE_DESCRIPTIONS,
            template=self._get_guideline_matches_text(
                ordinary_guideline_matches,
                tool_enabled_guideline_matches,
            ),
            props={
                "ordinary_guideline_matches": ordinary_guideline_matches,
                "tool_enabled_guideline_matches": tool_enabled_guideline_matches,
            },
            status=SectionStatus.ACTIVE
            if ordinary_guideline_matches or tool_enabled_guideline_matches
            else SectionStatus.PASSIVE,
        )
        builder.add_interaction_history(interaction_history)
        builder.add_staged_events(staged_events)

        if tool_insights.missing_data:
            builder.add_section(
                name="utterance-selector-missing-data-for-tools",
                template="""
MISSING REQUIRED DATA FOR TOOL CALLS:
-------------------------------------
The following is a description of missing data that has been deemed necessary
in order to run tools. The tools would have run, if they only had this data available.
If it makes sense in the current state of the interaction, you may choose to inform the user about this missing data: ###
{formatted_missing_data}
###
""",
                props={
                    "formatted_missing_data": json.dumps(
                        [
                            {
                                "datum_name": d.parameter,
                                **({"description": d.description} if d.description else {}),
                                **({"significance": d.significance} if d.significance else {}),
                                **({"examples": d.examples} if d.examples else {}),
                            }
                            for d in tool_insights.missing_data
                        ]
                    ),
                    "missing_data": tool_insights.missing_data,
                },
            )

        builder.add_section(
            name="utterance-selector-output-format",
            template="""
Produce a valid JSON object in the following format: ###

{formatted_output_format}
""",
            props={
                "formatted_output_format": self._get_output_format(
                    interaction_history,
                    list(chain(ordinary_guideline_matches, tool_enabled_guideline_matches)),
                    can_suggest_utterances,
                ),
                "interaction_history": interaction_history,
                "guidelines": list(
                    chain(ordinary_guideline_matches, tool_enabled_guideline_matches)
                ),
                "can_suggest_utterances": can_suggest_utterances,
            },
        )

        return builder

    def _get_output_format(
        self,
        interaction_history: Sequence[Event],
        guidelines: Sequence[GuidelineMatch],
        allow_suggestions: bool,
    ) -> str:
        last_user_message = next(
            (
                event.data["message"] if not event.data.get("flagged", False) else "<N/A>"
                for event in reversed(interaction_history)
                if (
                    event.kind == "message"
                    and event.source == "customer"
                    and isinstance(event.data, dict)
                )
            ),
            "",
        )
        guidelines_list_text = ", ".join([f'"{g.guideline}"' for g in guidelines])

        return f"""
{{
    "last_message_of_user": "{last_user_message}",
    "guidelines": [{guidelines_list_text}],
    "insights": [<Up to 3 original insights to adhere to>],
    "utterance_choice": {{
        "insights_about_the_user": "<insights based on your utterance selection and what you know about the user>",
        "utterance_choice_reasoning": "<reason about the user, current state of the conversation, guidelines given, insights generated, including specific details on the user's situation and request, and find the best most suitable, most specialized utterance to utilize at this point as a response>",
        "chosen_utterance": <chosen utterance text or null if no matching utterance is found>,
        "chosen_utterance_id": <id of chosen utterance or null if no matching utterance is found>
    }}
}}
###"""

    async def _generate_utterance(
        self,
        prompt: PromptBuilder,
        context: UtteranceContext,
        utterances: Sequence[Utterance],
        composition_mode: CompositionMode,
        temperature: float,
    ) -> tuple[GenerationInfo, Optional[_UtteranceSelectionResult]]:
        message_event_response = await self._utterance_selection_generator.generate(
            prompt=prompt,
            hints={"temperature": temperature},
        )

        self._logger.debug(
            f"Utterance Choice Completion:\n{message_event_response.content.model_dump_json(indent=2)}"
        )

        if (
            not message_event_response.content.utterance_choice
            or not message_event_response.content.utterance_choice.chosen_utterance_id
        ):
            if composition_mode in ["strict_utterance", "composited_utterance"]:
                self._logger.warning(
                    "Failed to find relevant utterances. Please review utterance selection prompt and completion."
                )

                return message_event_response.info, _UtteranceSelectionResult.no_match()
            else:
                raise FluidUtteranceFallback()

        utterance_id = UtteranceId(
            message_event_response.content.utterance_choice.chosen_utterance_id
        )

        if utterance_id == "<auto>":
            utterance_id = Utterance.TRANSIENT_ID
            utterance = message_event_response.content.utterance_choice.chosen_utterance
        else:
            utterance = next((u.value for u in utterances if u.id == utterance_id), None)

        if not utterance:
            self._logger.error(
                "Invalid utterance ID choice. Please review utterance selection prompt and completion."
            )

            return message_event_response.info, _UtteranceSelectionResult.no_match()

        rendered_utterance = await self._render_utterance(context, utterance)

        match composition_mode:
            case "composited_utterance":
                recomposed_utterance = await self._recompose(context, rendered_utterance)

                return message_event_response.info, _UtteranceSelectionResult(
                    message=recomposed_utterance,
                    utterances=[(utterance_id, utterance)],
                )
            case "strict_utterance" | "fluid_utterance":
                return message_event_response.info, _UtteranceSelectionResult(
                    message=rendered_utterance,
                    utterances=[(utterance_id, utterance)],
                )

        raise Exception("Unsupported composition mode")

    async def _render_utterance(self, context: UtteranceContext, utterance: str) -> str:
        env = jinja2.Environment()
        parse_result = env.parse(utterance)
        field_names = jinja2.meta.find_undeclared_variables(parse_result)

        args = {}

        for field_name in field_names:
            success, value = await self._field_extractor.extract(
                utterance,
                field_name,
                context,
            )

            if success:
                args[field_name] = value
            else:
                self._logger.error(f"Utterance field extraction: missing '{field_name}'")
                return DEFAULT_NO_MATCH_UTTERANCE

        try:
            return jinja2.Template(utterance).render(**args)
        except Exception as exc:
            self._logger.error(f"Utterance rendering failed: {traceback.format_exception(exc)}")
            return DEFAULT_NO_MATCH_UTTERANCE

    async def _recompose(self, context: UtteranceContext, raw_message: str) -> str:
        builder = PromptBuilder(
            on_build=lambda prompt: self._logger.debug(f"Composition Prompt:\n{prompt}")
        )

        builder.add_agent_identity(context.agent)
        builder.add_interaction_history(context.interaction_history)

        builder.add_section(
            name="utterance-selector-composition",
            template="""\
Please revise this message's style as you see fit, trying to make it continue the above conversation more naturally.
Make sure NOT to add, remove, or hallucinate information nor add or remove key words (nouns, verbs) to the message.
Just make it flow more with the conversation (if that's even needed—otherwise you can leave it as-is if it's already perfect): ###
{raw_message}
###

Respond with a JSON object {{ "revised_utterance": "<message>" }}
""",
            props={"raw_message": raw_message},
        )

        result = await self._utterance_composition_generator.generate(
            builder,
            hints={"temperature": 0.25},
        )

        self._logger.debug(f"Composition Completion:\n{result.content.model_dump_json(indent=2)}")

        return result.content.revised_utterance


def shot_utterance_id(number: int) -> str:
    return f"<example-only-utterance--{number}--do-not-use-in-your-completion>"


example_1_expected = UtteranceSelectionSchema(
    last_message_of_user="Hi, I'd like to know the schedule for the next trains to Boston, please.",
    guidelines=["When the user asks for train schedules, provide them accurately and concisely."],
    insights=[
        "Use markdown format when applicable.",
        "Provide the train schedule without specifying which trains are *next*.",
    ],
    utterance_choice=UtteranceChoice(
        insights_about_the_user="User is looking for the next trains to Boston",
        utterance_choice_reasoning="The guidelines tell me to provide train schedules in response to the user's request. There is indeed a specific utterance with a looping template that I can use to list the train schedule",
        chosen_utterance="""\
Here's the relevant train schedule:

| Train | Departure | Arrival |
|-------|-----------|---------|
{% for train in trains %}
| {{train.number}}   | {{train.departure}} | {{train.arrival}} |
{% endfor %}
""",
        chosen_utterance_id=shot_utterance_id(4),
    ),
)

example_1_shot = UtteranceSelectorShot(
    composition_modes=["strict_utterance", "composited_utterance", "fluid_utterance"],
    description="When needed to respond with a list, prefer an utterance that contains a looping template",
    expected_result=example_1_expected,
)


example_2_expected = UtteranceSelectionSchema(
    last_message_of_user="Hi, I'd like an onion cheeseburger please.",
    guidelines=[
        "When the user chooses and orders a burger, then provide it",
        "When the user chooses specific ingredients on the burger, only provide those ingredients if we have them fresh in stock; otherwise, reject the order",
    ],
    insights=["All of our cheese has expired and is currently out of stock"],
    utterance_choice=UtteranceChoice(
        insights_about_the_user="The user is a long-time user and we should treat him with extra respect",
        utterance_choice_reasoning="There are utterances that help me guide to conversation to provide the burfer. However, I can't provide the cheeseburger since cheese is out of stock, so I should instead use the utterance that says we're out of an ingredient. At the same time, I should try to choose something to say that approaches the long-term user with respect and grace.",
        chosen_utterance="Unfortunately we're out of {{ingredient}}. Would you like anything else instead?",
        chosen_utterance_id="<auto>",
    ),
)

example_2_shot = UtteranceSelectorShot(
    composition_modes=["fluid_utterance"],
    description="A reply where one instruction was prioritized over another",
    expected_result=example_2_expected,
)


example_3_expected = UtteranceSelectionSchema(
    last_message_of_user="Hi there, can I get something to drink? What do you have on tap?",
    guidelines=["When the user asks for a drink, check the menu and offer what's on it"],
    insights=["There's no menu information in my context"],
    utterance_choice=UtteranceChoice(
        insights_about_the_user="According to contextual information about the user, this is their first time here",
        utterance_choice_reasoning="The user wants a drink, and I was told to check the menu and offer what's on it. While there are utterances for communicating that, I see that menu information was not given to my in the context. I should therefore choose the utterance that says I can't access the menu.",
        chosen_utterance="I'm sorry, but I'm having trouble accessing our menu at the moment. Can I help you with anything else?",
        chosen_utterance_id=shot_utterance_id(2),
    ),
)

example_3_shot = UtteranceSelectorShot(
    composition_modes=["strict_utterance", "composited_utterance", "fluid_utterance"],
    description="Non-adherence to guideline due to missing data",
    expected_result=example_3_expected,
)


example_4_expected = UtteranceSelectionSchema(
    last_message_of_user="This is not what I was asking for!",
    guidelines=[],
    insights=["I should not keep repeating myself as it makes me sound robotic"],
    utterance_choice=UtteranceChoice(
        utterance_choice_reasoning="I've been repeating myself asking for clarifications from the user regarding their request. To avoid repeating myself further as per the insights, I should simply apologize for not being able to assist.",
        chosen_utterance="I apologize for failing to assist you with your issue. If there's anything else I can do for you, please let me know.",
        chosen_utterance_id=shot_utterance_id(4),
    ),
)

example_4_shot = UtteranceSelectorShot(
    composition_modes=["strict_utterance", "composited_utterance", "fluid_utterance"],
    description="Avoiding repetitive responses—in this case, given that the previous response by the agent was 'I am sorry, could you please clarify your request?'",
    expected_result=example_4_expected,
)


example_5_expected = UtteranceSelectionSchema(
    last_message_of_user=("Hey, how can I contact customer support?"),
    guidelines=[],
    insights=[
        "When I cannot help with a topic, I should tell the user I can't help with it",
    ],
    utterance_choice=UtteranceChoice(
        utterance_choice_reasoning="I don't have any information or utterances about customer support, so I can't help the user with this. A good utterance for this would be the one that explains I cannot help with a topic and asks if there are other ways I could help. However, there's a more specialized utterance that deals specifically with customer support, so I should choose that one as it's the most suitable for this particular scenario.",
        chosen_utterance="Unfortunately, I cannot refer you to live customer support. Is there anything else I can help you with?",
        chosen_utterance_id=shot_utterance_id(9),
    ),
)

example_5_shot = UtteranceSelectorShot(
    composition_modes=["strict_utterance", "composited_utterance", "fluid_utterance"],
    description="An insight is derived and followed on not offering to help with something you don't know about",
    expected_result=example_5_expected,
)


_baseline_shots: Sequence[UtteranceSelectorShot] = [
    example_1_shot,
    example_2_shot,
    example_3_shot,
    example_4_shot,
    example_5_shot,
]

shot_collection = ShotCollection[UtteranceSelectorShot](_baseline_shots)
