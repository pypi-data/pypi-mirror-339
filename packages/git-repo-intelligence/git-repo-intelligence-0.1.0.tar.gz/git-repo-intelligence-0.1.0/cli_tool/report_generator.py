"""
Report Generator module for GitHub commit log analyzer.

This module handles the formatting and generation of reports
based on analyzed commit data.
"""

import json
import os
from collections import defaultdict, Counter
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple

# Import ai_analyzer but handle the case where it might fail
try:
    from .ai_analyzer import analyze_with_ai, init_openai_client
    ai_available = True
except ImportError:
    ai_available = False


def generate_report(commit_data: List[Dict[str, Any]], prompts: Optional[Dict] = None) -> str:
    """
    Generate a comprehensive report from the analyzed commit data.
    
    Args:
        commit_data: The analyzed commit data
        prompts: Optional configuration for AI prompts
        
    Returns:
        Formatted report as a string
    """
    if not commit_data:
        return "No commits found in the specified date range."
    
    # Sort commits by date (newest first, as that's how Git typically presents them)
    sorted_commits = sorted(commit_data, key=lambda x: x['date'], reverse=True)
    
    # Initialize AI if prompts are provided and AI is available
    ai_results = {}
    if prompts and ai_available:
        # The OpenAI client is already initialized in main.py if API key was provided
        # We'll only try to initialize here if it hasn't been done already
        try:
            from .ai_analyzer import openai_client
            if not openai_client:
                # Only initialize if not already initialized
                init_openai_client()
        except ImportError:
            pass
        
        # Get AI analysis
        ai_results = analyze_with_ai(sorted_commits, prompts)
    
    # Check if we have a comprehensive report
    if ai_results and "comprehensive_report" in ai_results and ai_results["comprehensive_report"] != "AI analysis unavailable":
        return _format_comprehensive_report(ai_results["comprehensive_report"], sorted_commits)
    
    # Otherwise, generate the standard report with sections
    sections = []
    
    # 1. Summary section
    sections.append(_generate_summary_section(sorted_commits))
    
    # 2. Chronological progression section
    sections.append(_generate_chronological_section(sorted_commits))
    
    # 3. Time consumption analysis
    sections.append(_generate_time_consumption_section(sorted_commits, ai_results))
    
    # 4. Technical challenges section
    sections.append(_generate_technical_challenges_section(sorted_commits, ai_results))
    
    # 5. AI-powered insights section (if available)
    if ai_results and "progression" in ai_results:
        sections.append(_generate_ai_insights_section(ai_results))
    
    # Combine all sections
    return "\n\n".join(sections)


def _format_comprehensive_report(report: str, commits: List[Dict[str, Any]]) -> str:
    """
    Format the comprehensive report from the AI.
    
    Args:
        report: The comprehensive report from the AI
        commits: The commit data for any additional statistics
        
    Returns:
        Formatted comprehensive report
    """
    # Get repository name from the first commit
    repo_name = ""
    if commits and len(commits) > 0 and 'repo_path' in commits[0]:
        repo_name = os.path.basename(commits[0]['repo_path'])
    
    # Calculate date range for the report
    date_range = ""
    if len(commits) > 0:
        start_date = min(commit['date'] for commit in commits).strftime('%Y-%m-%d')
        end_date = max(commit['date'] for commit in commits).strftime('%Y-%m-%d')
        date_range = f" ({start_date} to {end_date})"
    
    # Add a professional header with metadata
    header = [
        f"# {repo_name} Technical Analysis{date_range}",
        "",
        "_Generated by Git Commit Analyzer with AI-powered insights_",
        "",
        "---",
        ""
    ]
    
    # Add repository stats in a more compact format
    stats = _generate_stats_summary(commits)
    
    # Extract sections from the AI report if needed
    # For most cases, the AI should generate all needed sections
    
    # Combine header, stats, and AI report
    return "\n".join(header + [stats, "", report])


def _generate_stats_summary(commits: List[Dict[str, Any]]) -> str:
    """
    Generate a compact summary of repository statistics.
    
    Args:
        commits: The analyzed commit data
        
    Returns:
        Formatted statistics summary as markdown
    """
    # Calculate basic stats
    total_commits = len(commits)
    unique_authors = set(commit['author'].split('<')[0].strip() for commit in commits)
    total_added = sum(commit['stats']['lines_added'] for commit in commits)
    total_removed = sum(commit['stats']['lines_removed'] for commit in commits)
    
    # Count file types
    file_types = Counter()
    for commit in commits:
        for file_type, count in commit['stats']['file_types'].items():
            file_types[file_type] += count
    
    # Format as a compact summary in markdown format
    summary = [
        "## Repository Statistics",
        "",
        "| Metric | Value |",
        "| ------ | ----- |",
        f"| Total Commits | {total_commits} |",
        f"| Contributors | {len(unique_authors)} |",
        f"| Lines Added | {total_added:,} |",
        f"| Lines Removed | {total_removed:,} |",
        f"| Net Change | {total_added - total_removed:,} lines |",
    ]
    
    # Add file type breakdown if available
    if file_types:
        summary.append("")
        summary.append("### File Types")
        summary.append("")
        summary.append("| Extension | Count |")
        summary.append("| --------- | ----- |")
        for file_type, count in file_types.most_common(10):
            summary.append(f"| {file_type} | {count} |")
    
    return "\n".join(summary)


def _generate_summary_section(commits: List[Dict[str, Any]]) -> str:
    """
    Generate the summary section of the report.
    
    Args:
        commits: The analyzed commit data
        
    Returns:
        Formatted summary section as a string
    """
    # Calculate basic stats
    total_commits = len(commits)
    date_range = f"{commits[-1]['date'].strftime('%Y-%m-%d')} to {commits[0]['date'].strftime('%Y-%m-%d')}"
    
    # Count unique authors
    authors = set(commit['author'].split('<')[0].strip() for commit in commits)
    
    # Count total lines changed
    total_added = sum(commit['stats']['lines_added'] for commit in commits)
    total_removed = sum(commit['stats']['lines_removed'] for commit in commits)
    
    # Count file types
    file_types = Counter()
    for commit in commits:
        for file_type, count in commit['stats']['file_types'].items():
            file_types[file_type] += count
    
    # Format the summary
    summary = [
        "SUMMARY",
        "-------",
        f"Period: {date_range}",
        f"Total Commits: {total_commits}",
        f"Contributors: {len(authors)}",
        f"Lines Added: {total_added}",
        f"Lines Removed: {total_removed}",
        f"Net Change: {total_added - total_removed} lines",
    ]
    
    # Add file type breakdown if available
    if file_types:
        summary.append("\nFile Types Modified:")
        for file_type, count in file_types.most_common(5):
            summary.append(f"  {file_type}: {count} times")
    
    return "\n".join(summary)


def _generate_chronological_section(commits: List[Dict[str, Any]]) -> str:
    """
    Generate the chronological progression section of the report.
    
    Args:
        commits: The analyzed commit data
        
    Returns:
        Formatted chronological section as a string
    """
    # Reverse the commits to show oldest first for chronological progression
    commits_chrono = sorted(commits, key=lambda x: x['date'])
    
    chrono_lines = [
        "\nCHRONOLOGICAL PROGRESSION",
        "------------------------"
    ]
    
    for commit in commits_chrono:
        date_str = commit['date'].strftime('%Y-%m-%d %H:%M')
        short_hash = commit['short_hash']
        
        # Get first line of commit message
        message_first_line = commit['message'].strip().split('\n')[0]
        
        # Add keywords if any
        keywords_str = ""
        if commit['keywords']:
            keywords_str = f" [{', '.join(commit['keywords'])}]"
        
        # Format the commit entry
        commit_entry = f"{date_str} ({short_hash}): {message_first_line}{keywords_str}"
        
        # Add stats
        stats = commit['stats']
        if stats['lines_added'] > 0 or stats['lines_removed'] > 0:
            stats_str = f" +{stats['lines_added']}/-{stats['lines_removed']} in {stats['files_changed']} files"
            chrono_lines.append(f"{commit_entry}\n    {stats_str}")
        else:
            chrono_lines.append(commit_entry)
    
    return "\n".join(chrono_lines)


def _generate_time_consumption_section(commits: List[Dict[str, Any]], 
                                      ai_results: Optional[Dict[str, Any]] = None) -> str:
    """
    Generate the time consumption analysis section of the report.
    
    Args:
        commits: The analyzed commit data
        ai_results: Optional AI analysis results
        
    Returns:
        Formatted time consumption section as a string
    """
    time_lines = [
        "\nTIME CONSUMPTION ANALYSIS",
        "------------------------"
    ]
    
    # Add AI analysis if available
    if ai_results and "time_consumption" in ai_results and ai_results["time_consumption"] != "AI analysis unavailable":
        time_lines.append("\nAI Analysis:")
        time_lines.append(ai_results["time_consumption"])
        time_lines.append("\nStatistical Analysis:")
    
    # Group commits by day
    commits_by_day = defaultdict(list)
    for commit in commits:
        day_str = commit['date'].strftime('%Y-%m-%d')
        commits_by_day[day_str].append(commit)
    
    # Sort days
    sorted_days = sorted(commits_by_day.keys())
    
    # Find days with high activity
    high_activity_days = []
    for day in sorted_days:
        day_commits = commits_by_day[day]
        total_changes = sum(c['stats']['lines_added'] + c['stats']['lines_removed'] for c in day_commits)
        
        if len(day_commits) >= 3 or total_changes >= 100:
            high_activity_days.append((day, len(day_commits), total_changes))
    
    # Add daily activity summary
    time_lines.append("\nDaily Activity:")
    for day in sorted_days:
        day_commits = commits_by_day[day]
        changes = sum(c['stats']['lines_added'] + c['stats']['lines_removed'] for c in day_commits)
        time_lines.append(f"  {day}: {len(day_commits)} commits, {changes} lines changed")
    
    # Add high activity periods
    if high_activity_days:
        time_lines.append("\nHigh Activity Periods:")
        for day, commit_count, changes in sorted(high_activity_days, key=lambda x: x[2], reverse=True):
            time_lines.append(f"  {day}: {commit_count} commits with {changes} lines changed")
            
            # Add the key commits from that day
            day_commits = commits_by_day[day]
            for commit in sorted(day_commits, key=lambda x: x['stats']['lines_added'] + x['stats']['lines_removed'], reverse=True)[:2]:
                message_first_line = commit['message'].strip().split('\n')[0]
                stats = commit['stats']
                time_lines.append(f"    - {commit['short_hash']}: {message_first_line[:50]}... "
                                 f"(+{stats['lines_added']}/-{stats['lines_removed']})")
    
    return "\n".join(time_lines)


def _generate_technical_challenges_section(commits: List[Dict[str, Any]],
                                          ai_results: Optional[Dict[str, Any]] = None) -> str:
    """
    Generate the technical challenges section of the report.
    
    Args:
        commits: The analyzed commit data
        ai_results: Optional AI analysis results
        
    Returns:
        Formatted technical challenges section as a string
    """
    challenge_lines = [
        "\nTECHNICAL CHALLENGES",
        "-------------------"
    ]
    
    # Add AI analysis if available
    if ai_results and "technical_challenges" in ai_results and ai_results["technical_challenges"] != "AI analysis unavailable":
        challenge_lines.append("\nAI-Identified Challenges:")
        challenge_lines.append(ai_results["technical_challenges"])
    else:
        challenge_lines.append("\nNote: This analysis is based on commit messages and code diff patterns. "
                             "For a more detailed analysis, set OPENAI_API_KEY and provide a prompt configuration.")
    
    # Enhanced feature: Add complexity analysis based on commit stats
    complex_commits = []
    for commit in commits:
        # Identify potential complex changes
        stats = commit['stats']
        if stats['lines_added'] + stats['lines_removed'] > 100 or stats['files_changed'] > 5:
            complex_commits.append((
                f"Large change with {stats['files_changed']} files and "
                f"{stats['lines_added'] + stats['lines_removed']} lines modified",
                commit['short_hash'],
                commit['date']
            ))
    
    # Add complexity section if any complex changes found
    if complex_commits:
        challenge_lines.append("\nComplex Changes:")
        
        # Sort instances by date
        sorted_instances = sorted(complex_commits, key=lambda x: x[2], reverse=True)
        
        for desc, commit_hash, date in sorted_instances:
            date_str = date.strftime('%Y-%m-%d')
            challenge_lines.append(f"  - [{date_str}] ({commit_hash}) {desc}")
    
    # Detect potential challenges based on commit messages
    bugfix_commits = []
    refactor_commits = []
    
    for commit in commits:
        # Extract keywords from the commit
        keywords = commit.get('keywords', [])
        
        if 'bugfix' in keywords:
            bugfix_commits.append((
                commit['message'].split('\n')[0],  # Get first line of message
                commit['short_hash'],
                commit['date']
            ))
        
        if 'refactor' in keywords:
            refactor_commits.append((
                commit['message'].split('\n')[0],  # Get first line of message
                commit['short_hash'],
                commit['date']
            ))
    
    # Add bug fixes section
    if bugfix_commits:
        challenge_lines.append("\nBug Fixes:")
        
        # Sort instances by date
        sorted_instances = sorted(bugfix_commits, key=lambda x: x[2], reverse=True)
        
        for desc, commit_hash, date in sorted_instances:
            date_str = date.strftime('%Y-%m-%d')
            challenge_lines.append(f"  - [{date_str}] ({commit_hash}) {desc}")
    
    # Add refactoring section
    if refactor_commits:
        challenge_lines.append("\nRefactoring Efforts:")
        
        # Sort instances by date
        sorted_instances = sorted(refactor_commits, key=lambda x: x[2], reverse=True)
        
        for desc, commit_hash, date in sorted_instances:
            date_str = date.strftime('%Y-%m-%d')
            challenge_lines.append(f"  - [{date_str}] ({commit_hash}) {desc}")
    
    # If no challenges found, add placeholder
    if not complex_commits and not bugfix_commits and not refactor_commits:
        challenge_lines.append("\nNo specific challenges detected in the analyzed commits.")
    
    return "\n".join(challenge_lines)


def _generate_ai_insights_section(ai_results: Dict[str, Any]) -> str:
    """
    Generate an AI insights section based on the AI analysis results.
    
    Args:
        ai_results: The AI analysis results
        
    Returns:
        Formatted AI insights section as a string
    """
    insights_lines = [
        "\nAI-POWERED DEVELOPMENT INSIGHTS",
        "------------------------------"
    ]
    
    # Add progression analysis
    if "progression" in ai_results and ai_results["progression"] != "AI analysis unavailable":
        insights_lines.append("\nDevelopment Progression:")
        insights_lines.append(ai_results["progression"])
    
    # Add note about API key
    if "progression" not in ai_results or ai_results["progression"] == "AI analysis unavailable":
        insights_lines.append("\nNote: AI-powered insights are unavailable. To enable this feature:")
        insights_lines.append("  1. Set the OPENAI_API_KEY environment variable")
        insights_lines.append("  2. Provide a prompt configuration file using --prompt-config")
        insights_lines.append("  3. Ensure the 'openai' package is installed (pip install openai)")
    
    return "\n".join(insights_lines) 