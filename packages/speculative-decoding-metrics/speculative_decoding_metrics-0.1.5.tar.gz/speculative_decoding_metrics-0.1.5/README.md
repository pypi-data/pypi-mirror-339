# Speculative-Decoding-DraftToken-Analysis
#MLX #AppleSilicon #SpeculativeDecoding 
This project analyzes the **performance and quality trade-offs** in speculative decoding using draft tokens and different quantized model configurations. It evaluates:

- âš¡ **Generation speed** (tokens/sec)  
- ğŸ” **Semantic similarity (Accuracy metric)** (cosine similarity via sentence embeddings)  
- ğŸ“ **Text quality (Accuracy metric)** (ROUGE-L score)  
 
---
### â–¶ï¸ Install the pip package
```bash
pip install speculative-decoding-metrics
```
---
### Customize the Speculative decoding analysis
###### After installing the package, create a demo script with a default prompt and model: 
demo/run_example.py
```
from speculative_decoding_metrics.main import run_evaluation

run_evaluation(
    base_model="phi-3-mini-4k-instruct",  #Use a model that'll run on your local
    main_quant="8bit",          #use q8 instead of "8bit" based on HuggingFace Repo name 
    draft_quant="4bit",         #use q4 instead of "4bit" based on HuggingFace Repo name
    prompt="Tell me a bedtime story",
    num_draft_tokens_list=[0, 1, 2, 3, 4]
)

```
## â–¶ï¸ Run the Demo
```bash
python demo/run_example.py
```

---

## ğŸ“Œ What Is Speculative Decoding?

**Speculative decoding** speeds up language generation by using a smaller "draft" model to propose tokens, which are then verified by a larger "main" model.

This repo benchmarks speculative decoding using:

- **Main model**: Quantized to `8bit` â†’ `mlx-community/<model>-8bit`
- **Draft model**: Quantized to `4bit` â†’ `mlx-community/<model>-4bit`

---

## ğŸ“Š Visualized Metrics
Based on these results, the user can decide which speculative decoding they want to run for the best results!
This package generates plots comparing output quality and speed across draft token counts:

1. **Tokens/sec** â€“ Speed boost with draft tokens  
2. **Cosine Similarity** â€“ Semantic match with baseline (no draft)  
3. **ROUGE-L** â€“ Text overlap quality score  

![Results Graph](assets/image.png)

---

## ğŸ›  Customization Tips

- ğŸ”§ **Change the prompt** â€“ Modify the `prompt` in `demo/run_example.py`  
- ğŸ§  **Try other models** â€“ Swap the `base_model` string (e.g., Mistral, TinyLlama)  
- ğŸ›ï¸ **Adjust draft token range** â€“ Modify `num_draft_tokens_list` for finer control  

---

## Acknowledgments

- **Apple MLX** â€“ Lightweight ML framework  
- **HuggingFace** â€“ Transformers + SentenceTransformers 
- **Google Research** â€“ ROUGE scoring tools  
