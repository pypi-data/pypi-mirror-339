name: <<TEMPLATE_NAME>>
<% if RETRY > 0 %>
retryStrategy:
  limit: <<RETRY>>
<% endif %>
container:
  <% if HAS_SECRET %>
  envFrom:
    - secretRef:
        name: <<SECRET_NAME>>
  <% endif %>
  args:
  - |-
    <% if not IS_ERR_TOLER %>
    set -e;
    <% endif %>

    <% if STORAGE_TYPE == STORAGE_ENUM.FIREBASE_STORAGE %>
    echo "$<<SECRET_KEY>>" > /tmp/firebase.json
    export GOOGLE_APPLICATION_CREDENTIALS=/tmp/firebase.json

    function copy_logs {
      <% if HAS_SECRET %>gcloud auth activate-service-account --key-file=$(GOOGLE_APPLICATION_CREDENTIALS)<% endif %>
      gsutil -m cp -r /var/run/argo/ctr/main/combined {{inputs.parameters.upload-base-path}}/{{workflow.name}}/{{pod.name}}/main.log
    };
    trap copy_logs EXIT;

    <% if HAS_SECRET %>gcloud auth activate-service-account --key-file=$(GOOGLE_APPLICATION_CREDENTIALS);<% endif %>
    FILES=($(echo '{{inputs.parameters.files-json}}' | jq -r '.[] | .public_url'))
    for public_url in "${FILES[@]}"; do
    file=$(basename $public_url)
    gsutil cp $public_url /tmp/$file;
    done
    <% endif %>

    <% if STORAGE_TYPE == STORAGE_ENUM.MINIO %>
    if [[ ! "<<STORAGE_ENDPOINT_URL>>" == *"s3"* ]]; then
      echo "Changing AWS_ENDPOINT_URL to: https://<<STORAGE_ENDPOINT_URL>> "
      export AWS_ENDPOINT_URL="https://<<STORAGE_ENDPOINT_URL>>"
    fi

    function copy_logs {
      aws s3 cp /var/run/argo/ctr/main/combined {{inputs.parameters.upload-base-path}}/{{workflow.name}}/{{pod.name}}/main.log
    };
    trap copy_logs EXIT;

    FILES=($(echo '{{inputs.parameters.files-json}}' | jq -r '.[] | .public_url'))
    for public_url in "${FILES[@]}"; do
    file=$(basename $public_url)
    aws s3 cp $public_url /tmp/$file;
    done
    <% endif %>

    # <% if DOWNLOAD_LOGGING %>copy_logs;<% endif %>
  command:
  - /bin/bash
  - -c
  image: << CLOUD_BASE_IMAGE >>
inputs:
  parameters:
  - name: files-json
  - name: upload-base-path
outputs:
  artifacts:
  - name: downloaded-files
    path: /tmp/




# name: <<TEMPLATE_NAME>>
# container:
#   <% if HAS_SECRET %>
#   volumeMounts:
#   - name: <<SECRET_NAME>>
#     mountPath: /etc/storage-auth
#   <% endif %>
#   args:
#   - |-
#     <% if not IS_ERR_TOLER %>
#     set -e;
#     <% endif %>
#     export AWS_ENDPOINT_URL=https://<<STORAGE_ENDPOINT_URL>>
#     <% if STORAGE_TYPE == STORAGE_ENUM.FIREBASE_STORAGE %>
#     function copy_logs {
#       <% if HAS_SECRET %>gcloud auth activate-service-account --key-file=/etc/storage-auth/<<SECRET_KEY>><% endif %>
#       gsutil -m cp -r /var/run/argo/ctr/main/combined {{inputs.parameters.upload-base-path}}/{{workflow.name}}/{{pod.name}}/main.log
#     };
#     trap 'copy_logs; exit 1;' ERR;
#     <% if HAS_SECRET %>gcloud auth activate-service-account --key-file=/etc/storage-auth/<<SECRET_KEY>>;<% endif %>
#     FILES=($(echo '{{inputs.parameters.files-json}}' | jq -r '.[] | .public_url'))
#     for public_url in "${FILES[@]}"; do
#     file=$(basename $public_url)
#     gsutil cp $public_url /tmp/$file;
#     done
#     <% endif %>

#     <% if STORAGE_TYPE == STORAGE_ENUM.MINIO %>
#     function copy_logs {
#       aws s3 cp /var/run/argo/ctr/main/combined {{inputs.parameters.upload-base-path}}/{{workflow.name}}/{{pod.name}}/main.log
#     };
#     trap 'copy_logs; exit 1;' ERR;
#     FILES=($(echo '{{inputs.parameters.files-json}}' | jq -r '.[] | .public_url'))
#     for public_url in "${FILES[@]}"; do
#     file=$(basename $public_url)
#     aws s3 cp $public_url /tmp/$file;
#     done
#     <% endif %>

#     <% if DOWNLOAD_LOGGING %>copy_logs;<% endif %>
#   command:
#   - /bin/bash
#   - -c
#   image: << CLOUD_BASE_IMAGE >>
# inputs:
#   parameters:
#   - name: files-json
#   - name: upload-base-path
# outputs:
#   artifacts:
#   - name: downloaded-files
#     path: /tmp/
