# -*- coding: utf-8 -*-
# Generated by the protocol buffer compiler.  DO NOT EDIT!
# NO CHECKED-IN PROTOBUF GENCODE
# source: model_service.proto
# Protobuf Python Version: 5.29.0
"""Generated protocol buffer code."""
from google.protobuf import descriptor as _descriptor
from google.protobuf import descriptor_pool as _descriptor_pool
from google.protobuf import runtime_version as _runtime_version
from google.protobuf import symbol_database as _symbol_database
from google.protobuf.internal import builder as _builder
_runtime_version.ValidateProtobufRuntimeVersion(
    _runtime_version.Domain.PUBLIC,
    5,
    29,
    0,
    '',
    'model_service.proto'
)
# @@protoc_insertion_point(imports)

_sym_db = _symbol_database.Default()




DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n\x13model_service.proto\x12\rmodel_service\",\n\x0b\x43hatMessage\x12\x0c\n\x04role\x18\x01 \x01(\t\x12\x0f\n\x07\x63ontent\x18\x02 \x01(\t\"\xd3\x03\n\rChatInputItem\x12\x10\n\x08provider\x18\x01 \x01(\t\x12\x12\n\nmodel_name\x18\x02 \x01(\t\x12\x10\n\x08priority\x18\x03 \x01(\x05\x12,\n\x08messages\x18\x04 \x03(\x0b\x32\x1a.model_service.ChatMessage\x12\x13\n\x0btemperature\x18\x05 \x01(\x02\x12\r\n\x05top_p\x18\x06 \x01(\x02\x12\x0e\n\x06stream\x18\x07 \x01(\x08\x12\x12\n\nmax_tokens\x18\x08 \x01(\x05\x12\x0c\n\x04stop\x18\t \x03(\t\x12?\n\nlogit_bias\x18\n \x03(\x0b\x32+.model_service.ChatInputItem.LogitBiasEntry\x12\x0c\n\x04user\x18\x0b \x01(\t\x12\x0f\n\x07user_id\x18\x0c \x01(\t\x12\x0e\n\x06org_id\x18\r \x01(\t\x12\x36\n\x05\x65xtra\x18\x0e \x03(\x0b\x32\'.model_service.ChatInputItem.ExtraEntry\x1a\x30\n\x0eLogitBiasEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12\r\n\x05value\x18\x02 \x01(\x02:\x02\x38\x01\x1a,\n\nExtraEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12\r\n\x05value\x18\x02 \x01(\t:\x02\x38\x01\":\n\x0b\x43hatRequest\x12+\n\x05items\x18\x01 \x03(\x0b\x32\x1c.model_service.ChatInputItem\"O\n\x05Usage\x12\x15\n\rprompt_tokens\x18\x01 \x01(\x05\x12\x19\n\x11\x63ompletion_tokens\x18\x02 \x01(\x05\x12\x14\n\x0ctotal_tokens\x18\x03 \x01(\x05\"e\n\x10\x43hatResponseItem\x12\x0c\n\x04type\x18\x01 \x01(\t\x12\x0f\n\x07\x63ontent\x18\x02 \x01(\t\x12#\n\x05usage\x18\x03 \x01(\x0b\x32\x14.model_service.Usage\x12\r\n\x05\x65rror\x18\x04 \x01(\t\">\n\x0c\x43hatResponse\x12.\n\x05items\x18\x01 \x03(\x0b\x32\x1f.model_service.ChatResponseItem2\x9d\x01\n\x0cModelService\x12G\n\x04\x43hat\x12\x1c.model_service.ChatInputItem\x1a\x1f.model_service.ChatResponseItem0\x01\x12\x44\n\tBatchChat\x12\x1a.model_service.ChatRequest\x1a\x1b.model_service.ChatResponseb\x06proto3')

_globals = globals()
_builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, _globals)
_builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'model_service_pb2', _globals)
if not _descriptor._USE_C_DESCRIPTORS:
  DESCRIPTOR._loaded_options = None
  _globals['_CHATINPUTITEM_LOGITBIASENTRY']._loaded_options = None
  _globals['_CHATINPUTITEM_LOGITBIASENTRY']._serialized_options = b'8\001'
  _globals['_CHATINPUTITEM_EXTRAENTRY']._loaded_options = None
  _globals['_CHATINPUTITEM_EXTRAENTRY']._serialized_options = b'8\001'
  _globals['_CHATMESSAGE']._serialized_start=38
  _globals['_CHATMESSAGE']._serialized_end=82
  _globals['_CHATINPUTITEM']._serialized_start=85
  _globals['_CHATINPUTITEM']._serialized_end=552
  _globals['_CHATINPUTITEM_LOGITBIASENTRY']._serialized_start=458
  _globals['_CHATINPUTITEM_LOGITBIASENTRY']._serialized_end=506
  _globals['_CHATINPUTITEM_EXTRAENTRY']._serialized_start=508
  _globals['_CHATINPUTITEM_EXTRAENTRY']._serialized_end=552
  _globals['_CHATREQUEST']._serialized_start=554
  _globals['_CHATREQUEST']._serialized_end=612
  _globals['_USAGE']._serialized_start=614
  _globals['_USAGE']._serialized_end=693
  _globals['_CHATRESPONSEITEM']._serialized_start=695
  _globals['_CHATRESPONSEITEM']._serialized_end=796
  _globals['_CHATRESPONSE']._serialized_start=798
  _globals['_CHATRESPONSE']._serialized_end=860
  _globals['_MODELSERVICE']._serialized_start=863
  _globals['_MODELSERVICE']._serialized_end=1020
# @@protoc_insertion_point(module_scope)
