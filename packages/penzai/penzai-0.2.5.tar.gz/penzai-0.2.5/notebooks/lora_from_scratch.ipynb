{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DfejMHs4lr8V"
   },
   "source": [
    "*Copyright 2024 The Penzai Authors.*\n",
    "\n",
    "*Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at*\n",
    "\n",
    "> http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "*Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "USGIPdLYDzSo"
   },
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google-deepmind/penzai/blob/main/notebooks/lora_from_scratch.ipynb) [![Open in Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/google-deepmind/penzai/blob/main/notebooks/lora_from_scratch.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PrvmM33zdK-Z"
   },
   "source": [
    "# LoRA From Scratch - Patching Pretrained Models in Penzai\n",
    "\n",
    "Penzai is designed to make it easy to make targeted modifications to neural networks after they have been trained. In this notebook, we'll show how to take Penzai's reference implementation of [Gemma 7B](https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf) open-weights transformer model, patch it to support Low-Rank Adaptation (LoRA [Hu et al. 2021](https://arxiv.org/abs/2106.09685)), and train the new parameters on a toy problem with a hand-written loss function.\n",
    "\n",
    "The goal of this notebook is to show how *you* could implement something like LoRA from scratch in less than a hundred lines of code, starting from a Penzai implementation of a model that doesn't support it already, and without having to fork the existing implementation source code or even modify the pretrained model's configuration. We'll define everything we need as we go and make changes to models interactively. In fact, our implementation will end up being completely modular; we'll start by applying LoRA to a small MLP and then immediately be able to transfer our implementation to Gemma 7B.\n",
    "\n",
    "Let's get started!\n",
    "\n",
    "```{note}\n",
    "This tutorial uses the V2 neural network API, defined in `pz.experimental.v2`.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SHr2rnIL8DzM"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VkW4lYKAu-oR"
   },
   "source": [
    "Before we can get started in earnest, we need to set up the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ozG8ERNavDos"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xmxgAcFQmZkB"
   },
   "source": [
    "To run this notebook, you need a Python environment with `penzai` and its dependencies installed.\n",
    "\n",
    "In Colab or Kaggle, you can install it using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XGZH58j8mPkj"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  import penzai\n",
    "except ImportError:\n",
    "  !pip install penzai[notebook]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iog3oMAMGCMG"
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v26wYYx6QSn3"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import orbax.checkpoint\n",
    "import optax\n",
    "from jax.experimental import mesh_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Mh2mAuiQ4aa"
   },
   "outputs": [],
   "source": [
    "import treescope\n",
    "import penzai\n",
    "from penzai import pz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ljCMQGV00mZ1"
   },
   "outputs": [],
   "source": [
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MQU6QfoZVYOL"
   },
   "outputs": [],
   "source": [
    "from penzai.models import transformer\n",
    "from penzai.models import simple_mlp\n",
    "from penzai.toolshed import token_visualization\n",
    "from penzai.toolshed import basic_training\n",
    "from penzai.toolshed import jit_wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OGzhV5uWvkvB"
   },
   "source": [
    "### Setting up Penzai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TjGkV8F8vmpi"
   },
   "source": [
    "For this tutorial, we'll enable [Treescope](https://treescope.readthedocs.io/en/stable/) (Penzai's companion pretty-printer) as the default IPython pretty-printer. This is recommended when using Penzai in an interactive environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YodWk_jmva_7"
   },
   "outputs": [],
   "source": [
    "treescope.basic_interactive_setup(autovisualize_arrays=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XBk_7yT1EwUi"
   },
   "source": [
    "## Intro to Penzai's declarative combinator design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "87L-xgdcE5CV"
   },
   "source": [
    "We'll start by giving a brief introduction to Penzai's design conventions, and how they make it easy to insert adapters into pretrained models. Let's begin by initializing a small MLP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sTDKRO7YdI32"
   },
   "outputs": [],
   "source": [
    "mlp = simple_mlp.MLP.from_config(\n",
    "    name=\"mlp\",\n",
    "    init_base_rng=jax.random.key(0),\n",
    "    feature_sizes=[8, 32, 32, 8],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UGhVjjC9osuY"
   },
   "source": [
    "Like most Penzai models and layers, this MLP takes named arrays as input and returns them as output. A named array is just a wrapped JAX array where a subset of its positional axes have been tagged with names. (See the [named axes tutorial](named_axes.ipynb) for more info on how to use Penzai's named axis system.)\n",
    "\n",
    "We can call the MLP directly on an array of inputs to run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zs3C95Onoq4Q"
   },
   "outputs": [],
   "source": [
    "%%autovisualize\n",
    "mlp(pz.nx.NamedArray.wrap(jnp.arange(8, dtype=jnp.float32)).tag(\"features\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V0UpSqo4Dsbu"
   },
   "source": [
    "Penzai models are written in a *declarative*, *combinator*-based style. This means that the structure of the model directly matches the sequence of high-level operations that the model will run in its forward pass. Composite models, like our MLP, just hold onto their sublayers in a list and run these sublayers in order. Primitive layers, like `Linear`, hold on to their parameters as attributes instead of reading them from an external parameter dictionary.\n",
    "\n",
    "We can see the sublayers by pretty-printing the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TnW8H7n3EMTi"
   },
   "outputs": [],
   "source": [
    "%%autovisualize\n",
    "mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UBO5r9ZJEPkK"
   },
   "source": [
    "By convention, most of the \"complicated\" logic in Penzai model classes happens when we initialize them, using the `.from_config` method we called earlier. Once the model is built, the pretty-printed representation provides a full specification of everything the model does, and the parameters are stored as direct attributes on the layers that need them. A general design principle of Penzai is \"*what you see is what you get*\"; you should be able to learn everything you need to know about a model by printing it out.\n",
    "\n",
    "In fact, you can click on a pretty-printed output and press `r` to add qualified names to the pretty-printed visualization (try it above!), which will tell you exactly what type each layer has. (If you remove the parameters first using `pz.unbind_params`, you can even copy and paste the pretty-printed output to rebuild the model structure!)\n",
    "\n",
    "Note that many classes are annotated with \"Sequential\", which means they are just an informatively-named sequence of other layers that run one after another. You can also \"flatten\" a model into a list of sublayers that run in sequence, discarding this extra information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kCto92VEGs2J"
   },
   "outputs": [],
   "source": [
    "pz.nn.inline_groups(pz.nn.Sequential([mlp]), lambda _: True, lambda _: True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VXeIPCUwGtHT"
   },
   "source": [
    "And you can freely add new logic as well, even if it wasn't configured in the initial model. For instance, here's how you could insert a new layer that prints out its intermediate activation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zSG1eTVnGyzp"
   },
   "outputs": [],
   "source": [
    "@pz.pytree_dataclass  # <- This tags our class as being a Python dataclass and a JAX pytree node.\n",
    "class DisplayIntermediateValue(pz.nn.Layer):  # <- pz.nn.Layer is the base class of Penzai layers.\n",
    "\n",
    "  def __call__(self, intermediate_value, **unused_side_inputs):\n",
    "    # Show the value:\n",
    "    pz.show(\"Showing an intermediate value:\", intermediate_value)\n",
    "    # And return it unchanged.\n",
    "    return intermediate_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hGwcDOP9oNLF"
   },
   "outputs": [],
   "source": [
    "patched = (\n",
    "    pz.select(mlp)\n",
    "    .at(lambda model: model.sublayers[2])\n",
    "    .insert_after(DisplayIntermediateValue())\n",
    ")\n",
    "pz.select(patched).at_instances_of(DisplayIntermediateValue).show_selection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WaWhMdCKGWhn"
   },
   "source": [
    "`patched` is a *copy* of our model that includes our new layer, and it will run our new logic when the model is called:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ChIKmDvEpRAN"
   },
   "outputs": [],
   "source": [
    "%%autovisualize\n",
    "patched(pz.nx.NamedArray.wrap(jnp.arange(8, dtype=jnp.float32)).tag(\"features\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hvd2rjPdGzAN"
   },
   "source": [
    "This ability makes it remarkably easy to implement adapters like LoRA!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dYltKGTd8AwL"
   },
   "source": [
    "## Building a simple LoRA Layer in Penzai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4SdGouoz8DDU"
   },
   "source": [
    "Low-Rank Adaptation (LoRA) is a parameter-efficient fine-tuning strategy that augments each linear operation in the model with a decomposed low-rank adapter. The original weight matrix is frozen, and two smaller learnable parameter matrices are used to perturb its output. These parameters are kept separate from the original matrix, so gradients of these new parameters can be easily updated in a compute- and memory-efficient way.\n",
    "\n",
    "The effective weight matrix can be decomposed like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WfrJpIBmAEAF"
   },
   "source": [
    "```\n",
    " ┌────────────────┐       ┌─────┐                      \n",
    " │                │       │     │                      \n",
    " │                │       │  A: │   ┌────────────────┐\n",
    " │    W: d*d      │   +   │ d*r │ * │     B: r*d     │\n",
    " │                │       │     │   └────────────────┘\n",
    " │                │       │     │                      \n",
    " └────────────────┘       └─────┘                      \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-GRJO0e0CI1T"
   },
   "source": [
    "Here `W` is the original frozen weight matrix, `A` is a randomly-initialized matrix, and `B` is initialized to zero to ensure that the adapted model is equivalent to the original one at initialization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ro1gMEmoCxsl"
   },
   "source": [
    "To enable LoRA, we'll do three things for each linear layer in our model:\n",
    "- Freeze the original weight,\n",
    "- Initialize our low-rank matrices A and B,\n",
    "- And replace the original linear layer with the composition of W, A, and B.\n",
    "\n",
    "Let's try it out with a simple MLP like the one we built in the last section. We'll just randomly initialize one for demonstration purposes; in a real LoRA adaptation setting we would generally load this from a pre-trained model checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qbp5urEgYF3M"
   },
   "outputs": [],
   "source": [
    "mlp = simple_mlp.MLP.from_config(\n",
    "    name=\"mlp\",\n",
    "    init_base_rng=jax.random.key(0),\n",
    "    feature_sizes=[2, 32, 32, 2],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iAqNF0Kmqw1V"
   },
   "source": [
    "### Step 1: Freeze parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YQajx3Dipyap"
   },
   "source": [
    "We'll start by freezing all the parameters. Learnable parameters are identifiable because they are instances of `pz.Parameter`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pp49AKByG9b2"
   },
   "outputs": [],
   "source": [
    "pz.select(mlp).at_instances_of(pz.Parameter).show_selection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQr5gF-UHhzp"
   },
   "source": [
    "In this case, the parameters are also the JAX PyTree leaves of the model. This is because they are mutable objects, and are designed to be updated by optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BFOvZqq5HqZs"
   },
   "outputs": [],
   "source": [
    "jax.tree_util.tree_leaves(mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PO7WWeimqIo_"
   },
   "source": [
    "If needed, we can extract these parameters while safely handling repeated parameters using the function `pz.unbind_params`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FbMOe-TkH0V7"
   },
   "outputs": [],
   "source": [
    "mlp_with_slots, params = pz.unbind_params(mlp)\n",
    "pz.show(\"mlp_with_slots:\", mlp_with_slots)\n",
    "pz.show(\"params:\", params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OHz3dyJWH7fg"
   },
   "source": [
    "In this case, however, we just need to \"freeze\" the parameters, which makes them immutable. We can do this using `pz.freeze_params`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JDMmpCpHqVkS"
   },
   "outputs": [],
   "source": [
    "frozen_mlp = pz.freeze_params(mlp)\n",
    "frozen_mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EQmBpMfmqnG0"
   },
   "outputs": [],
   "source": [
    "# No more parameters:\n",
    "pz.select(frozen_mlp).at_instances_of(pz.Parameter).get_sequence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5nhqH3unII_U"
   },
   "outputs": [],
   "source": [
    "# Leaves are now ordinary JAX arrays:\n",
    "jax.tree_util.tree_leaves(frozen_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OlndCo72qq0c"
   },
   "source": [
    "### Step 2: Replace `Linear` layers with low-rank adapted versions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NGXp8MbdrFts"
   },
   "source": [
    "Next, we'll replace the Linear layers with implementations of LoRA.\n",
    "\n",
    "In essence, a LoRA block is a sum of two computation paths: one that uses the original linear layer, and one that uses a sequence of two linear operations. This pattern can be directly mapped to one of Penzai's simple built-in combinators, `BranchAndAddTogether`. We can take each linear layer, like this one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "laM_x44Irpsb"
   },
   "outputs": [],
   "source": [
    "frozen_mlp.sublayers[0].sublayers[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oIuLfQ81ryVM"
   },
   "source": [
    "And replace it with a block like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tfPohoo7r0JD"
   },
   "outputs": [],
   "source": [
    "pz.nn.BranchAndAddTogether([\n",
    "    # The original layer with frozen parameters:\n",
    "    pz.nn.NamedGroup(\"Pretrained\", [\n",
    "        frozen_mlp.sublayers[0].sublayers[0],\n",
    "    ]),\n",
    "    # And a low-rank adapter:\n",
    "    pz.nn.NamedGroup(\"Update\", [\n",
    "        pz.nn.Linear.from_config(\n",
    "            name=\"LoRA-A\",\n",
    "            init_base_rng=jax.random.key(1),\n",
    "            input_axes={\"features\": 8},\n",
    "            output_axes={\"lowrank\": 2},\n",
    "        ),\n",
    "        pz.nn.Linear.from_config(\n",
    "            name=\"LoRA-B\",\n",
    "            init_base_rng=jax.random.key(1),\n",
    "            input_axes={\"lowrank\": 2},\n",
    "            output_axes={\"features_out\": 8},\n",
    "            initializer=pz.nn.zero_initializer,\n",
    "        ),\n",
    "    ]),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dtRGWzYEuYl5"
   },
   "source": [
    "Note that the above code is a direct translation of a LoRA block into the structure of our model. The matrices A and B are represented as separate Linear blocks inside the overall combinator, and the order of execution is determined by the positions in the `NamedGroup`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-n5DDQHUu6fR"
   },
   "source": [
    "To simplify the process of making this transformation at every Linear block, we can encapsulate it into a new Layer subclass. Since the computation can already be written as a combination of existing pieces, the idiomatic Penzai approach is to define our new Layer as a subclass of `pz.nn.Sequential`, so that it can be easily flattened (like we did with the MLP) id needed. `Sequential` already defines the necessary attributes and `__call__` method, so we just need to provide a named initializer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JmOUZn5GvLeH"
   },
   "outputs": [],
   "source": [
    "@pz.pytree_dataclass(has_implicitly_inherited_fields=True)\n",
    "class LowRankAdapter(pz.nn.Sequential):\n",
    "\n",
    "  @classmethod\n",
    "  def from_linear(\n",
    "      cls,\n",
    "      linear: pz.nn.Linear,\n",
    "      name: str,\n",
    "      init_base_rng: jax.Array | None,\n",
    "      rank: int,\n",
    "      lowrank_axis: str = \"lowrank\",\n",
    "  ) -> 'LowRankAdapter':\n",
    "    \"\"\"Builds a LoRA layer from a Linear layer.\n",
    "\n",
    "    Args:\n",
    "      linear: The linear layer to adapt.\n",
    "      name: Name for this layer's parameters. Must be globally unique across all\n",
    "        LoRA blocks; we recommend using `jax.tree_util.keystr` or\n",
    "        `pz.pretty_keystr` and setting the name based on the path to the\n",
    "        original Linear layer being replaced.\n",
    "      init_base_rng: The base RNG to use for initializing model parameters.\n",
    "      rank: The rank of the low-rank adapter.\n",
    "      lowrank_axis: The axis name for low-rank adaptation.\n",
    "\n",
    "    Returns:\n",
    "      A LoRA block with uninitialized parameters and the same initial\n",
    "      behavior as `linear`.\n",
    "    \"\"\"\n",
    "    return cls([\n",
    "        pz.nn.BranchAndAddTogether([\n",
    "            pz.nn.NamedGroup(\"Pretrained\", [linear]),\n",
    "            pz.nn.NamedGroup(\n",
    "                \"Update\",\n",
    "                [\n",
    "                    pz.nn.Linear.from_config(\n",
    "                        name=f\"{name}/LoRA_A\",\n",
    "                        init_base_rng=init_base_rng,\n",
    "                        input_axes=linear.input_axes,\n",
    "                        output_axes={lowrank_axis: rank},\n",
    "                        parallel_axes=linear.parallel_axes,\n",
    "                    ),\n",
    "                    pz.nn.Linear.from_config(\n",
    "                        name=f\"{name}/LoRA_B\",\n",
    "                        init_base_rng=init_base_rng,\n",
    "                        input_axes={lowrank_axis: rank},\n",
    "                        output_axes=linear.output_axes,\n",
    "                        parallel_axes=linear.parallel_axes,\n",
    "                        initializer=pz.nn.zero_initializer,\n",
    "                    ),\n",
    "                ],\n",
    "            ),\n",
    "        ])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-6qWE_qKHvFH"
   },
   "source": [
    "Note: Idiomatic Penzai layers generally avoid overriding `__init__`, since dataclasses take their attributes as parameters to `__init__` and we want to ensure the output of the pretty-printer directly corresponds to code we could use to rebuild the model even if we've modified its attributes. When we have nontrivial construction logic, we'll usually define it in a class method like `from_linear` or `from_config` instead.\n",
    "\n",
    "Layer constructors are generally responsible for ensuring their parameter names are unique within a model, and for initializing their parameters when constructed. For this reason, most layer constructors take arguments `name` and `init_base_rng`. (Note that the name is combined with the RNG when initializing each parameter, so we don't need to manually split the RNGs.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Id0dsEtUu12s"
   },
   "source": [
    "\n",
    "\n",
    "The next step is to write a helper function for inserting LoRA blocks into a model. We'll use Penzai's `pretty_keystr` function (a fancier version of `jax.tree_util.keystr`) to ensure each block has a unique name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oiUuqWB69KPV"
   },
   "outputs": [],
   "source": [
    "def loraify_all_linears(model, rank: int, init_base_rng):\n",
    "  return (\n",
    "      pz.select(model)\n",
    "      .at_instances_of(pz.nn.Linear)\n",
    "      .apply(\n",
    "          lambda keypath, lin: LowRankAdapter.from_linear(\n",
    "              lin,\n",
    "              name=\"LoRA:\" + pz.pretty_keystr(keypath, model),\n",
    "              init_base_rng=init_base_rng,\n",
    "              rank=rank,\n",
    "          ),\n",
    "          with_keypath=True,\n",
    "      )\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nSxTRFCFBiDb"
   },
   "source": [
    "Now we can run it on our MLP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y4JiyS9PBlc2"
   },
   "outputs": [],
   "source": [
    "loraified_mlp = loraify_all_linears(\n",
    "    frozen_mlp, rank=2, init_base_rng=jax.random.key(42)\n",
    ")\n",
    "loraified_mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sVMaJL1GB-mA"
   },
   "source": [
    "You can directly check that this transformation is doing the right thing by expanding each `Affine` layer and making sure the `LowRankAdapter` looks right.\n",
    "\n",
    "Note that `loraified_mlp_uninit` is a *copy* of `frozen_mlp` with the requested modifications. In Penzai, transformations of models always return new copies of the model, so you don't have to worry about accidentally making an irreversible change.\n",
    "\n",
    "Only the model *structure* is copied; the JAX arrays still share memory between the models, and any mutable parameters in the original model will also be shared with the new one. In this case, though, we froze the parameters of `frozen_mlp` first, so only the new parameters are mutable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vepcWBORKehy"
   },
   "outputs": [],
   "source": [
    "pz.select(loraified_mlp).at_instances_of(pz.Parameter).get_sequence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vcp8DZHjCxhL"
   },
   "source": [
    "### Step 3: Training the LoRA weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r27WANoeEa3m"
   },
   "source": [
    "We can now train these adapter parameters using Penzai's basic training loop helpers, or use a custom training loop for them. As a demonstration, we'll train this model to implement XOR by only fitting the low-rank adapter parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EePTWC2ZEmFK"
   },
   "outputs": [],
   "source": [
    "\n",
    "def loss_fn(model, rng, state, example_inputs, example_labels):\n",
    "  assert state is None\n",
    "  model_out = model(example_inputs)\n",
    "  log_probs = jax.nn.log_softmax(\n",
    "      model_out.unwrap(\"batch\", \"features\"), axis=-1\n",
    "  )\n",
    "  losses = -log_probs * example_labels\n",
    "  loss = jnp.sum(losses) / 4\n",
    "  return loss, None, {\"loss\": loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-JHLUzcpXKBF"
   },
   "outputs": [],
   "source": [
    "trainer = basic_training.StatefulTrainer.build(\n",
    "    model=loraified_mlp,\n",
    "    optimizer_def=optax.adam(0.1),\n",
    "    root_rng=jax.random.key(42),\n",
    "    loss_fn=loss_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wA2aCwHSXQtW"
   },
   "outputs": [],
   "source": [
    "trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WkO7q6XzXNCG"
   },
   "outputs": [],
   "source": [
    "xor_inputs = pz.nx.wrap(\n",
    "    jnp.array([[-1, -1], [-1, 1], [1, -1], [1, 1]], dtype=jnp.float32),\n",
    "    \"batch\",\n",
    "    \"features\",\n",
    ")\n",
    "xor_labels = jnp.array([[0, 1], [1, 0], [1, 0], [0, 1]], dtype=jnp.float32)\n",
    "\n",
    "for i in range(20):\n",
    "  out = trainer.step(example_inputs=xor_inputs, example_labels=xor_labels)\n",
    "  print(i, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iOosZhMWJnJF"
   },
   "source": [
    "The parameters in the model will be updated in place, allowing us to use the trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NzjoscuYY047"
   },
   "outputs": [],
   "source": [
    "%%autovisualize\n",
    "loraified_mlp(xor_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1j8eKyG6YoHH"
   },
   "outputs": [],
   "source": [
    "%%autovisualize\n",
    "pz.nx.nmap(jnp.argmax)(loraified_mlp(xor_inputs).untag(\"features\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ZqHvuvuYkkb"
   },
   "source": [
    "Looks like it worked!\n",
    "\n",
    "(Note: If you prefer a \"functional\" training loop, you can extract an immutable version of your parameters by calling `pz.unbind_params(loraified_mlp, frozen=True)`, update them yourself, then substitute the immutable parameters back in using `pz.bind_variables`.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T3B0Y9P5oaNc"
   },
   "source": [
    "## Adding LoRA to Gemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lt54N33AZOh5"
   },
   "source": [
    "Let's now try adding LoRA to the Gemma 7B pretrained model. Because of Penzai's compositional design, the implementation in the previous section will just work out of the box!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ISnc54lwJIV"
   },
   "source": [
    "### Loading Gemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PJbVbYBewKnm"
   },
   "source": [
    "We'll start by loading the weights from the Gemma checkpoint. We'll use the 7B checkpoint for this tutorial, and shard it over our local devices using JAX's automatic partitioning. (You can read more about JAX's automatic distributed arrays [on this JAX documentation page](https://jax.readthedocs.io/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization.html).)\n",
    "\n",
    "If you prefer, you can also run this tutorial with the 2B checkpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0X3qrwbK4SvX"
   },
   "source": [
    "You can download the Gemma checkpoints using a Kaggle account and an API key. If you don't have an API key already, you can:\n",
    "\n",
    "1. Visit https://www.kaggle.com/ and create an account if needed.\n",
    "2. Go to your account settings, then the 'API' section.\n",
    "3. Click 'Create new token' to download your key.\n",
    "\n",
    "Next, if you are running this notebook in Google Colab:\n",
    "\n",
    "1. Click the \"key\" symbol on the left toolbar to open the \"Secrets\" tab.\n",
    "2. Add two new secrets, named \"KAGGLE_USERNAME\" and \"KAGGLE_KEY\", and set their values based on the API key you downloaded.\n",
    "3. Run the cell below and grant this notebook access to the secrets you just made.\n",
    "\n",
    "If you are not running this notebook in Google Colab, you can instead run the cell below, input your username and API key in the textboxes, and click the login button."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uNgVlaJl2IbZ"
   },
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "try:\n",
    "  from google.colab import userdata\n",
    "  kagglehub.config.set_kaggle_credentials(\n",
    "      userdata.get(\"KAGGLE_USERNAME\"), userdata.get(\"KAGGLE_KEY\")\n",
    "  )\n",
    "except ImportError:\n",
    "  kagglehub.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n8Oxko4R4yaK"
   },
   "source": [
    "If everything went well, you should see:\n",
    "\n",
    "```\n",
    "Kaggle credentials set.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eUnej-yy5beB"
   },
   "source": [
    "Before downloading Gemma, you will also need to consent to the Gemma Terms of Use. If you haven't done that yet, you can do so here:\n",
    "\n",
    "> https://www.kaggle.com/models/google/gemma/license/consent\n",
    "\n",
    "(Make sure you choose to \"Verify via Kaggle Account\" with the same account you used to log in above!)\n",
    "\n",
    "Once you've agreed to the terms, you can run the next cell to download the Gemma weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BmUAGwXE41la"
   },
   "outputs": [],
   "source": [
    "weights_dir = kagglehub.model_download('google/gemma/Flax/7b')\n",
    "ckpt_path = os.path.join(weights_dir, '7b')\n",
    "vocab_path = os.path.join(weights_dir, 'tokenizer.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9nlbX3C-1sB6"
   },
   "source": [
    "We can then load the SentencePiece vocabulary and restore the checkpointed parameters into JAX using `orbax`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pEXnGmeUGxCK"
   },
   "outputs": [],
   "source": [
    "vocab = spm.SentencePieceProcessor()\n",
    "vocab.Load(vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bn6Xwlk3xlr5"
   },
   "outputs": [],
   "source": [
    "checkpointer = orbax.checkpoint.PyTreeCheckpointer()\n",
    "metadata = checkpointer.metadata(ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VewGmKxMwIbs"
   },
   "outputs": [],
   "source": [
    "n_devices = jax.local_device_count()\n",
    "sharding_devices = mesh_utils.create_device_mesh((n_devices,))\n",
    "sharding = jax.sharding.PositionalSharding(sharding_devices)\n",
    "restore_args = jax.tree_util.tree_map(\n",
    "    lambda m: orbax.checkpoint.ArrayRestoreArgs(\n",
    "        restore_type=jax.Array,\n",
    "        sharding=sharding.reshape((1,) * (len(m.shape) - 1) + (n_devices,))\n",
    "    ),\n",
    "    metadata,\n",
    ")\n",
    "flat_params = checkpointer.restore(ckpt_path, restore_args=restore_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g9amupev6WKj"
   },
   "outputs": [],
   "source": [
    "gemma_model = transformer.variants.gemma.gemma_from_pretrained_checkpoint(\n",
    "    flat_params,\n",
    "    upcast_activations_to_float32=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nxt4YxCZu1fP"
   },
   "outputs": [],
   "source": [
    "del flat_params\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TsInFjJ-Znb-"
   },
   "source": [
    "Here's what the Gemma model looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DGIHE981ZorN"
   },
   "outputs": [],
   "source": [
    "%%autovisualize\n",
    "gemma_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U786gQ5TZC2x"
   },
   "source": [
    "Try clicking the triangle markers to explore the structure of Gemma and look at some of the parameters!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zh2OGC5QZdcW"
   },
   "source": [
    "### Converting Gemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YifL5OPuZfgM"
   },
   "source": [
    "Now we can freeze its parameters and LoRA-ify its linear blocks in the same way that we did for the simple MLP.\n",
    "\n",
    "The Penzai implementation of Gemma uses the same `Linear` layer to implement all of the learnable operations, in both the MLP blocks and the attention blocks. So we'll use a slightly-modified helper function that lets us be more specific about which `Linear` layers we want to replace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C5DTU_XAbepz"
   },
   "outputs": [],
   "source": [
    "def loraify_linears_in_selection(\n",
    "    selection, rank: int, init_base_rng: jax.Array | None,\n",
    "):\n",
    "  model = selection.deselect()\n",
    "  return selection.at_instances_of(pz.nn.Linear).apply(\n",
    "      lambda keypath, lin: LowRankAdapter.from_linear(\n",
    "          lin,\n",
    "          name=\"LoRA:\" + pz.pretty_keystr(keypath, model),\n",
    "          init_base_rng=init_base_rng,\n",
    "          rank=rank,\n",
    "      ),\n",
    "      with_keypath=True,\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CYHT7vR1bpkP"
   },
   "source": [
    "Now we go through and apply each of the transformation steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D7RtmqjbZcy8"
   },
   "outputs": [],
   "source": [
    "# Step 1: Freeze the pretrained parameters.\n",
    "frozen_gemma_model = pz.freeze_params(gemma_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p11gtxQUZ31a"
   },
   "outputs": [],
   "source": [
    "# Step 2: LoRA-ify the Linear blocks. Following Hu et al. (2021), we'll only\n",
    "# LoRA-ify the attention parameters.\n",
    "loraified_gemma_model = loraify_linears_in_selection(\n",
    "    pz.select(frozen_gemma_model).at_instances_of(pz.nn.Attention),\n",
    "    rank=16,\n",
    "    init_base_rng=jax.random.key(123),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mkhQVq7sa8XD"
   },
   "outputs": [],
   "source": [
    "# Step 3 (optional): Look at it to make sure the transformation looks right.\n",
    "pz.select(loraified_gemma_model).at_instances_of(LowRankAdapter).show_selection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jgtlh83Tcb3W"
   },
   "source": [
    "If we wanted, we could have just as easily adapted the MLP layers, by changing\n",
    "```\n",
    ".at_instances_of(gemma.model_core.GemmaAttention)\n",
    "```\n",
    "to\n",
    "```\n",
    ".at_instances_of(gemma.model_core.GemmaFeedForward)\n",
    "```\n",
    "We could have also customized which blocks have LoRA parameters by using other features of Penzai's selector system (see the separate [selectors tutorial](selectors.ipynb) for more details)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wz8hTWZQdZji"
   },
   "source": [
    "### Fine-tuning Gemma with LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fciyNWtkdcVg"
   },
   "source": [
    "We can now fine-tune our LoRA-ified Gemma model! For this tutorial, we'll just generate some synthetic data. Specifically, we'll show it some examples of evaluating a mysterious function, and train it to figure out what the function does. We won't worry too much about efficiency of the data pipeline, since our goal is just to show how LoRA fine-tuning could work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W-qyTC9QaNx-"
   },
   "outputs": [],
   "source": [
    "def mystery_function(a, b):\n",
    "  return a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xgoxBZN0e-6G"
   },
   "outputs": [],
   "source": [
    "def generate_example(np_rng):\n",
    "  a, b = np_rng.choice(1000, size=(2,))\n",
    "  c = mystery_function(a, b)\n",
    "  return f\">>> mystery_function({a}, {b})\\n{c}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Rjvtb08fW46"
   },
   "outputs": [],
   "source": [
    "def tokenize_batch(examples, pad_length=32, include_eos=True):\n",
    "  padded_tokens = []\n",
    "  for example in examples:\n",
    "    example_tokens = [vocab.bos_id()] + vocab.EncodeAsIds(example)\n",
    "    if include_eos:\n",
    "      example_tokens = example_tokens + [vocab.eos_id()]\n",
    "    assert len(example_tokens) <= pad_length\n",
    "    # Pad from the right (simplifies input positional embeddings)\n",
    "    example_tokens = (\n",
    "        example_tokens\n",
    "        + [vocab.pad_id()] * (pad_length - len(example_tokens))\n",
    "    )\n",
    "    padded_tokens.append(example_tokens)\n",
    "  return pz.nx.wrap(jnp.array(padded_tokens)).tag(\"batch\", \"seq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0_-80NmZgsyR"
   },
   "source": [
    "Penzai has some useful utilities for visualizing token arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "owQzY2q-gDKi"
   },
   "outputs": [],
   "source": [
    "%%autovisualize treescope.ArrayAutovisualizer.for_tokenizer(vocab)\n",
    "np_rng = np.random.default_rng(123)\n",
    "input_examples = tokenize_batch([generate_example(np_rng) for _ in range(20)])\n",
    "input_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gTWG27KmgWTX"
   },
   "outputs": [],
   "source": [
    "token_visualization.show_token_array(input_examples, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Zy1I7KWjTNL"
   },
   "source": [
    "Let's train our new parameters on this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VScBxB7-jebR"
   },
   "outputs": [],
   "source": [
    "def xent_loss_fn(model, rng, state, input_examples):\n",
    "  del rng, state  # Unused.\n",
    "  # Run the model on shifted examples.\n",
    "  tokens_without_last = input_examples[{\"seq\": pz.slice[:-1]}]\n",
    "  outputs = model(tokens_without_last)\n",
    "  # Compute log-probabilities along the \"vocabulary\" axis.\n",
    "  all_log_probs = pz.nx.nmap(jax.nn.log_softmax)(\n",
    "      outputs.untag(\"vocabulary\")\n",
    "  ).tag(\"vocabulary\")\n",
    "  # Index by the correct tokens.\n",
    "  correct_next_tokens = input_examples[{\"seq\": pz.slice[1:]}]\n",
    "  correct_log_probs = all_log_probs[{\"vocabulary\": correct_next_tokens}]\n",
    "  # Mask padding tokens.\n",
    "  correct_log_probs = pz.nx.nmap(jnp.where)(\n",
    "      correct_next_tokens == vocab.pad_id(),\n",
    "      0.0,\n",
    "      correct_log_probs,\n",
    "  )\n",
    "  # Take averages.\n",
    "  loss = -correct_log_probs.untag(\"batch\", \"seq\").unwrap().mean()\n",
    "  return loss, None, {\"loss\": loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F9aSti2QNqMn"
   },
   "outputs": [],
   "source": [
    "trainer = basic_training.StatefulTrainer.build(\n",
    "    model=loraified_gemma_model,\n",
    "    optimizer_def=optax.adamw(5e-5, weight_decay=0.01),\n",
    "    root_rng=jax.random.key(42),\n",
    "    loss_fn=xent_loss_fn,\n",
    "    donate_states=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vwYyQxUun196"
   },
   "outputs": [],
   "source": [
    "# Train on 200 batches of 16 examples -> 3,200 examples\n",
    "# (For reference, there are 1000 * 1000 = 1,000,000 possible examples in the\n",
    "# synthetic distribution we are using.)\n",
    "print_steps = {*range(10), *range(10, 200, 10)}\n",
    "while trainer.state.value.step < 200:\n",
    "  input_examples = tokenize_batch([\n",
    "      generate_example(np_rng) for _ in range(16)\n",
    "  ])\n",
    "  out = trainer.step(input_examples=input_examples)\n",
    "  if int(trainer.state.value.step) in print_steps:\n",
    "    print(trainer.state.value.step, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vxdrFk633e3J"
   },
   "source": [
    "To see what the model learned, we can pull out the model from the train state and look at its parameters. In this case, all of the learnable parameters were added by our LoRA adapter.\n",
    "\n",
    "We'll turn on the autovisualizer so that we can see the distribution of values in the arrays at a glance; try clicking on a few to expand their visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Hd8chYFAJH4"
   },
   "outputs": [],
   "source": [
    "%%autovisualize\n",
    "_, params = pz.unbind_params(loraified_gemma_model)\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FOVwcpOWBW9Z"
   },
   "source": [
    "Recall that we initialized all of the \"B\" matrices to zero. So the fact that they are no longer zero indicates that the model has definitely learned something!\n",
    "\n",
    "But has it learned what we wanted? Let's try running it on a randomly sampled batch of examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6_hGorZjAIHA"
   },
   "outputs": [],
   "source": [
    "%%autovisualize treescope.ArrayAutovisualizer.for_tokenizer(vocab)\n",
    "np_rng = np.random.default_rng(98765)\n",
    "validation_examples = tokenize_batch([generate_example(np_rng) for _ in range(32)])\n",
    "validation_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8i8TUnkWDDu9"
   },
   "outputs": [],
   "source": [
    "token_visualization.show_token_array(validation_examples, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H6d-z4mnCNuT"
   },
   "outputs": [],
   "source": [
    "tokens_without_last = validation_examples[{\"seq\": pz.slice[:-1]}]\n",
    "outputs = loraified_gemma_model(tokens_without_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cy35m0sLCoJt"
   },
   "outputs": [],
   "source": [
    "# Compute log-probabilities along the \"vocabulary\" axis.\n",
    "all_log_probs = pz.nx.nmap(jax.nn.log_softmax)(\n",
    "    outputs.untag(\"vocabulary\")\n",
    ").tag(\"vocabulary\")\n",
    "\n",
    "# Index by the correct tokens.\n",
    "correct_next_tokens = validation_examples[{\"seq\": pz.slice[1:]}]\n",
    "correct_log_probs = all_log_probs[{\"vocabulary\": correct_next_tokens}]\n",
    "\n",
    "# Plot the probability of the correct digit.\n",
    "# This uses the same renderer as %%autovisualize, but doesn't truncate the array\n",
    "# and lets us mask out elements.\n",
    "treescope.render_array(\n",
    "    pz.nx.nmap(jnp.exp)(correct_log_probs),\n",
    "    valid_mask=(correct_next_tokens != vocab.pad_id()),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uo6BmNQxFZn1"
   },
   "source": [
    "We can see that the model is predicting the arguments to `mystery_function` with about 10% accuracy, which is reasonable because those digits are random. It also seems to be almost perfectly accurate on the answers, indicating that it has successfully fit the distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ogdBDPpPd92G"
   },
   "source": [
    "## Running inference on our LoRA-ified model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9pHt23-PBGWI"
   },
   "source": [
    "Now that we've fine-tuned the model, we can convert it into decoding mode and sample from it.\n",
    "\n",
    "In Penzai, autoregressive decoding is performed by a separate class `KVCachingTransformerLM`, instead of being an alternative mode of `Transformer`. This is an instance of a more general pattern in Penzai models: each model and layer does a single thing at runtime, instead of doing different things depending on what arguments you pass. In fact, idiomatic Penzai layers always define a single function `__call__`, and that function always takes a single positional argument (although that argument can be a dictionary or tuple if needed) along with keyword \"side inputs\". This makes it easy to compose many layers together in a uniform way without having to worry about how to handle function arguments.\n",
    "\n",
    "The decoding mode transformation is actually very similar to the LoRA adaptation transformation we defined above. Instead of replacing `Linear` blocks with new `LowRankAdapter` blocks (which have new parameters), this transformation replaces `Attention` blocks with `KVCachingAttention` blocks (which have new state variables)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IEXkCzVCf8HO"
   },
   "source": [
    "Since the key-value caching for Gemma is itself implemented as a patching transformation, this means that key-value caching can be immediately applied to our final `train_state.model` even though we've already edited the model structure to add new adapted parameters. Our modifications don't conflict with the attention block structure, so the modifications can be easily composed.\n",
    "\n",
    "Here's how we can enable decoding mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TfX60kwYhVU9"
   },
   "outputs": [],
   "source": [
    "finetuned_inference_model = (\n",
    "    transformer.sampling_mode.KVCachingTransformerLM.from_uncached(\n",
    "        loraified_gemma_model, cache_len=64, batch_axes={\"batch\": 4},\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kirZHkTXhqQk"
   },
   "source": [
    "Let's look inside to see the changes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nFO003K_hr_3"
   },
   "outputs": [],
   "source": [
    "# You can use a function to pick out an initial node to expand in the\n",
    "# visualization. (You can also copy such a function by clicking the grey copy\n",
    "# icon at the end of each line.)\n",
    "pz.select(finetuned_inference_model).at(\n",
    "    (lambda root: root.body.sublayers[5].sublayers[0].delta.sublayers[1].kv_cache)\n",
    ").show_value()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f_TYfG_VhxGW"
   },
   "source": [
    "The `LowRankAdapter` classes we inserted are still there in the model, but there have been a few other changes to the model structure:\n",
    "- The outermost class is of a different type `KVCachingTransformer`.\n",
    "- This outer class also has new attributes, which track metadata and state necessary for sampling.\n",
    "- Inside each of the transformer blocks, the `Attention` layers have been replaced with new `KVCachingAttention` layers that also have internal state variables.\n",
    "\n",
    "Like `Parameter` objects, `StateVariable` objects are mutable. The difference is that `StateVariable`s are intended to be modified while the model runs. You can unbind them using `pz.unbind_state_vars` and rebind them using `pz.bind_variables` (which works for both parameters and state variables)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D4nRm6JyiY-t"
   },
   "source": [
    "Now that we've converted the model, we can use some existing helper functions to sample from it. We'll also wrap our model in `Jitted` so that it JIT-compiles itself whenever it is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o-7AvEYO_owI"
   },
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \">>> mystery_function(123, 123)\",\n",
    "    \">>> mystery_function(101, 15)\",\n",
    "    \">>> mystery_function(999, 876)\",\n",
    "    \">>>\", # Let the model write and solve its own problem\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "apdnwcKv_r1b"
   },
   "outputs": [],
   "source": [
    "%%autovisualize treescope.ArrayAutovisualizer.for_tokenizer(vocab)\n",
    "tokenized_prompts = tokenize_batch(prompts, 16, include_eos=False)\n",
    "tokenized_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KeGSSlT63NR5"
   },
   "outputs": [],
   "source": [
    "samples = transformer.simple_decoding_loop.temperature_sample_pyloop(\n",
    "    (\n",
    "        pz.select(finetuned_inference_model)\n",
    "        .at(lambda root: root.body)\n",
    "        .apply(jit_wrapper.Jitted)\n",
    "    ),\n",
    "    prompt=tokenized_prompts,\n",
    "    rng=jax.random.key(3),\n",
    "    max_sampling_steps=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YIxoMT_-ASIi"
   },
   "outputs": [],
   "source": [
    "%%autovisualize treescope.ArrayAutovisualizer.for_tokenizer(vocab)\n",
    "pz.show(samples)\n",
    "token_visualization.show_token_array(samples, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aj6L5Cr0BhAC"
   },
   "source": [
    "As desired, our fine-tuned model seems to have learned the behavior of `mystery_function` using the low-rank updates to its parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FJJy4aY-C1Oo"
   },
   "source": [
    "## Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XfUSaDeKDOV-"
   },
   "source": [
    "This notebook demonstrates how Penzai makes it easy to edit the structure of a pretrained model without requiring any changes to the original model implementation. Our `LowRankAdapter` class and associated utilities took less than a hundred lines of code, and were immediately compatible with the pretrained Gemma 7B model, including both training and sampling modes.\n",
    "\n",
    "The definitions in this notebook are also available in `penzai.toolshed.lora`, and can be imported from there if you are interested in using Penzai to perform parameter-efficient fine-tuning.\n",
    "\n",
    "However, LoRA is just one example of what you can do with Penzai's powerful patching and model rewriting utilities. The key-value caching transformation is another, which we've seen above. And these tools can also be used to study intermediate activations and perform targeted counterfactual interventions to specific layers in the model, which we discuss in the [\"Induction Heads\" tutorial](induction_heads.ipynb). Penzai is designed to simplify the general process of editing, visualizing, and analyzing pretrained models; the goal is not to implement every possible type of fine-tuning or patching, but instead to give you powerful general-purpose tools and then get out of your way.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "LoRA From Scratch - Patching Pretrained Models in Penzai",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
