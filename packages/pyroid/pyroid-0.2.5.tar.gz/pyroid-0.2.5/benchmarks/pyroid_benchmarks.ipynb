# Pyroid Performance Benchmarks


This notebook demonstrates the performance advantages of Pyroid compared to pure Python implementations.
import time

import random

import re

import numpy as np

import matplotlib.pyplot as plt

import seaborn as sns



# Set up matplotlib style

plt.style.use('seaborn-v0_8-whitegrid')

sns.set_palette("colorblind")



# Try to import pyroid

try:

    import pyroid

    PYROID_AVAILABLE = True

except ImportError:

    print("Warning: pyroid not found. Please install pyroid to run benchmarks.")

    PYROID_AVAILABLE = False
## Benchmarking Utilities
def benchmark(func, *args, **kwargs):

    """Simple benchmarking function."""

    start_time = time.time()

    result = func(*args, **kwargs)

    end_time = time.time()

    duration_ms = (end_time - start_time) * 1000

    return result, duration_ms



def plot_comparison(title, results):

    """Plot a comparison of benchmark results."""

    plt.figure(figsize=(10, 6))

    

    names = list(results.keys())

    durations = [results[name] for name in names]

    

    # Set colors based on implementation

    colors = []

    for name in names:

        if "Python" in name:

            colors.append("#1f77b4")  # Blue

        elif "NumPy" in name:

            colors.append("#ff7f0e")  # Orange

        elif "pyroid" in name:

            colors.append("#2ca02c")  # Green

        else:

            colors.append("#d62728")  # Red

    

    bars = plt.bar(names, durations, color=colors)

    

    # Add duration labels on top of bars

    for bar in bars:

        height = bar.get_height()

        plt.text(bar.get_x() + bar.get_width()/2., height + 0.01 * max(durations),

                f"{height:.1f}ms",

                ha='center', va='bottom', rotation=0)

    

    # Add speedup labels for pyroid

    if "Pure Python" in results and "pyroid" in results:

        speedup = results["Pure Python"] / results["pyroid"]

        plt.text(names.index("pyroid"), results["pyroid"] / 2,

                f"{speedup:.1f}x faster",

                ha='center', va='center', color='white', fontweight='bold')

    

    plt.title(title, fontsize=16)

    plt.ylabel("Time (ms)", fontsize=12)

    plt.yscale("log")

    plt.tight_layout()

    plt.show()
## 1. Math Operations
def run_sum_benchmark(size=1_000_000):

    """Benchmark summing a large list of numbers."""

    print(f"Generating {size:,} random numbers...")

    numbers = [random.random() for _ in range(size)]

    print("Data generation complete.\n")

    

    results = {}

    

    # Pure Python sum

    print("Running Pure Python sum...")

    python_result, python_duration = benchmark(sum, numbers)

    print(f"Result: {python_result}")

    print(f"Time: {python_duration:.2f}ms")

    results["Pure Python"] = python_duration

    

    # NumPy sum

    print("\nRunning NumPy sum...")

    numpy_result, numpy_duration = benchmark(np.sum, numbers)

    print(f"Result: {numpy_result}")

    print(f"Time: {numpy_duration:.2f}ms")

    results["NumPy"] = numpy_duration

    

    # pyroid sum

    if PYROID_AVAILABLE:

        print("\nRunning pyroid parallel_sum...")

        pyroid_result, pyroid_duration = benchmark(pyroid.parallel_sum, numbers)

        print(f"Result: {pyroid_result}")

        print(f"Time: {pyroid_duration:.2f}ms")

        results["pyroid"] = pyroid_duration

        

        # Calculate speedups

        print(f"\nSpeedup vs Python: {python_duration / pyroid_duration:.1f}x")

        print(f"Speedup vs NumPy: {numpy_duration / pyroid_duration:.1f}x")

    

    # Plot the results

    plot_comparison(f"Sum {size:,} Numbers", results)

    

    return results



# Run the benchmark

sum_results = run_sum_benchmark()
## 2. String Operations
def run_regex_benchmark(size=100_000):

    """Benchmark regex replacement on a large text."""

    print(f"Generating text with {size:,} repetitions...")

    text = "Hello world! " * size

    print(f"Text length: {len(text):,} characters")

    print("Data generation complete.\n")

    

    results = {}

    

    # Pure Python regex

    print("Running Pure Python regex replacement...")

    python_result, python_duration = benchmark(lambda t: re.sub(r"Hello", "Hi", t), text)

    print(f"Result length: {len(python_result):,} characters")

    print(f"Time: {python_duration:.2f}ms")

    results["Pure Python"] = python_duration

    

    # pyroid regex

    if PYROID_AVAILABLE:

        print("\nRunning pyroid parallel_regex_replace...")

        pyroid_result, pyroid_duration = benchmark(pyroid.parallel_regex_replace, text, r"Hello", "Hi")

        print(f"Result length: {len(pyroid_result):,} characters")

        print(f"Time: {pyroid_duration:.2f}ms")

        results["pyroid"] = pyroid_duration

        

        # Calculate speedup

        print(f"\nSpeedup vs Python: {python_duration / pyroid_duration:.1f}x")

    

    # Plot the results

    plot_comparison(f"Regex Replace {len(text):,} Characters", results)

    

    return results



# Run the benchmark

regex_results = run_regex_benchmark()
## 3. Data Operations
def run_sort_benchmark(size=1_000_000):

    """Benchmark sorting a large list."""

    print(f"Generating {size:,} random integers...")

    data = [random.randint(1, 1000000) for _ in range(size)]

    print("Data generation complete.\n")

    

    results = {}

    

    # Pure Python sort

    print("Running Pure Python sort...")

    python_result, python_duration = benchmark(sorted, data)

    print(f"Result length: {len(python_result):,} items")

    print(f"First 5 items: {python_result[:5]}")

    print(f"Time: {python_duration:.2f}ms")

    results["Pure Python"] = python_duration

    

    # pyroid sort

    if PYROID_AVAILABLE:

        print("\nRunning pyroid parallel_sort...")

        pyroid_result, pyroid_duration = benchmark(lambda d: pyroid.parallel_sort(d, None, False), data)

        print(f"Result length: {len(pyroid_result):,} items")

        print(f"First 5 items: {pyroid_result[:5]}")

        print(f"Time: {pyroid_duration:.2f}ms")

        results["pyroid"] = pyroid_duration

        

        # Calculate speedup

        print(f"\nSpeedup vs Python: {python_duration / pyroid_duration:.1f}x")

    

    # Plot the results

    plot_comparison(f"Sort {size:,} Items", results)

    

    return results



# Run the benchmark

sort_results = run_sort_benchmark()
## 4. Real-world Scenario: Data Processing Pipeline
def run_data_pipeline_benchmark(size=500_000):

    """Benchmark a data processing pipeline."""

    print(f"Generating {size:,} records of test data...")

    data = [{"id": i, "value": random.random(), "category": random.choice(["A", "B", "C", "D"])} for i in range(size)]

    print("Data generation complete.\n")

    

    results = {}

    

    # Pure Python implementation

    print("Running Pure Python data pipeline...")

    

    def python_pipeline(data):

        # Step 1: Filter records where value > 0.5

        filtered = [item for item in data if item["value"] > 0.5]

        

        # Step 2: Transform values (multiply by 10)

        transformed = [{"id": item["id"], "value": item["value"] * 10, "category": item["category"]} for item in filtered]

        

        # Step 3: Group by category

        grouped = {}

        for item in transformed:

            category = item["category"]

            if category not in grouped:

                grouped[category] = []

            grouped[category].append(item)

        

        # Step 4: Aggregate

        results = []

        for category, items in grouped.items():

            total = sum(item["value"] for item in items)

            count = len(items)

            results.append({"category": category, "total": total, "count": count, "average": total / count})

        

        # Step 5: Sort by average

        results.sort(key=lambda x: x["average"], reverse=True)

        

        return results

    

    python_result, python_duration = benchmark(python_pipeline, data)

    print(f"Result: {len(python_result)} categories")

    print(f"Time: {python_duration:.2f}ms")

    results["Pure Python"] = python_duration

    

    # pyroid implementation

    if PYROID_AVAILABLE:

        print("\nRunning pyroid data pipeline...")

        

        def pyroid_pipeline(data):

            # Step 1: Filter records where value > 0.5

            filtered = pyroid.parallel_filter(data, lambda item: item["value"] > 0.5)

            

            # Step 2: Transform values (multiply by 10)

            transformed = pyroid.parallel_map(filtered, lambda item: {"id": item["id"], "value": item["value"] * 10, "category": item["category"]})

            

            # Step 3: Group by category (still using Python as pyroid doesn't have a direct equivalent)

            grouped = {}

            for item in transformed:

                category = item["category"]

                if category not in grouped:

                    grouped[category] = []

                grouped[category].append(item)

            

            # Step 4: Aggregate using pyroid for each group

            results = []

            for category, items in grouped.items():

                values = pyroid.parallel_map(items, lambda item: item["value"])

                total = pyroid.parallel_sum(values)

                count = len(items)

                results.append({"category": category, "total": total, "count": count, "average": total / count})

            

            # Step 5: Sort by average

            results = pyroid.parallel_sort(results, lambda x: x["average"], True)

            

            return results

        

        pyroid_result, pyroid_duration = benchmark(pyroid_pipeline, data)

        print(f"Result: {len(pyroid_result)} categories")

        print(f"Time: {pyroid_duration:.2f}ms")

        results["pyroid"] = pyroid_duration

        

        # Calculate speedup

        print(f"\nSpeedup vs Python: {python_duration / pyroid_duration:.1f}x")

    

    # Plot the results

    plot_comparison(f"Data Processing Pipeline ({size:,} records)", results)

    

    return results



# Run the benchmark

pipeline_results = run_data_pipeline_benchmark()

## 5. DataFrame Operations

def run_dataframe_benchmark(size=100_000):
    """Benchmark DataFrame operations."""
    print(f"Generating DataFrame with {size:,} rows...")
    
    # Generate test data
    df = {
        'A': [random.random() for _ in range(size)],
        'B': [random.random() for _ in range(size)],
        'C': [random.random() for _ in range(size)]
    }
    
    # Convert to pandas DataFrame for comparison
    import pandas as pd
    pandas_df = pd.DataFrame(df)
    
    print("Data generation complete.\n")
    
    results = {}
    
    # Define a function to apply
    def square(x):
        return [val * val for val in x]
    
    # Pandas apply
    print("Running pandas apply...")
    pandas_result, pandas_duration = benchmark(lambda: pandas_df.apply(lambda x: x ** 2))
    print(f"Result shape: {pandas_result.shape}")
    print(f"Time: {pandas_duration:.2f}ms")
    results["pandas"] = pandas_duration
    
    # pyroid apply
    if PYROID_AVAILABLE:
        print("\nRunning pyroid dataframe_apply...")
        pyroid_result, pyroid_duration = benchmark(pyroid.dataframe_apply, df, square, 0)
        print(f"Result: {len(pyroid_result)} columns")
        print(f"Time: {pyroid_duration:.2f}ms")
        results["pyroid"] = pyroid_duration
        
        # Calculate speedup
        print(f"\nSpeedup vs pandas: {pandas_duration / pyroid_duration:.1f}x")
    
    # Plot the results
    plot_comparison(f"DataFrame Apply ({size:,} rows)", results)
    
    # GroupBy benchmark
    print("\n\nRunning GroupBy benchmark...")
    
    # Generate test data with groups
    group_df = {
        'category': [random.choice(['A', 'B', 'C', 'D', 'E']) for _ in range(size)],
        'value1': [random.random() * 100 for _ in range(size)],
        'value2': [random.random() * 100 for _ in range(size)]
    }
    
    # Convert to pandas DataFrame for comparison
    pandas_group_df = pd.DataFrame(group_df)
    
    group_results = {}
    
    # Pandas groupby
    print("Running pandas groupby...")
    pandas_group_result, pandas_group_duration = benchmark(
        lambda: pandas_group_df.groupby('category').agg({
            'value1': 'mean',
            'value2': 'sum'
        })
    )
    print(f"Result shape: {pandas_group_result.shape}")
    print(f"Time: {pandas_group_duration:.2f}ms")
    group_results["pandas"] = pandas_group_duration
    
    # pyroid groupby
    if PYROID_AVAILABLE:
        print("\nRunning pyroid dataframe_groupby_aggregate...")
        agg_dict = {'value1': 'mean', 'value2': 'sum'}
        pyroid_group_result, pyroid_group_duration = benchmark(
            pyroid.dataframe_groupby_aggregate, group_df, ['category'], agg_dict
        )
        print(f"Result: {len(pyroid_group_result)} columns")
        print(f"Time: {pyroid_group_duration:.2f}ms")
        group_results["pyroid"] = pyroid_group_duration
        
        # Calculate speedup
        print(f"\nSpeedup vs pandas: {pandas_group_duration / pyroid_group_duration:.1f}x")
    
    # Plot the results
    plot_comparison(f"DataFrame GroupBy ({size:,} rows)", group_results)
    
    return results, group_results

# Run the benchmark
df_apply_results, df_groupby_results = run_dataframe_benchmark()

## 6. Machine Learning Operations

def run_ml_benchmark(n_points=2000, n_features=10):
    """Benchmark machine learning operations."""
    print(f"Generating {n_points} points with {n_features} dimensions...")
    
    # Generate random points
    points = [[random.random() for _ in range(n_features)] for _ in range(n_points)]
    points_np = np.array(points)
    
    print("Data generation complete.\n")
    
    results = {}
    
    # Distance matrix benchmark
    print("Running distance matrix benchmark...")
    
    # NumPy distance matrix
    print("Running NumPy distance matrix...")
    def numpy_distance_matrix(points):
        n = len(points)
        dist_matrix = np.zeros((n, n))
        for i in range(n):
            for j in range(i+1, n):
                dist = np.sqrt(np.sum((points[i] - points[j]) ** 2))
                dist_matrix[i, j] = dist
                dist_matrix[j, i] = dist
        return dist_matrix
    
    numpy_result, numpy_duration = benchmark(numpy_distance_matrix, points_np)
    print(f"Result shape: {numpy_result.shape}")
    print(f"Time: {numpy_duration:.2f}ms")
    results["NumPy"] = numpy_duration
    
    # pyroid distance matrix
    if PYROID_AVAILABLE:
        print("\nRunning pyroid parallel_distance_matrix...")
        pyroid_result, pyroid_duration = benchmark(pyroid.parallel_distance_matrix, points, "euclidean")
        print(f"Result shape: ({len(pyroid_result)}, {len(pyroid_result[0])})")
        print(f"Time: {pyroid_duration:.2f}ms")
        results["pyroid"] = pyroid_duration
        
        # Calculate speedup
        print(f"\nSpeedup vs NumPy: {numpy_duration / pyroid_duration:.1f}x")
    
    # Plot the results
    plot_comparison(f"Distance Matrix ({n_points} points)", results)
    
    # Feature scaling benchmark
    print("\n\nRunning feature scaling benchmark...")
    
    # Generate random data
    data = [[random.gauss(0, 10) for _ in range(n_features)] for _ in range(n_points)]
    data_np = np.array(data)
    
    scaling_results = {}
    
    # Scikit-learn StandardScaler
    from sklearn.preprocessing import StandardScaler
    print("Running Scikit-learn StandardScaler...")
    scaler = StandardScaler()
    sklearn_result, sklearn_duration = benchmark(lambda: scaler.fit_transform(data_np))
    print(f"Result shape: {sklearn_result.shape}")
    print(f"Time: {sklearn_duration:.2f}ms")
    scaling_results["Scikit-learn"] = sklearn_duration
    
    # pyroid feature scaling
    if PYROID_AVAILABLE:
        print("\nRunning pyroid parallel_feature_scaling...")
        pyroid_result, pyroid_duration = benchmark(pyroid.parallel_feature_scaling, data, "standard")
        print(f"Result shape: ({len(pyroid_result)}, {len(pyroid_result[0])})")
        print(f"Time: {pyroid_duration:.2f}ms")
        scaling_results["pyroid"] = pyroid_duration
        
        # Calculate speedup
        print(f"\nSpeedup vs Scikit-learn: {sklearn_duration / pyroid_duration:.1f}x")
    
    # Plot the results
    plot_comparison(f"Feature Scaling ({n_points} samples)", scaling_results)
    
    return results, scaling_results

# Run the benchmark
ml_distance_results, ml_scaling_results = run_ml_benchmark()

## 7. Text and NLP Operations

def run_text_nlp_benchmark(n_texts=5000, words_per_text=50):
    """Benchmark text and NLP operations."""
    print(f"Generating {n_texts} texts with approximately {words_per_text} words each...")
    
    # Generate random texts
    import string
    def generate_random_text(word_count, word_length=5):
        words = []
        for _ in range(word_count):
            word = ''.join(random.choice(string.ascii_lowercase) for _ in range(word_length))
            words.append(word)
        return ' '.join(words)
    
    texts = [generate_random_text(words_per_text) for _ in range(n_texts)]
    
    print("Data generation complete.\n")
    
    results = {}
    
    # Tokenization benchmark
    print("Running tokenization benchmark...")
    
    # NLTK tokenization
    import nltk
    try:
        nltk.data.find('tokenizers/punkt')
    except LookupError:
        nltk.download('punkt')
    from nltk.tokenize import word_tokenize
    
    print("Running NLTK tokenization...")
    nltk_result, nltk_duration = benchmark(lambda: [word_tokenize(text) for text in texts])
    print(f"Result: {len(nltk_result)} tokenized texts")
    print(f"Time: {nltk_duration:.2f}ms")
    results["NLTK"] = nltk_duration
    
    # pyroid tokenization
    if PYROID_AVAILABLE:
        print("\nRunning pyroid parallel_tokenize...")
        pyroid_result, pyroid_duration = benchmark(pyroid.parallel_tokenize, texts, True, True)
        print(f"Result: {len(pyroid_result)} tokenized texts")
        print(f"Time: {pyroid_duration:.2f}ms")
        results["pyroid"] = pyroid_duration
        
        # Calculate speedup
        print(f"\nSpeedup vs NLTK: {nltk_duration / pyroid_duration:.1f}x")
    
    # Plot the results
    plot_comparison(f"Text Tokenization ({n_texts} texts)", results)
    
    # TF-IDF benchmark
    print("\n\nRunning TF-IDF benchmark...")
    
    # Generate sample documents
    n_docs = 1000
    docs = [generate_random_text(100) for _ in range(n_docs)]
    
    tfidf_results = {}
    
    # Scikit-learn TfidfVectorizer
    from sklearn.feature_extraction.text import TfidfVectorizer
    print("Running Scikit-learn TfidfVectorizer...")
    vectorizer = TfidfVectorizer()
    sklearn_result, sklearn_duration = benchmark(lambda: vectorizer.fit_transform(docs))
    print(f"Result shape: {sklearn_result.shape}")
    print(f"Time: {sklearn_duration:.2f}ms")
    tfidf_results["Scikit-learn"] = sklearn_duration
    
    # pyroid TF-IDF
    if PYROID_AVAILABLE:
        print("\nRunning pyroid parallel_tfidf...")
        pyroid_result, pyroid_duration = benchmark(pyroid.parallel_tfidf, docs, False)
        print(f"Result: (tfidf_matrix: {len(pyroid_result[0])}, vocabulary: {len(pyroid_result[1])})")
        print(f"Time: {pyroid_duration:.2f}ms")
        tfidf_results["pyroid"] = pyroid_duration
        
        # Calculate speedup
        print(f"\nSpeedup vs Scikit-learn: {sklearn_duration / pyroid_duration:.1f}x")
    
    # Plot the results
    plot_comparison(f"TF-IDF Calculation ({n_docs} documents)", tfidf_results)
    
    return results, tfidf_results

# Run the benchmark
text_tokenize_results, text_tfidf_results = run_text_nlp_benchmark()

## 8. I/O Operations

def run_io_benchmark(num_files=5, rows_per_file=10000):
    """Benchmark I/O operations."""
    import os
    import csv
    import tempfile
    import pandas as pd
    
    # Create temporary directory
    temp_dir = tempfile.mkdtemp()
    print(f"Created temporary directory: {temp_dir}")
    
    # Create test CSV files
    file_paths = []
    print(f"Creating {num_files} CSV files with {rows_per_file} rows each...")
    
    for i in range(num_files):
        file_path = os.path.join(temp_dir, f"test_file_{i}.csv")
        file_paths.append(file_path)
        
        with open(file_path, 'w', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(['id', 'name', 'value', 'flag'])
            
            for j in range(rows_per_file):
                writer.writerow([
                    j,
                    f"item_{j}",
                    random.random() * 100,
                    random.choice(['true', 'false'])
                ])
    
    print("CSV files created.\n")
    
    results = {}
    
    # CSV reading benchmark
    print("Running CSV reading benchmark...")
    
    # Pandas read_csv
    print("Running pandas read_csv...")
    def pandas_read_csv(file_paths):
        return [pd.read_csv(file_path) for file_path in file_paths]
    
    pandas_result, pandas_duration = benchmark(pandas_read_csv, file_paths)
    print(f"Result: {len(pandas_result)} DataFrames")
    print(f"Time: {pandas_duration:.2f}ms")
    results["pandas"] = pandas_duration
    
    # pyroid parallel_read_csv
    if PYROID_AVAILABLE:
        print("\nRunning pyroid parallel_read_csv...")
        schema = {
            'id': 'int',
            'value': 'float',
            'flag': 'bool'
        }
        pyroid_result, pyroid_duration = benchmark(pyroid.parallel_read_csv, file_paths, schema)
        print(f"Result: {len(pyroid_result)} dictionaries")
        print(f"Time: {pyroid_duration:.2f}ms")
        results["pyroid"] = pyroid_duration
        
        # Calculate speedup
        print(f"\nSpeedup vs pandas: {pandas_duration / pyroid_duration:.1f}x")
    
    # Plot the results
    plot_comparison(f"CSV Reading ({num_files} files)", results)
    
    # JSON parsing benchmark
    print("\n\nRunning JSON parsing benchmark...")
    
    # Create test JSON strings
    num_strings = 10000
    items_per_string = 100
    
    print(f"Creating {num_strings} JSON strings with {items_per_string} items each...")
    json_strings = []
    
    import json
    for i in range(num_strings):
        data = {
            "id": i,
            "name": f"record_{i}",
            "values": [random.random() for _ in range(items_per_string)],
            "metadata": {
                "created": "2025-04-04",
                "version": "1.0",
                "tags": ["test", "benchmark", f"tag_{i}"]
            }
        }
        json_strings.append(json.dumps(data))
    
    print("JSON strings created.\n")
    
    json_results = {}
    
    # Python json.loads
    print("Running Python json.loads...")
    def python_json_parse(json_strings):
        return [json.loads(s) for s in json_strings]
    
    python_result, python_duration = benchmark(python_json_parse, json_strings)
    print(f"Result: {len(python_result)} parsed objects")
    print(f"Time: {python_duration:.2f}ms")
    json_results["Pure Python"] = python_duration
    
    # pyroid parallel_json_parse
    if PYROID_AVAILABLE:
        print("\nRunning pyroid parallel_json_parse...")
        pyroid_result, pyroid_duration = benchmark(pyroid.parallel_json_parse, json_strings)
        print(f"Result: {len(pyroid_result)} parsed objects")
        print(f"Time: {pyroid_duration:.2f}ms")
        json_results["pyroid"] = pyroid_duration
        
        # Calculate speedup
        print(f"\nSpeedup vs Python: {python_duration / pyroid_duration:.1f}x")
    
    # Plot the results
    plot_comparison(f"JSON Parsing ({num_strings} strings)", json_results)
    
    # Clean up
    for file_path in file_paths:
        os.remove(file_path)
    os.rmdir(temp_dir)
    print(f"Cleaned up temporary directory: {temp_dir}")
    
    return results, json_results

# Run the benchmark
io_csv_results, io_json_results = run_io_benchmark()

## 9. Image Processing Operations

def run_image_benchmark(num_images=10, width=800, height=600):
    """Benchmark image processing operations."""
    from PIL import Image, ImageFilter
    import io
    
    print(f"Creating {num_images} test images ({width}x{height})...")
    
    # Create test images
    image_data = []
    for i in range(num_images):
        # Create a gradient image
        img = Image.new('RGB', (width, height))
        pixels = img.load()
        
        for x in range(width):
            for y in range(height):
                r = int(255 * x / width)
                g = int(255 * y / height)
                b = int(255 * ((x + y) / (width + height)))
                pixels[x, y] = (r, g, b)
        
        # Convert to bytes
        buffer = io.BytesIO()
        img.save(buffer, format='JPEG')
        image_data.append(buffer.getvalue())
    
    print("Test images created.\n")
    
    results = {}
    
    # Image resizing benchmark
    print("Running image resizing benchmark...")
    
    # PIL resize
    print("Running PIL resize...")
    def pil_resize(images, size):
        results = []
        for img_data in images:
            img = Image.open(io.BytesIO(img_data))
            resized = img.resize(size)
            buffer = io.BytesIO()
            resized.save(buffer, format='JPEG')
            results.append(buffer.getvalue())
        return results
    
    pil_result, pil_duration = benchmark(pil_resize, image_data, (400, 300))
    print(f"Result: {len(pil_result)} resized images")
    print(f"Time: {pil_duration:.2f}ms")
    results["PIL"] = pil_duration
    
    # pyroid parallel_resize
    if PYROID_AVAILABLE:
        print("\nRunning pyroid parallel_resize...")
        pyroid_result, pyroid_duration = benchmark(pyroid.parallel_resize, image_data, (400, 300), "lanczos3")
        print(f"Result: {len(pyroid_result)} resized images")
        print(f"Time: {pyroid_duration:.2f}ms")
        results["pyroid"] = pyroid_duration
        
        # Calculate speedup
        print(f"\nSpeedup vs PIL: {pil_duration / pyroid_duration:.1f}x")
    
    # Plot the results
    plot_comparison(f"Image Resizing ({num_images} images)", results)
    
    # Image filtering benchmark
    print("\n\nRunning image filtering benchmark...")
    
    filter_results = {}
    
    # PIL filter
    print("Running PIL filter (blur)...")
    def pil_filter(images, filter_type):
        results = []
        for img_data in images:
            img = Image.open(io.BytesIO(img_data))
            if filter_type == "blur":
                filtered = img.filter(ImageFilter.GaussianBlur(radius=2))
            elif filter_type == "sharpen":
                filtered = img.filter(ImageFilter.SHARPEN)
            elif filter_type == "edge":
                filtered = img.filter(ImageFilter.FIND_EDGES)
            else:
                filtered = img
            buffer = io.BytesIO()
            filtered.save(buffer, format='JPEG')
            results.append(buffer.getvalue())
        return results
    
    pil_filter_result, pil_filter_duration = benchmark(pil_filter, image_data, "blur")
    print(f"Result: {len(pil_filter_result)} filtered images")
    print(f"Time: {pil_filter_duration:.2f}ms")
    filter_results["PIL"] = pil_filter_duration
    
    # pyroid parallel_image_filter
    if PYROID_AVAILABLE:
        print("\nRunning pyroid parallel_image_filter...")
        params = {"sigma": 2.0}
        pyroid_filter_result, pyroid_filter_duration = benchmark(pyroid.parallel_image_filter, image_data, "blur", params)
        print(f"Result: {len(pyroid_filter_result)} filtered images")
        print(f"Time: {pyroid_filter_duration:.2f}ms")
        filter_results["pyroid"] = pyroid_filter_duration
        
        # Calculate speedup
        print(f"\nSpeedup vs PIL: {pil_filter_duration / pyroid_filter_duration:.1f}x")
    
    # Plot the results
    plot_comparison(f"Image Filtering ({num_images} images)", filter_results)
    
    return results, filter_results

# Run the benchmark
image_resize_results, image_filter_results = run_image_benchmark()

## Summary of All Operations

# Collect all results
all_results = {
    "Sum 1M Numbers": sum_results,
    "Regex Replace": regex_results,
    "Sort 1M Items": sort_results,
    "Data Pipeline": pipeline_results,
    "DataFrame Apply": df_apply_results,
    "DataFrame GroupBy": df_groupby_results,
    "ML Distance Matrix": ml_distance_results,
    "ML Feature Scaling": ml_scaling_results,
    "Text Tokenization": text_tokenize_results,
    "TF-IDF Calculation": text_tfidf_results,
    "CSV Reading": io_csv_results,
    "JSON Parsing": io_json_results,
    "Image Resizing": image_resize_results,
    "Image Filtering": image_filter_results
}

# Calculate speedups
speedups = {}
for name, results in all_results.items():
    if isinstance(results, tuple):
        # For benchmarks that return multiple results, use the first one
        results = results[0]
    
    if "Pure Python" in results and "pyroid" in results:
        speedups[name] = results["Pure Python"] / results["pyroid"]
    elif "pandas" in results and "pyroid" in results:
        speedups[name] = results["pandas"] / results["pyroid"]
    elif "NumPy" in results and "pyroid" in results:
        speedups[name] = results["NumPy"] / results["pyroid"]
    elif "Scikit-learn" in results and "pyroid" in results:
        speedups[name] = results["Scikit-learn"] / results["pyroid"]
    elif "NLTK" in results and "pyroid" in results:
        speedups[name] = results["NLTK"] / results["pyroid"]
    elif "PIL" in results and "pyroid" in results:
        speedups[name] = results["PIL"] / results["pyroid"]

# Create a bar chart of speedups
plt.figure(figsize=(14, 8))
bars = plt.bar(speedups.keys(), speedups.values(), color="#2ca02c")

# Add speedup labels on top of bars
for bar in bars:
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2., height + 0.1,
            f"{height:.1f}x",
            ha='center', va='bottom', fontweight='bold')

plt.title("Pyroid Speedup vs Standard Libraries", fontsize=18)
plt.ylabel("Speedup Factor (x)", fontsize=14)
plt.xticks(rotation=45, ha='right')
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

## Conclusion

Pyroid significantly outperforms pure Python implementations across a variety of operations:

1. **Math Operations**: Pyroid's parallel sum is much faster than Python's built-in sum function and even outperforms NumPy.

2. **String Operations**: Pyroid's parallel regex replacement is significantly faster than Python's re.sub.

3. **Data Operations**: Pyroid's parallel sort outperforms Python's built-in sorted function.

4. **DataFrame Operations**: Pyroid's dataframe operations are faster than pandas equivalents.

5. **Machine Learning Operations**: Pyroid's ML operations outperform NumPy and scikit-learn implementations.

6. **Text and NLP Operations**: Pyroid's text processing functions are faster than NLTK and scikit-learn.

7. **I/O Operations**: Pyroid's parallel file operations outperform pandas and Python's built-in functions.

8. **Image Processing Operations**: Pyroid's image processing is faster than PIL implementations.

9. **Real-world Scenarios**: In data processing pipelines that combine multiple operations, Pyroid shows impressive performance gains.

These benchmarks demonstrate that Pyroid is an excellent choice for performance-critical Python applications, especially those involving large datasets or CPU-intensive operations. The Rust-powered implementation with true parallelism (no GIL limitations) provides significant speedups across all tested domains.
