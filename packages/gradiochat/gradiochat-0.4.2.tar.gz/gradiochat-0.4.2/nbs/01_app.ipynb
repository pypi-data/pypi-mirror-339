{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# App logic\n",
    "\n",
    "> Contains BaseChatApp and large language model integration logic. Implements the core functionality indepent of the UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jelle/code/gradiochat/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "from typing import Protocol, runtime_checkable, Generator, List\n",
    "from openai import OpenAI\n",
    "from ollama import Client as OllamaSDK\n",
    "from ollama import AsyncClient as AsyncOllamaSDK\n",
    "\n",
    "from gradiochat.config import ModelConfig, Message, ChatAppConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import statement\n",
    "\n",
    "```python\n",
    "from gradiochat.app import *\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation of new code\n",
    "\n",
    "I used the `Protocol` and `runtime_checkable` for the first time. Here's a short explainer.\n",
    "\n",
    "#### Python's Protocol System\n",
    "\n",
    "Protocols were introduced in Python 3.8 through PEP 544 and are part of the `typing` module. They provide a way to define interfaces that classes can implement without explicitly inheriting from them - this is called \"structural typing\" or \"duck typing.\"\n",
    "\n",
    "#### Protocols vs Abstract Base Classes (ABCs)\n",
    "\n",
    "**Abstract Base Classes (Traditional Approach):**\n",
    "- Require explicit inheritance (`class MyClass(AbstractBaseClass):`)\n",
    "- Use the `@abstractmethod` decorator to mark methods that must be implemented\n",
    "- Check for compatibility based on the class hierarchy (nominal typing)\n",
    "- Enforce implementation at class definition time\n",
    "\n",
    "**Protocols (New Approach):**\n",
    "- Don't require inheritance - classes just need to implement the required methods\n",
    "- Use the `Protocol` class and `@runtime_checkable` decorator\n",
    "- Check for compatibility based on method signatures (structural typing)\n",
    "- Can check compatibility at runtime with `isinstance()` if marked as `@runtime_checkable`\n",
    "\n",
    "#### How It Works\n",
    "\n",
    "In our code:\n",
    "\n",
    "```python\n",
    "@runtime_checkable\n",
    "class LLMClientProtocol(Protocol):\n",
    "    def chat_completion(self, messages: List[Message], **kwargs) -> str:\n",
    "        ...\n",
    "    \n",
    "    def chat_completion_stream(self, messages: List[Message], **kwargs) -> Generator[str, None, None]:\n",
    "        ...\n",
    "```\n",
    "\n",
    "This defines an interface that says \"any class with methods named `chat_completion` and `chat_completion_stream` with these signatures is considered compatible with `LLMClientProtocol`.\"\n",
    "\n",
    "The `...` in the method bodies is a special syntax that means \"this method is required but not implemented here.\" It's similar to `pass` but specifically for protocol definitions.\n",
    "\n",
    "#### Benefits in Our Context\n",
    "\n",
    "1. **Flexibility**: We can create any class that implements these methods, and it will be compatible with `LLMClientProtocol` without explicitly inheriting from it.\n",
    "\n",
    "2. **Easy Testing**: We can create mock implementations that automatically satisfy the protocol by just implementing the required methods.\n",
    "\n",
    "3. **Type Checking**: Tools like mypy can verify that our classes implement all required methods with the correct signatures.\n",
    "\n",
    "4. **Runtime Checking**: With `@runtime_checkable`, we can use `isinstance(obj, LLMClientProtocol)` to check if an object implements the protocol.\n",
    "\n",
    "#### Example of Use\n",
    "\n",
    "```python\n",
    "def process_with_any_llm_client(client: LLMClientProtocol, messages: List[Message]):\n",
    "    # This function will accept any object that has the required methods,\n",
    "    # regardless of its class hierarchy\n",
    "    response = client.chat_completion(messages)\n",
    "    return response\n",
    "```\n",
    "\n",
    "This would accept our `HuggingFaceClient` or any other class that implements the required methods, without forcing them to inherit from a common base class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the general LLMClientProtocol structure\n",
    "\n",
    "Which means it should have the methods defined in `LLMClientProtocol`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@runtime_checkable\n",
    "class LLMClientProtocol(Protocol):\n",
    "    \"\"\"Protocol defining the interface for LLM clients\"\"\"\n",
    "    \n",
    "    def chat_completion(self, messages: List[Message], **kwargs) -> str:\n",
    "        \"\"\"Generate a response from the LLM\"\"\"\n",
    "        ...\n",
    "    \n",
    "    def chat_completion_stream(self, messages: List[Message], **kwargs) -> Generator[str, None, None]:\n",
    "        \"\"\"Generate a streaming response from the LLM\"\"\"\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the LLM Clients\n",
    "\n",
    "This should at least follow the structure of `LLMClientProtocol` but can of course be expanded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HuggingFaceClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class HuggingFaceClient():\n",
    "    \"\"\"Client for interacting with HuggingFace models\"\"\"\n",
    "    \n",
    "    def __init__(self, model_config: ModelConfig):\n",
    "        \"\"\"Initialize the client with model configuration\"\"\"\n",
    "        self.model_config = model_config\n",
    "\n",
    "        # Default to HF Inference API if no base URL is provided\n",
    "        base_url = model_config.api_base_url or \"https://router.huggingface.co/hf-inference/v1\"\n",
    "\n",
    "        self.client = OpenAI(\n",
    "            base_url=base_url,\n",
    "            api_key=model_config.api_key or \"hf_no_api_key_provided\"\n",
    "        )\n",
    "    \n",
    "    def chat_completion(self, \n",
    "            messages: List[Message], # List of messages conforming to the Message pydantic dataclass\n",
    "            **kwargs\n",
    "            ) -> str:\n",
    "        \"\"\"Generate a chat completion from the HuggingFace model\"\"\"\n",
    "        # Convert our Message objects to the format expected by the OpenAI client\n",
    "        openai_messages = [{\"role\": msg.role, \"content\": msg.content} for msg in messages]\n",
    "\n",
    "        completion = self.client.chat.completions.create(\n",
    "            model=self.model_config.model_name,\n",
    "            messages=openai_messages,\n",
    "            max_completion_tokens=kwargs.get(\"max_completion_tokens\", self.model_config.max_completion_tokens),\n",
    "            temperature=kwargs.get(\"temperature\", self.model_config.temperature)\n",
    "        )\n",
    "\n",
    "        # Extract the generated text\n",
    "        return completion.choices[0].message.content\n",
    "    \n",
    "    def chat_completion_stream(self, messages: List[Message], **kwargs) -> Generator[str, None, None]:\n",
    "        \"\"\"Generate a streaming chat completion\"\"\"\n",
    "        # For now, use non-streaming version as a placeholder\n",
    "        # We'll implement proper streaming later\n",
    "        result = self.chat_completion(messages, **kwargs)\n",
    "        yield result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TogetherAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TogetherAiClient():\n",
    "    \"\"\"Client for interacting with models through the TogetherAI API server\n",
    "    We use the openai package\"\"\"\n",
    "    \n",
    "    def __init__(self, model_config: ModelConfig):\n",
    "        \"\"\"Initialize the client with model configuration\"\"\"\n",
    "        self.model_config = model_config\n",
    "\n",
    "        self.client = OpenAI(\n",
    "            base_url=model_config.api_base_url or \"https://api.together.xyz/v1\", # Default to Together AI Inference API if no base URL is provided\n",
    "            api_key=model_config.api_key,\n",
    "        )\n",
    "    \n",
    "    def chat_completion(self, \n",
    "            messages: List[Message], # List of messages conforming to the Message pydantic dataclass\n",
    "            **kwargs\n",
    "            ) -> str:\n",
    "        \"\"\"Generate a chat completion from the Together AI API\"\"\"\n",
    "        # Convert our Message objects to the format expected by the OpenAI client\n",
    "        openai_messages = [{\"role\": msg.role, \"content\": msg.content} for msg in messages]\n",
    "\n",
    "        completion = self.client.chat.completions.create(\n",
    "            model=self.model_config.model_name,\n",
    "            messages=openai_messages,\n",
    "            max_completion_tokens=kwargs.get(\"max_completion_tokens\", self.model_config.max_completion_tokens),\n",
    "            temperature=kwargs.get(\"temperature\", self.model_config.temperature),\n",
    "            top_p=kwargs.get(\"top_p\", self.model_config.top_p),\n",
    "            stop=kwargs.get(\"stop\", self.model_config.stop) or [\"<|eot_id|>\",\"<|eom_id|>\"]\n",
    "        )\n",
    "\n",
    "        # Extract the generated text\n",
    "        return completion.choices[0].message.content\n",
    "    \n",
    "    def chat_completion_stream(self,\n",
    "            messages: List[Message], # List of messages conforming to the Message pydantic dataclass\n",
    "            **kwargs) -> Generator[str, None, None]:\n",
    "        \"\"\"Generate a streaming chat completion\"\"\"\n",
    "\n",
    "        openai_messages = [{\"role\": msg.role, \"content\": msg.content} for msg in messages]\n",
    "\n",
    "        stream = self.client.chat.completions.create(\n",
    "            model=self.model_config.model_name,\n",
    "            messages=openai_messages,\n",
    "            max_completion_tokens=kwargs.get(\"max_completion_tokens\", self.model_config.max_completion_tokens),\n",
    "            temperature=kwargs.get(\"temperature\", self.model_config.temperature),\n",
    "            top_p=kwargs.get(\"top_p\", self.model_config.top_p),\n",
    "            stop=kwargs.get(\"stop\", self.model_config.stop) or [\"<|eot_id|>\",\"<|eom_id|>\"],\n",
    "            stream=True\n",
    "        )\n",
    "\n",
    "        for token in stream:\n",
    "            if hasattr(token, 'choices') and token.choices[0].delta.content is not None:\n",
    "                yield token.choices[0].delta.content\n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Local Ollama client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class OllamaClient():\n",
    "    \"\"\"Client for interacting with models through a local Ollama API server\n",
    "    Uses the official Ollama Python library\"\"\"\n",
    "    \n",
    "    def __init__(self, model_config: ModelConfig):\n",
    "        \"\"\"Initialize the client with model configuration\"\"\"\n",
    "        self.model_config = model_config\n",
    "\n",
    "        # Extract host from api_base_url or use default\n",
    "        host = model_config.api_base_url or \"http://localhost:11434\"\n",
    "        \n",
    "        # Create Ollama client\n",
    "        self.client = OllamaSDK(host=host)\n",
    "    \n",
    "    def chat_completion(self, \n",
    "            messages: List[Message], # List of messages conforming to the Message pydantic dataclass\n",
    "            **kwargs\n",
    "            ) -> str:\n",
    "        \"\"\"Generate a chat completion from the Ollama API\"\"\"\n",
    "        # Convert our Message objects to the format expected by the Ollama client\n",
    "        ollama_messages = [{\"role\": msg.role, \"content\": msg.content} for msg in messages]\n",
    "\n",
    "        # Prepare parameters\n",
    "        params = {\n",
    "            \"model\": self.model_config.model_name,\n",
    "            \"messages\": ollama_messages,\n",
    "            \"options\": {\n",
    "                \"temperature\": kwargs.get(\"temperature\", self.model_config.temperature),\n",
    "                \"top_p\": kwargs.get(\"top_p\", self.model_config.top_p),\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Add max_tokens if specified\n",
    "        max_tokens = kwargs.get(\"max_completion_tokens\", self.model_config.max_completion_tokens)\n",
    "        if max_tokens is not None:\n",
    "            params[\"options\"][\"num_predict\"] = max_tokens\n",
    "            \n",
    "        # Add stop sequences if specified\n",
    "        stop = kwargs.get(\"stop\", self.model_config.stop)\n",
    "        if stop is not None:\n",
    "            params[\"options\"][\"stop\"] = stop\n",
    "\n",
    "        # Call the Ollama API\n",
    "        response = self.client.chat(**params)\n",
    "\n",
    "        # Extract the generated text\n",
    "        return response.message.content\n",
    "    \n",
    "    def chat_completion_stream(self,\n",
    "            messages: List[Message], # List of messages conforming to the Message pydantic dataclass\n",
    "            **kwargs) -> Generator[str, None, None]:\n",
    "        \"\"\"Generate a streaming chat completion\"\"\"\n",
    "        # Convert our Message objects to the format expected by the Ollama client\n",
    "        ollama_messages = [{\"role\": msg.role, \"content\": msg.content} for msg in messages]\n",
    "\n",
    "        # Prepare parameters\n",
    "        params = {\n",
    "            \"model\": self.model_config.model_name,\n",
    "            \"messages\": ollama_messages,\n",
    "            \"stream\": True,\n",
    "            \"options\": {\n",
    "                \"temperature\": kwargs.get(\"temperature\", self.model_config.temperature),\n",
    "                \"top_p\": kwargs.get(\"top_p\", self.model_config.top_p),\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Add max_tokens if specified\n",
    "        max_tokens = kwargs.get(\"max_completion_tokens\", self.model_config.max_completion_tokens)\n",
    "        if max_tokens is not None:\n",
    "            params[\"options\"][\"num_predict\"] = max_tokens\n",
    "            \n",
    "        # Add stop sequences if specified\n",
    "        stop = kwargs.get(\"stop\", self.model_config.stop)\n",
    "        if stop is not None:\n",
    "            params[\"options\"][\"stop\"] = stop\n",
    "\n",
    "        # Call the Ollama API with streaming\n",
    "        stream = self.client.chat(**params)\n",
    "\n",
    "        # Yield each chunk of content\n",
    "        for chunk in stream:\n",
    "            if chunk.message and chunk.message.content:\n",
    "                yield chunk.message.content\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the LLM client\n",
    "\n",
    "This function creates the client using the available LLM Client classes.\n",
    "It gets the provider from the `model_config`. If it finds a LLM Client Class for this provider, it returns that client. If it doesn't find a LLM Client Class for that provider, it returns a ValueError."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def create_llm_client(model_config: ModelConfig) -> LLMClientProtocol:\n",
    "    \"\"\"\n",
    "    Factory function to create an LLM client based on the provider.\n",
    "    \"\"\"\n",
    "    if model_config.provider.lower() == \"huggingface\":\n",
    "        return HuggingFaceClient(model_config)\n",
    "    if model_config.provider.lower() == \"togetherai\":\n",
    "        return TogetherAiClient(model_config)\n",
    "    if model_config.provider.lower() == \"ollama\":\n",
    "        return OllamaClient(model_config)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported provider: {model_config.provider}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The internal logic of the chat app\n",
    "\n",
    "Now the `BaseChatApp` class is defined. This class is used to instantiate the properties en methods for the internal workings of the chat app. The UI is defined in the `ui` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ChatAppConfig' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#| export\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mBaseChatApp\u001b[39;00m:\n\u001b[1;32m      3\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Base class for creating configurable chat applications with Gradio\"\"\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config: ChatAppConfig):\n",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m, in \u001b[0;36mBaseChatApp\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mBaseChatApp\u001b[39;00m:\n\u001b[1;32m      3\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Base class for creating configurable chat applications with Gradio\"\"\"\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config: \u001b[43mChatAppConfig\u001b[49m):\n\u001b[1;32m      6\u001b[0m \u001b[38;5;250m        \u001b[39m\u001b[38;5;124;03m\"\"\"Initialize the chat application\"\"\"\u001b[39;00m\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m config\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ChatAppConfig' is not defined"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "class BaseChatApp:\n",
    "    \"\"\"Base class for creating configurable chat applications with Gradio\"\"\"\n",
    "    \n",
    "    def __init__(self, config: ChatAppConfig):\n",
    "        \"\"\"Initialize the chat application\"\"\"\n",
    "        self.config = config\n",
    "        self.chat_history = []\n",
    "        self._load_context()\n",
    "        self.client = create_llm_client(config.model)\n",
    "        \n",
    "    def _load_context(self) -> None:\n",
    "        \"\"\"Load context from markdown files\"\"\"\n",
    "        self.context_text = \"\"\n",
    "        for file_path in self.config.context_files:\n",
    "            if file_path.exists() and file_path.is_file():\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    self.context_text += f.read() + \"\\n\\n\"\n",
    "    \n",
    "    def prepare_messages(self, user_message: str) -> List[Message]:\n",
    "        \"\"\"Prepare the messages for the LLM, including system prompt and chat history\"\"\"\n",
    "        messages = []\n",
    "        \n",
    "        # Add system message with prompt and context\n",
    "        system_content = self.config.system_prompt\n",
    "        if self.context_text:\n",
    "            system_content += f\"\\n\\nAdditional information: {self.context_text}\"\n",
    "        \n",
    "        messages.append(Message(role=\"system\", content=system_content))\n",
    "        \n",
    "        # Add chat history\n",
    "        for msg in self.chat_history:\n",
    "            messages.append(Message(role=msg['role'], content=msg['content']))\n",
    "        \n",
    "        # Add current user message\n",
    "        messages.append(Message(role=\"user\", content=user_message))\n",
    "        \n",
    "        return messages\n",
    "    \n",
    "    def generate_response(self, user_message: str, **kwargs) -> str:\n",
    "        \"\"\"Generate a response to the user message\"\"\"\n",
    "        messages = self.prepare_messages(user_message)\n",
    "        return self.client.chat_completion(messages, **kwargs)\n",
    "    \n",
    "    def generate_stream(self, user_message: str, **kwargs) -> Generator[str, None, None]:\n",
    "        \"\"\"Generate a streaming response to the user message\"\"\"\n",
    "        messages = self.prepare_messages(user_message)\n",
    "        return self.client.chat_completion_stream(messages, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a HuggingFace test model config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "# Eval set to false, because the api key is stored in .env and thus can't be found when\n",
    "# nbdev_test is run\n",
    "hf_config = ModelConfig(\n",
    "    model_name=\"mistralai/Mistral-7B-Instruct-v0.3\", # \"Qwen/QwQ-32B\" is another possibility, but with vision you need another messages format\n",
    "    provider=\"huggingface\",\n",
    "    api_key_env_var=\"HF_API_KEY\",\n",
    "    api_base_url=\"https://router.huggingface.co/hf-inference/v1\",\n",
    "    max_completion_tokens=100,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# Create the client\n",
    "client = create_llm_client(hf_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Together AI test model config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TogetherAiClient' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 12\u001b[0m\n\u001b[1;32m      4\u001b[0m ta_config \u001b[38;5;241m=\u001b[39m ModelConfig(\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# model_name=\"mistralai/Mistral-Nemo-Instruct-2407\",\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta-llama/Llama-3.3-70B-Instruct-Turbo-Free\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m     provider\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtogetherai\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      8\u001b[0m     api_key_env_var\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTG_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Create the client\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m client \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_llm_client\u001b[49m\u001b[43m(\u001b[49m\u001b[43mta_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 9\u001b[0m, in \u001b[0;36mcreate_llm_client\u001b[0;34m(model_config)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m HuggingFaceClient(model_config)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_config\u001b[38;5;241m.\u001b[39mprovider\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtogetherai\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mTogetherAiClient\u001b[49m(model_config)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_config\u001b[38;5;241m.\u001b[39mprovider\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mollama\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m OllamaClient(model_config)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TogetherAiClient' is not defined"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "# Eval set to false, because the api key is stored in .env and thus can't be found when\n",
    "# nbdev_test is run\n",
    "ta_config = ModelConfig(\n",
    "    # model_name=\"mistralai/Mistral-Nemo-Instruct-2407\",\n",
    "    model_name=\"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\",\n",
    "    provider=\"togetherai\",\n",
    "    api_key_env_var=\"TG_API_KEY\",\n",
    ")\n",
    "\n",
    "# Create the client\n",
    "client = create_llm_client(ta_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Ollama test model config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'OllamaClient' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 11\u001b[0m\n\u001b[1;32m      4\u001b[0m olla_config \u001b[38;5;241m=\u001b[39m ModelConfig(\n\u001b[1;32m      5\u001b[0m     model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnchapman/ministral-8b-instruct-2410\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m     provider\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mollama\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m     api_key_env_var\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOLLAMA_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Create the client\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m client \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_llm_client\u001b[49m\u001b[43m(\u001b[49m\u001b[43molla_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 11\u001b[0m, in \u001b[0;36mcreate_llm_client\u001b[0;34m(model_config)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m TogetherAiClient(model_config)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_config\u001b[38;5;241m.\u001b[39mprovider\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mollama\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mOllamaClient\u001b[49m(model_config)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported provider: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_config\u001b[38;5;241m.\u001b[39mprovider\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'OllamaClient' is not defined"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "# Eval set to false, because the api key is stored in .env and thus can't be found when\n",
    "# nbdev_test is run\n",
    "olla_config = ModelConfig(\n",
    "    model_name=\"nchapman/ministral-8b-instruct-2410\",\n",
    "    provider=\"ollama\",\n",
    "    api_key_env_var=\"OLLAMA_API_KEY\",\n",
    ")\n",
    "\n",
    "# Create the client\n",
    "client = create_llm_client(olla_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Error code: 402 - {'error': 'You have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.'}\n",
      "\n",
      "Testing with overridden parameters:\n",
      "Error: Error code: 402 - {'error': 'You have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.'}\n"
     ]
    }
   ],
   "source": [
    "test_messages = [\n",
    "    Message(role=\"system\", content=\"You are Aurelius Augustinus, helping me to think deeply and be humble and thankfull.\"),\n",
    "    Message(role=\"user\", content=\"Why should I engage with the people around me?\")\n",
    "]\n",
    "# Test with a simple prompt\n",
    "try:\n",
    "    response = client.chat_completion(test_messages)\n",
    "    print(f\"Response received: {response[:100]}...\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# Test with overriden parameters\n",
    "try:\n",
    "    print(\"\\nTesting with overridden parameters:\")\n",
    "    response = client.chat_completion(test_messages, max_completion_tokens=50, temperature=0.9)\n",
    "    print(f\"Response: {response[:100]}...\")  # Show first 100 chars\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
