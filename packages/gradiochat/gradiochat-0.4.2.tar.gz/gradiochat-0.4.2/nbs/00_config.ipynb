{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration\n",
    "\n",
    "> Define the data classes using Pydantic, making it possible to configure the chat application and do input validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "\n",
    "# Only used for building documentation\n",
    "from gradiochat.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jelle/code/gradiochat/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "#| hide\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional, List, Tuple, Literal, Any\n",
    "import os\n",
    "import gradio as gr\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import statement\n",
    "\n",
    "```python\n",
    "from gradiochat.config import *\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some lessons and clarification about the used code\n",
    "\n",
    "1. `Pydantic` vs `Dataclasses`: Pydantic creates classes similar to Python's dataclasses but with additional features. The key differences are that Pydantic provides data validation, type coercion, and more robust error handling. It will automatically validate data during initialization and conversion.\n",
    "2. Pydantic and typing: Pydantic leverages Python's standard typing system but adds its own validation on top. It uses Python's type hints to know what types to validate against.\n",
    "3. The \"...\" placeholder: The ellipsis (...) is a special value in Pydantic that indicates a required field. It means \"this field must be provided when creating an instance\" - there's no default value. When you create a ModelConfig instance, you'll need to provide a value for model_name.\n",
    "4. `@property` usage: The `@property` decorator creates a getter method that's accessed like an attribute. In our case, api_key looks like a normal attribute but when accessed, it runs the method to retrieve the value from environment variables. This is a clean way to avoid storing sensitive information in the object itself.\n",
    "5. `Field` from `pydantic` can be used to add extra information and metadata to inform the reader and/or do data validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The configuration for the workings of the LLM chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ModelConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First the configuration for the LLM model to use in the `ModelConfig`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ModelConfig(BaseModel):\n",
    "    \"\"\"Configuration for the LLM model\"\"\"\n",
    "    model_name: str = Field(..., description=\"Name or path of the model to use\") # Name\n",
    "    provider: str = Field(default=\"huggingface\", description=\"Model provider (huggingface, openai, etc)\")\n",
    "    api_key_env_var: Optional[str] = Field(default=None, description=\"Environment variable name for API key\")\n",
    "    api_base_url: Optional[str] = Field(default=None, description=\"Base URL for API reqeuest\")\n",
    "    temperature: float = Field(default=0.7, description=\"Temperature for generation\")\n",
    "    max_completion_tokens: int = Field(default=1024, description=\"Maximum tokens to generate\")\n",
    "    top_p: float = Field(default=0.7, description=\"Adjust the number of choices for each predicted token [0-1]\")\n",
    "    top_k: int = Field(default=50, description=\"Limits the number of choices for the next predicted token. Not available for OpenAI API\")\n",
    "    frequency_penalty: float = Field(default=0, description=\"Reduces the likelihood of repeating prompt text or getting stuck in a loop [-2 -> 2]\")\n",
    "    stop: Optional[List[str]] = Field(default=[\"\\nUser:\", \"<|endoftext|>\"], description=\"Sequences to stop generation\")\n",
    "    stream: Optional[bool] = Field(default=None, description=\"If set to true, the model response data will be streamed to the client as it is generated using server-sent events.\")\n",
    "\n",
    "    \n",
    "    @property\n",
    "    def api_key(self) -> Optional[str]:\n",
    "        \"\"\"Get the API key from environment variables if specified\"\"\"\n",
    "        if self.api_key_env_var:\n",
    "            if os.environ.get(self.api_key_env_var):\n",
    "                return os.environ.get(self.api_key_env_var)\n",
    "            raise ValueError(f\"The environment variable {self.api_key_env_var} is not found in the .env file.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## ModelConfig\n",
       "Configuration for the LLM model\n",
       "\n",
       "| Variable | Type | Default | Details |\n",
       "|---|---|---|---|\n",
       "| `model_name` | `str` | PydanticUndefined | Name or path of the model to use |\n",
       "| `provider` | `str` | 'huggingface' | Model provider (huggingface, openai, etc) |\n",
       "| `api_key_env_var` | `Optional[str]` | None | Environment variable name for API key |\n",
       "| `api_base_url` | `Optional[str]` | None | Base URL for API reqeuest |\n",
       "| `temperature` | `float` | 0.7 | Temperature for generation |\n",
       "| `max_completion_tokens` | `int` | 1024 | Maximum tokens to generate |\n",
       "| `top_p` | `float` | 0.7 | Adjust the number of choices for each predicted token [0-1] |\n",
       "| `top_k` | `int` | 50 | Limits the number of choices for the next predicted token. Not available for OpenAI API |\n",
       "| `frequency_penalty` | `float` | 0 | Reduces the likelihood of repeating prompt text or getting stuck in a loop [-2 -> 2] |\n",
       "| `stop` | `Optional[list[str]]` | ['\\nUser:', '<\\|endoftext\\|>'] | Sequences to stop generation |\n",
       "| `stream` | `Optional[bool]` | None | If set to true, the model response data will be streamed to the client as it is generated using server-sent events. |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| echo: false\n",
    "pydantic_to_markdown_table(ModelConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Message config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next the configuration of the Message system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Message(BaseModel):\n",
    "    \"\"\"A message in a conversation\"\"\"\n",
    "    role: Literal[\"system\", \"user\", \"assistant\"] = Field(..., description=\"Role of the message sender\")\n",
    "    content: str = Field(..., description=\"Content of the message\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Message\n",
       "A message in a conversation\n",
       "\n",
       "| Variable | Type | Default | Details |\n",
       "|---|---|---|---|\n",
       "| `role` | `Literal[system, user, assistant]` | PydanticUndefined | Role of the message sender |\n",
       "| `content` | `str` | PydanticUndefined | Content of the message |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| echo: false\n",
    "pydantic_to_markdown_table(Message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chat config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the configuration for the chat implementation. Making sure the application can handle:\n",
    "- system prompt\n",
    "- context if applicable\n",
    "- a start 'user' prompt if applicable\n",
    "- user input\n",
    "\n",
    "The other settings that are available in this class can easily be infered from the description in the `ChatAppConfig` class itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ChatAppConfig(BaseModel):\n",
    "    \"\"\"Main configuration for a chat application\"\"\"\n",
    "    app_name: str = Field(..., description=\"Name of the application\")\n",
    "    description: str = Field(default=\"\", description=\"Description of the application\")\n",
    "    system_prompt: str = Field(..., description=\"System prompt for the LLM\")\n",
    "    starter_prompt: Optional[str] = Field(default=None, description=\"Initial prompt to start the conversation\")\n",
    "    context_files: List[Path] = Field(default=[], description=\"List of markdown files for additional context\")\n",
    "    model: ModelConfig\n",
    "    theme: Optional[Any] = Field(default=None, description=\"Gradio theme to use\")\n",
    "    logo_path: Optional[Path] = Field(default=None, description=\"Path to logo image\")\n",
    "    show_system_prompt: bool = Field(default=True, description=\"Whether to show system prompt in UI\")\n",
    "    show_context: bool = Field(default=True, description=\"Whether to show context in UI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## ChatAppConfig\n",
       "Main configuration for a chat application\n",
       "\n",
       "| Variable | Type | Default | Details |\n",
       "|---|---|---|---|\n",
       "| `app_name` | `str` | PydanticUndefined | Name of the application |\n",
       "| `description` | `str` | '' | Description of the application |\n",
       "| `system_prompt` | `str` | PydanticUndefined | System prompt for the LLM |\n",
       "| `starter_prompt` | `Optional[str]` | None | Initial prompt to start the conversation |\n",
       "| `context_files` | `list[Path]` | [] | List of markdown files for additional context |\n",
       "| `model` | `ModelConfig` | PydanticUndefined |  (see `ModelConfig` table) |\n",
       "| `theme` | `Optional[Any]` | None | Gradio theme to use |\n",
       "| `logo_path` | `Optional[Path]` | None | Path to logo image |\n",
       "| `show_system_prompt` | `bool` | True | Whether to show system prompt in UI |\n",
       "| `show_context` | `bool` | True | Whether to show context in UI |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| echo: false\n",
    "pydantic_to_markdown_table(ChatAppConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example configuration for a chat application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"app_name\": \"Test App\",\n",
      "  \"description\": \"\",\n",
      "  \"system_prompt\": \"You are a helpful assistant.\",\n",
      "  \"starter_prompt\": null,\n",
      "  \"context_files\": [],\n",
      "  \"model\": {\n",
      "    \"model_name\": \"gpt-3.5-turbo\",\n",
      "    \"provider\": \"huggingface\",\n",
      "    \"api_key_env_var\": \"TEST_API_KEY\",\n",
      "    \"api_base_url\": null,\n",
      "    \"temperature\": 0.7,\n",
      "    \"max_completion_tokens\": 1024,\n",
      "    \"top_p\": 0.7,\n",
      "    \"top_k\": 50,\n",
      "    \"frequency_penalty\": 0.0,\n",
      "    \"stop\": [\n",
      "      \"\\nUser:\",\n",
      "      \"<|endoftext|>\"\n",
      "    ],\n",
      "    \"stream\": null\n",
      "  },\n",
      "  \"theme\": null,\n",
      "  \"logo_path\": null,\n",
      "  \"show_system_prompt\": true,\n",
      "  \"show_context\": true\n",
      "}\n",
      "API Key available: Yes\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "# Eval set to false, because the api key is stored in .env and thus can't be found when\n",
    "# nbdev_test is run\n",
    "test_config = ChatAppConfig(\n",
    "    app_name=\"Test App\",\n",
    "    system_prompt=\"You are a helpful assistant.\",\n",
    "    model=ModelConfig(\n",
    "        model_name=\"gpt-3.5-turbo\",\n",
    "        api_key_env_var=\"TEST_API_KEY\",\n",
    "    )\n",
    ")\n",
    "\n",
    "print(test_config.model_dump_json(indent=2))\n",
    "\n",
    "print(f\"API Key available: {'Yes' if test_config.model.api_key else 'No'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
