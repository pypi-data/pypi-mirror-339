# coding: utf-8

"""
Class for reading and processing Digital RF (DRF) data generated by Grape2 PSWS stations.
Author: Cuong Nguyen
"""

"""
TODO: Add support for DRF input of arbitrary sampling rate
TODO: Add support for specifying number of hours to read
TODO: Think about metadata (e.g. include in cache or not)
TODO:
    - Add support for logging
    - Log exceptions
    - Configure logging level at the command-line 
TODO: Test with complex DRF input
TODO: To see if cache directory is automatically removed if the object is destroyed or closed in anyway
TODO: Get memory profile for the initial run
TODO: cache command line option
TODO: add support for Grape 1 and RX888 receivers
"""
"""
Caching is always enabled.
However, optional parameter to remove cache when done.
"""

import os
import atexit
import shutil
import digital_rf as drf
import math
import numpy as np
from tqdm import tqdm
from datetime import datetime, timezone
import pytz
import pickle
import logging
from typing import Optional, Dict, Any, List
from scipy import signal

logger = logging.getLogger(__name__)

class Reader:
    """ Class for reading and processing Digital RF (DRF) data generated by Grape2 PSWS stations."""

    def __init__(self, datadir: str, cachedir: str = "cache", cleanup_cache: bool = True, target_bandwidth: int = 10, batch_size_mins: int = 30):
        self.datadir = datadir
        self.target_bandwidth = target_bandwidth
        self.resampled_fs = target_bandwidth*2
        self.cleanup_cache = cleanup_cache
        self.f_c = 1000 # TODO: what does this actually mean?

        if batch_size_mins > 60:
            error_message = "Batch size must be less than or equal to 60 minutes"
            logging.error(error_message)
            raise ValueError(error_message)
        else:
            self.batch_size_mins = batch_size_mins

        self.dro = drf.DigitalRFReader(datadir)
        self.dmr = drf.DigitalMetadataReader(os.path.join(datadir, "ch0", "metadata"))
        self.fs = int(self.dmr.get_samples_per_second())
        self.start_index, self.end_index = self.dro.get_bounds("ch0")
        self.utc_date = datetime.fromtimestamp(
            self.start_index / self.fs, tz=timezone.utc
        ).replace(hour=0, minute=0, second=0, microsecond=0)
        self.midnight_index = int(self.utc_date.timestamp() * self.fs)
        # print("Continuous blocks:", self.dro.get_continuous_blocks(self.start_index, self.start_index + 86400*self.fs, "ch0"))
        # exit()
        # self.utc_date = self._get_initial_date()
        self.metadata = {}
        self._extract_metadata()

        # Create cache directory in the same directory as the source code
        source_dir = os.path.dirname(os.path.abspath(__file__))
        self.cachedir = os.path.join(source_dir, cachedir)
        os.makedirs(self.cachedir, exist_ok=True)
        self._ensure_data_cached()

        atexit.register(self._cleanup)

    def _get_initial_date(self) -> datetime.date:
        """Retrieve the initial UTC timestamp and return channel_indexit as a date."""
        rf_properties = self.dro.get_properties("ch0", sample=self.start_index)
        init_timestamp = rf_properties["init_utc_timestamp"]
        return datetime.datetime.fromtimestamp(init_timestamp, tz=pytz.utc)

    def _extract_metadata(self):
        """Extract metadata such as station, node, and center frequencies."""
        try:
            start_global_index = self.dmr.get_bounds()[0]
            header_meta = self.dmr.read(start_global_index, start_global_index + 1)[start_global_index]
            self.metadata = header_meta
            print(self.metadata)
            self.metadata.update({
                "station": header_meta["callsign"],
                "center_frequencies": header_meta["center_frequencies"],
                "lat": header_meta["lat"],
                "lon": header_meta["long"],
                "grid": header_meta["grid_square"],
                "utc_date": self.utc_date
            })
            if "station_node_number" in header_meta:
                self.metadata["node"] = header_meta["station_node_number"]
            if "ad_sample_rate"in header_meta:
                self.metadata["sampling_rate"] = header_meta["ad_sample_rate"]
            else:
                self.metadata["sampling_rate"] = self.dmr.get_samples_per_second()

        except Exception as e:
            error_message = f"Error extracting metadata: {e}"
            logging.error(error_message)
            raise ValueError(error_message)

    def _cleanup(self):
        """Remove the cache directory if 'remove_cache_on_exit' is True."""
        if self.cleanup_cache and os.path.exists(self.cachedir):
            shutil.rmtree(self.cachedir)
            log_message = f"Removed cache directory: {self.cachedir}"
            logging.info(log_message)
            print(log_message)

    def _ensure_data_cached(self):
        """ Ensure that rawdata is resampled and cached."""
        rawdata_cached = True
        for channel_index in range(len(self.metadata["center_frequencies"])):
            if not os.path.exists(self._get_cache_file_path(channel_index)):
                rawdata_cached = False
                break
        if rawdata_cached:
            return            
        else:
            log_message = "Rawdata not cached. Caching rawdata..."
            logger.info(log_message)
            print(log_message + " This will take some time. Snacks are not included.")
            for channel_index in range(len(self.metadata["center_frequencies"])):
                log_message = f"Reading DRF rawdata for {self.metadata['center_frequencies'][channel_index]} MHz channel..."
                logger.info(log_message)
                print(log_message)
                raw_data = self._read_rawdata_channel(channel_index)
                resampled_data = self._resample(raw_data)
                self._cache_data(
                    self._get_cache_file_path(channel_index), resampled_data
                )

    def _read_rawdata_channel(self, channel_index: int):
        """Read raw data from DRF for a specific channel."""
        # Get continuous data blocks for the entire day
        cont_blocks = self.dro.get_continuous_blocks(
            self.midnight_index, self.midnight_index + self.fs * 3600 * 24, "ch0"
        )

        # Preallocate array for full day of data
        data = np.full(self.fs * 3600 * 24, np.nan, dtype=np.float32)

        # Set up progress tracking
        total_samples = sum(cont_blocks.values())
        with tqdm(total=total_samples, unit="sample", desc="Reading Data") as pbar:
            # Process each continuous block
            for start_sample, block_size in cont_blocks.items():
                # Calculate array index for this block
                data_idx = start_sample - self.midnight_index

                # Read in manageable chunks (30-minute chunks by default)
                chunk_size = self.fs * 60 * self.batch_size_mins

                # Process block in chunks if needed
                for offset in range(0, block_size, chunk_size):
                    # Determine this chunk's size
                    read_size = min(chunk_size, block_size - offset)

                    # Read data and place in the right position in the array
                    data[data_idx + offset : data_idx + offset + read_size] = (
                        self.dro.read_vector(
                            start_sample + offset, read_size, "ch0", channel_index
                        )
                    )
                    pbar.update(read_size)

        return data

    def _cache_data(self, path: str, data):
        """Cache the data to a pickle file."""
        pickle.dump(data, open(path, "wb"))
        log_message = f"Cached data at: {path}"
        logging.info(log_message)
        print(log_message)

    def _get_cache_file_path(self, channel: int) -> str:
        """Generate the file path for the cached data."""
        center_frequency = str(self.metadata["center_frequencies"][channel]).replace(".", "p")
        return os.path.join(
            self.cachedir,
            f"{self.metadata['utc_date'].date()}_{self.metadata['node']}_RAWDATA_fs-{self.resampled_fs}Hz_cf-{center_frequency}MHz.ba.pkl",
        )

    def _resample(self, data):
        decimation_factor = math.ceil(self.fs / self.resampled_fs)
        # Process data in chunks to save memory
        chunk_size = min(10**6, len(data))

        # Use an overlap large enough to eliminate boundary effects from filtering
        # This is based on the typical filter length used in resampling
        overlap = 10 * decimation_factor

        resampled_chunks = []

        for i in range(0, len(data), chunk_size - overlap):
            # Extract chunk with overlap
            start_idx = max(0, i)
            end_idx = min(len(data), i + chunk_size)
            chunk = data[start_idx:end_idx]

            # Calculate time for this chunk
            t_chunk = np.arange(start_idx, end_idx) / self.fs

            # Frequency shifting
            cos_wave = np.cos(2 * np.pi * (self.f_c - self.target_bandwidth / 2) * t_chunk)
            shifted_chunk = chunk * cos_wave

            # Use resample_poly instead of decimate for better quality and control
            resampled_chunk = signal.resample_poly(shifted_chunk, 1, decimation_factor)

            # Determine which portion to keep (exclude boundaries except for first/last chunks)
            if start_idx == 0:  # First chunk
                valid_start = 0
            else:
                valid_start = overlap // (2 * decimation_factor)

            if end_idx == len(data):  # Last chunk
                valid_end = len(resampled_chunk)
            else:
                valid_end = len(resampled_chunk) - overlap // (2 * decimation_factor)

            # Keep only the valid portion
            if valid_end > valid_start:
                resampled_chunks.append(resampled_chunk[valid_start:valid_end])

        # Concatenate all valid portions
        return np.concatenate(resampled_chunks) if resampled_chunks else np.array([])

    def read_data(self, channel_index: int) -> np.ndarray:
        """ Read the cached datata for the specified channel. """
        cache_path = self._get_cache_file_path(channel_index)
        return pickle.load(open(cache_path, "rb"))

    def get_metadata(self) -> Dict[str, Any]:
        """Return metadata information."""
        return self.metadata


if __name__ == "__main__":
    # data_dir = sys.argv[1]
    # cache_dir = 'cache'
    # data_reader = PSWSDataReader(data_dir, cachedir=cache_dir, resampled_fs=2000) 
    # for i in range(3): 
    #     data_reader.read_data(i)
    logging.basicConfig(
        filename="reader.log",
        level=logging.DEBUG,  # TODO:  Add support for setting logging level at command-line level. Default: WARNING
        format="%(asctime)s | %(levelname)-10s | %(name)-15s | %(funcName)-30s | %(message)s",
        force=True
    )
    datadir = "/home/cuong/drive/GRAPE2-SFTP/w2naf"
    cachedir = "cache"
    reader = Reader(datadir, cachedir)
