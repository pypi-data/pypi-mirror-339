# RAG Evaluation

**RAG Evaluation** is a Python package designed for evaluating Retrieval-Augmented Generation (RAG) systems. It provides a systematic way to score and analyze the quality of responses generated by RAG Systems. This package is particularly suited for projects leveraging Large Language Models (LLMs) such as GPT, Gemini etc.

It integrates easily with the OpenAI API via openai and automatically handles environment variable-based API key loading through python-dotenv.


## Features

- **Multi-Metric Evaluation:** Evaluate responses using a variety of metrics:
  - **Query Relevance**
  - **Factual Accuracy**
  - **Coverage**
  - **Coherence**
  - **Fluency**
- **Standardized Prompting:** Uses a well-defined prompt template to assess responses consistently.
- **Customizable:** Easily extendable to add new metrics or evaluation criteria.
- **Easy Integration:** Provides a high-level function to integrate evaluation into your RAG pipelines.

## Installation


### Using pip

```bash
pip install rag_evaluation

### Usage

import openai
from rag_evaluation import get_openai_key, evaluate_response

## Set the API key (either via environment or by providing a default)
api_key = get_openai_key("OPENAI_API_KEY")

# Define your inputs
query = "Which large language model is currently the largest and most capable?"

response_text = """The largest and most capable LLMs are the generative pretrained transformers (GPTs). These models are 
                designed to handle complex language tasks, and their vast number of parameters gives them the ability to 
                understand and generate human-like text."""
                 
document = """A large language model (LLM) is a type of machine learning model designed for natural language processing 
            tasks such as language generation. LLMs are language models with many parameters, and are trained with 
            self-supervised learning on a vast amount of text. The largest and most capable LLMs are 
            generative pretrained transformers (GPTs). Modern models can be fine-tuned for specific tasks or guided 
            by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies 
            inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained in."""

# Evaluate using the default model (or specify a different model such as 'gpt-4o')
report = evaluate_response(query, response_text, document, model="gpt-4o-mini")
print(report)

**This function returns a pandas DataFrame with:**

Metric Names: Query Relevance, Factual Accuracy, Coverage, Coherence, Fluency.

Normalized Scores: A 0â€“1 score for each metric.

Percentage Scores: The normalized score expressed as a percentage.

Overall Accuracy: A weighted average score across all metrics.

