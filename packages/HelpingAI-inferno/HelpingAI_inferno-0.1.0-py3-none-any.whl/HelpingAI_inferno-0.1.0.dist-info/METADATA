Metadata-Version: 2.1
Name: HelpingAI-inferno
Version: 0.1.0
Summary: A professional, production-ready inference server for running any AI model with universal model compatibility and multi-hardware support
Home-page: https://github.com/HelpingAI/inferno
Author: HelpingAI
Author-email: info@helpingai.com
Classifier: Development Status :: 4 - Beta
Classifier: Intended Audience :: Developers
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Requires-Python: >=3.8
Description-Content-Type: text/plain
License-File: LICENSE
Requires-Dist: torch>=2.0.0
Requires-Dist: transformers>=4.30.0
Requires-Dist: fastapi>=0.95.0
Requires-Dist: uvicorn>=0.22.0
Requires-Dist: pydantic>=1.10.0
Requires-Dist: requests>=2.28.0
Requires-Dist: tqdm>=4.64.0
Requires-Dist: py-cpuinfo>=9.0.0
Requires-Dist: bitsandbytes>=0.40.0
Requires-Dist: psutil>=5.9.0
Requires-Dist: huggingface-hub>=0.16.0
Requires-Dist: rich>=10.0.0
Provides-Extra: gguf
Requires-Dist: llama-cpp-python; extra == "gguf"
Requires-Dist: cmake; extra == "gguf"
Requires-Dist: ninja; extra == "gguf"
Provides-Extra: tpu
Requires-Dist: torch-xla; extra == "tpu"

Inferno is a high-performance inference server that can run any AI model from Hugging Face, local files, or GGUF format. It features automatic memory management, hardware detection, and supports CPU, GPU, TPU, and Apple Silicon platforms.
